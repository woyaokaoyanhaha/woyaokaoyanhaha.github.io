# -*- coding: utf-8 -*-
"""
ESM2é›†æˆBBBç©¿é€è‚½æ­£æ ·æœ¬ç”Ÿæˆå™¨
åŸºäºintegrated_bbb_positive_generator.pyï¼Œä½¿ç”¨ESM2æ¨¡å‹è¡¨å¾è›‹ç™½è´¨
ç”¨æˆ·: woyaokaoyanhaha
å½“å‰æ—¶é—´: 2025-07-16 15:30:00 UTC
æ¶æ„: ESM2ç‰¹å¾æå– + æ‰©æ•£æ¨¡å‹ + Transformerç”Ÿæˆ
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import math
from tqdm import tqdm
import random
from typing import List, Tuple, Dict, Set
import logging
from pathlib import Path
import warnings
import json
import time
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import hashlib
import gc

# ESM2ç›¸å…³å¯¼å…¥
try:
    import esm
    ESM2_AVAILABLE = True
except ImportError:
    ESM2_AVAILABLE = False
    print("âš ï¸ ESM2æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install fair-esm")

warnings.filterwarnings('ignore')

# ==================== ESM2æ•´åˆæ­£æ ·æœ¬ç”Ÿæˆé…ç½® ====================
class ESM2IntegratedPositiveConfig:
    """ESM2æ•´åˆBBBç©¿é€è‚½æ­£æ ·æœ¬ç”Ÿæˆé…ç½®"""
    
    # åŸºç¡€å‚æ•°
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    seq_len = 20  # ä¿æŒä¸åŸå§‹ä»£ç ä¸€è‡´
    vocab_size = 20
    
    # ESM2æ¨¡å‹é…ç½®
    esm2_model_name = "esm2_t6_8M_UR50D"  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜
    esm2_repr_layers = [6]  # æœ€åä¸€å±‚
    esm2_max_seq_len = 20
    esm2_embedding_dim = 320  # ESM2-8Mçš„åµŒå…¥ç»´åº¦
    freeze_esm2 = True  # å†»ç»“ESM2å‚æ•°
    
    # æ•°æ®å¹³è¡¡å‚æ•°
    existing_positive_samples = 329
    existing_negative_samples = 6851
    target_positive_samples = 6851
    need_to_generate = 6522
    
    # è®­ç»ƒå‚æ•° - é’ˆå¯¹ESM2ä¼˜åŒ–
    batch_size = 8  # è°ƒæ•´æ‰¹æ¬¡å¤§å°é€‚åº”ESM2
    lr = 3e-5
    n_epochs = 100
    early_stopping_patience = 15
    
    # æ‰©æ•£å‚æ•°
    n_timesteps = 1000
    beta_start = 0.0001
    beta_end = 0.02
    
    # æ¨¡å‹å¤æ‚åº¦ - é›†æˆESM2
    embedding_dim = 256  # å¢åŠ åµŒå…¥ç»´åº¦ä»¥åŒ¹é…ESM2
    transformer_layers = 4
    attention_heads = 8
    dropout_rate = 0.15
    
    # ç”Ÿæˆå‚æ•°
    n_sequences = 6522
    quality_threshold = 0.55
    
    # é‡‡æ ·å‚æ•°
    sampling_steps = 50
    temperature_start = 2.5
    temperature_end = 0.8
    temperature_schedule = "cosine"
    top_k = 15
    top_p = 0.88
    
    # BBBè‚½å®é™…éœ€æ±‚å‚æ•°
    target_length_min = 6
    target_length_max = 15
    target_length_optimal = 9
    target_mw_min = 500
    target_mw_max = 2000
    target_mw_optimal = 1200
    target_charge_min = 1
    target_charge_max = 6
    target_charge_optimal = 3
    
    # BBBè‚½å¿…éœ€ç‰¹å¾
    min_cationic_residues = 2
    max_cationic_residues = 6
    min_hydrophobic_residues = 1
    max_hydrophobic_residues = 8
    min_aromatic_residues = 0
    max_aromatic_residues = 4
    
    # å¤šæ ·æ€§å¢å¼ºå‚æ•°
    diversity_boost = True
    diversity_temperature = 2.2
    nucleus_sampling = True
    repetition_penalty = 1.25
    length_penalty = 0.12
    
    # å»é‡å‚æ•°
    enable_deduplication = True
    similarity_threshold = 0.78
    max_generation_attempts = 35000
    duplicate_check_window = 150
    
    # åŠ¨æ€é‡‡æ ·å‚æ•°
    dynamic_sampling = True
    adaptive_temperature = True
    diversity_penalty_weight = 0.25
    
    # æ˜¾å­˜ä¼˜åŒ–å‚æ•°
    mixed_precision = True
    gradient_accumulation_steps = 4
    gradient_clip_norm = 1.0
    num_workers = 0  # ESM2ä½¿ç”¨æ—¶å»ºè®®è®¾ä¸º0
    pin_memory = False
    
    # æ•°æ®å¢å¼º
    augment_data = True
    augment_ratio = 3.0
    noise_augmentation = True
    
    # è´¨é‡è¯„ä¼°æƒé‡
    property_weights = {
        'length_score': 0.20,
        'charge_score': 0.20,
        'molecular_weight': 0.18,
        'cationic_ratio': 0.18,
        'hydrophobic_ratio': 0.14,
        'bbb_motifs': 0.10
    }
    
    # æ–‡ä»¶è·¯å¾„
    input_file = "train_pos_org.fasta"
    output_dir = "esm2_integrated_positive_output"
    model_save_path = "esm2_integrated_positive_model.pth"
    
    # éªŒè¯å’Œä¿å­˜
    val_split = 0.12
    save_interval = 25
    
    # ç”Ÿæˆæ‰¹æ¬¡æ§åˆ¶
    generation_batch_size = 32
    
    # æ—¥å¿—é…ç½®
    log_level = logging.INFO
    log_to_console = True
    log_to_file = True
    
    # ç”¨æˆ·ä¿¡æ¯
    user_login = "woyaokaoyanhaha"
    current_time = "2025-07-16 15:30:00"


config = ESM2IntegratedPositiveConfig()

# ==================== æ°¨åŸºé…¸å±æ€§å®šä¹‰ ====================
AA_PROPERTIES = {
    'A': {'mw': 89.1, 'hydro': 1.8, 'charge': 0, 'polar': 0, 'volume': 88.6},
    'R': {'mw': 174.2, 'hydro': -4.5, 'charge': 1, 'polar': 1, 'volume': 173.4},
    'N': {'mw': 132.1, 'hydro': -3.5, 'charge': 0, 'polar': 1, 'volume': 114.1},
    'D': {'mw': 133.1, 'hydro': -3.5, 'charge': -1, 'polar': 1, 'volume': 111.1},
    'C': {'mw': 121.0, 'hydro': 2.5, 'charge': 0, 'polar': 0, 'volume': 108.5},
    'Q': {'mw': 146.1, 'hydro': -3.5, 'charge': 0, 'polar': 1, 'volume': 143.8},
    'E': {'mw': 147.1, 'hydro': -3.5, 'charge': -1, 'polar': 1, 'volume': 138.4},
    'G': {'mw': 75.1, 'hydro': -0.4, 'charge': 0, 'polar': 0, 'volume': 60.1},
    'H': {'mw': 155.2, 'hydro': -3.2, 'charge': 0, 'polar': 1, 'volume': 153.2},
    'I': {'mw': 131.2, 'hydro': 4.5, 'charge': 0, 'polar': 0, 'volume': 166.7},
    'L': {'mw': 131.2, 'hydro': 3.8, 'charge': 0, 'polar': 0, 'volume': 166.7},
    'K': {'mw': 146.2, 'hydro': -3.9, 'charge': 1, 'polar': 1, 'volume': 168.6},
    'M': {'mw': 149.2, 'hydro': 1.9, 'charge': 0, 'polar': 0, 'volume': 162.9},
    'F': {'mw': 165.2, 'hydro': 2.8, 'charge': 0, 'polar': 0, 'volume': 189.9},
    'P': {'mw': 115.1, 'hydro': -1.6, 'charge': 0, 'polar': 0, 'volume': 112.7},
    'S': {'mw': 105.1, 'hydro': -0.8, 'charge': 0, 'polar': 1, 'volume': 89.0},
    'T': {'mw': 119.1, 'hydro': -0.7, 'charge': 0, 'polar': 1, 'volume': 116.1},
    'W': {'mw': 204.2, 'hydro': -0.9, 'charge': 0, 'polar': 0, 'volume': 227.8},
    'Y': {'mw': 181.2, 'hydro': -1.3, 'charge': 0, 'polar': 1, 'volume': 193.6},
    'V': {'mw': 117.1, 'hydro': 4.2, 'charge': 0, 'polar': 0, 'volume': 140.0}
}

AMINO_ACIDS = list(AA_PROPERTIES.keys())
AA_TO_IDX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}
IDX_TO_AA = {i: aa for i, aa in enumerate(AMINO_ACIDS)}

# ==================== ESM2ç‰¹å¾æå–å™¨ ====================
class ESM2FeatureExtractor:
    """ESM2ç‰¹å¾æå–å™¨"""
    
    def __init__(self, config, logger):
        self.config = config
        self.logger = logger
        
        if not ESM2_AVAILABLE:
            raise ImportError("ESM2æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install fair-esm")
        
        self.logger.info(f"ğŸ§¬ åˆå§‹åŒ–ESM2ç‰¹å¾æå–å™¨: {config.esm2_model_name}")
        
        # åŠ è½½ESM2æ¨¡å‹
        self.model, self.alphabet = self._load_esm2_model()
        self.model = self.model.to(config.device)
        self.model.eval()
        
        # å†»ç»“ESM2å‚æ•°
        if config.freeze_esm2:
            for param in self.model.parameters():
                param.requires_grad = False
            self.logger.info("ğŸ”’ ESM2å‚æ•°å·²å†»ç»“")
        
        self.batch_converter = self.alphabet.get_batch_converter()
        
        self.logger.info("âœ… ESM2ç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_esm2_model(self):
        """åŠ è½½ESM2æ¨¡å‹"""
        try:
            if self.config.esm2_model_name == "esm2_t6_8M_UR50D":
                model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()
                self.config.esm2_embedding_dim = 320
            elif self.config.esm2_model_name == "esm2_t12_35M_UR50D":
                model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()
                self.config.esm2_embedding_dim = 480
            else:
                model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()
                self.config.esm2_embedding_dim = 320
            
            return model, alphabet
        except Exception as e:
            self.logger.error(f"âŒ ESM2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def extract_features(self, sequences: List[str], batch_size: int = None) -> torch.Tensor:
        """æå–ESM2ç‰¹å¾"""
        if batch_size is None:
            batch_size = max(1, self.config.batch_size // 4)  # ESM2éœ€è¦æ›´å°çš„æ‰¹æ¬¡
        
        self.logger.info(f"ğŸ” æå– {len(sequences)} ä¸ªåºåˆ—çš„ESM2ç‰¹å¾")
        all_features = []
        
        # æ˜¾å­˜ç›‘æ§
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(sequences), batch_size), desc="æå–ESM2ç‰¹å¾"):
            batch_sequences = sequences[i:i+batch_size]
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            batch_labels = [(f"seq_{j}", seq) for j, seq in enumerate(batch_sequences)]
            
            try:
                # è½¬æ¢ä¸ºESM2è¾“å…¥æ ¼å¼
                batch_tokens = self.batch_converter(batch_labels)[2]
                
                # é™åˆ¶åºåˆ—é•¿åº¦
                if batch_tokens.size(1) > self.config.esm2_max_seq_len:
                    batch_tokens = batch_tokens[:, :self.config.esm2_max_seq_len]
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                batch_tokens = batch_tokens.to(self.config.device)
                
                # æå–ç‰¹å¾
                with torch.no_grad():
                    if self.config.mixed_precision and self.config.device != 'cpu':
                        with torch.cuda.amp.autocast():
                            results = self.model(batch_tokens, repr_layers=self.config.esm2_repr_layers)
                    else:
                        results = self.model(batch_tokens, repr_layers=self.config.esm2_repr_layers)
                    
                    # è·å–è¡¨ç¤º
                    representations = results["representations"][self.config.esm2_repr_layers[0]]
                    
                    # å»æ‰ç‰¹æ®Štokenï¼ˆCLSå’ŒSEPï¼‰
                    sequence_representations = representations[:, 1:-1]
                    
                    # å¤„ç†è¡¨ç¤º
                    batch_features = self._process_representations(sequence_representations, batch_sequences)
                    
                    # ç§»åˆ°CPUèŠ‚çœæ˜¾å­˜
                    batch_features = batch_features.cpu()
                    all_features.append(batch_features)
                
            except Exception as e:
                self.logger.error(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} ç‰¹å¾æå–å¤±è´¥: {e}")
                # åˆ›å»ºå¤‡ç”¨ç‰¹å¾
                backup_features = torch.zeros(len(batch_sequences), self.config.seq_len, self.config.esm2_embedding_dim)
                all_features.append(backup_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        if all_features:
            all_features = torch.cat(all_features, dim=0)
        else:
            all_features = torch.zeros(len(sequences), self.config.seq_len, self.config.esm2_embedding_dim)
        
        self.logger.info(f"âœ… ESM2ç‰¹å¾æå–å®Œæˆ: {all_features.shape}")
        return all_features
    
    def _process_representations(self, representations: torch.Tensor, sequences: List[str]) -> torch.Tensor:
        """å¤„ç†ESM2è¡¨ç¤º"""
        batch_size = representations.size(0)
        device = representations.device
        
        features = []
        
        for i in range(batch_size):
            seq_len = min(len(sequences[i]), representations.size(1))
            seq_repr = representations[i, :seq_len]
            
            # å¡«å……æˆ–æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
            if seq_len < self.config.seq_len:
                padding = torch.zeros(self.config.seq_len - seq_len, self.config.esm2_embedding_dim, device=device)
                seq_features = torch.cat([seq_repr, padding], dim=0)
            else:
                seq_features = seq_repr[:self.config.seq_len]
            
            features.append(seq_features)
        
        return torch.stack(features, dim=0)


# ==================== ESM2æ•´åˆæ•°æ®é›† ====================
class ESM2IntegratedDataset(Dataset):
    """ESM2æ•´åˆæ•°æ®é›†"""
    
    def __init__(self, sequences: List[str], esm2_features: torch.Tensor, logger):
        self.sequences = sequences
        self.esm2_features = esm2_features
        self.logger = logger
        
        # ç¼–ç åºåˆ—
        self.encoded_sequences = []
        for seq in sequences:
            # å¡«å……åºåˆ—
            padded_seq = self._pad_sequence(seq, config.seq_len)
            indices = self._sequence_to_indices(padded_seq)
            self.encoded_sequences.append(torch.tensor(indices, dtype=torch.long))
        
        self.logger.info(f"âœ… ESM2æ•´åˆæ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(sequences)} ä¸ªåºåˆ—")
    
    def _pad_sequence(self, seq: str, max_len: int) -> str:
        """å¡«å……åºåˆ—åˆ°æŒ‡å®šé•¿åº¦"""
        if len(seq) > max_len:
            return seq[:max_len]
        return seq + 'A' * (max_len - len(seq))
    
    def _sequence_to_indices(self, seq: str) -> List[int]:
        """å°†åºåˆ—è½¬æ¢ä¸ºç´¢å¼•"""
        return [AA_TO_IDX.get(aa, 0) for aa in seq]
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return {
            'sequence': self.sequences[idx],
            'encoded_sequence': self.encoded_sequences[idx],
            'esm2_features': self.esm2_features[idx]
        }


# ==================== ESM2æ•´åˆæ‰©æ•£è°ƒåº¦å™¨ ====================
class ESM2IntegratedDiffusionScheduler:
    """ESM2æ•´åˆæ‰©æ•£è°ƒåº¦å™¨"""
    
    def __init__(self, n_timesteps: int = config.n_timesteps,
                 vocab_size: int = config.vocab_size,
                 device: str = config.device):
        self.n_timesteps = n_timesteps
        self.vocab_size = vocab_size
        self.device = device
        
        # ç”Ÿæˆbetaè°ƒåº¦
        self.betas = torch.linspace(config.beta_start, config.beta_end, n_timesteps).to(device)
        self.alphas = (1.0 - self.betas).to(device)
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0).to(device)
        
        # é¢„è®¡ç®—å¸¸ç”¨å€¼
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod).to(device)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod).to(device)
        
        # åéªŒæ–¹å·®
        if n_timesteps > 1:
            alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), self.alphas_cumprod[:-1]])
            self.posterior_variance = (
                self.betas * (1.0 - alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
            ).to(device)
        else:
            self.posterior_variance = torch.tensor([0.0], device=device)
        
        self.posterior_variance = torch.clamp(self.posterior_variance, min=1e-20)
    
    def add_noise(self, x_start: torch.Tensor, t: torch.Tensor, noise: torch.Tensor = None):
        """å‘ç¦»æ•£åºåˆ—æ·»åŠ å™ªå£°"""
        if noise is None:
            noise = torch.randint_like(x_start, 0, self.vocab_size)
        
        t = t.to(self.device)
        alpha_t = self.alphas_cumprod[t]
        
        batch_size, seq_len = x_start.shape
        alpha_t_expanded = alpha_t.view(batch_size, 1).expand(batch_size, seq_len)
        
        # å™ªå£°æ©ç 
        noise_mask = torch.rand_like(alpha_t_expanded) > alpha_t_expanded
        x_noisy = torch.where(noise_mask, noise, x_start)
        
        return x_noisy, noise


# ==================== ESM2æ•´åˆæ‰©æ•£æ¨¡å‹ ====================
class ESM2IntegratedDiffusionModel(nn.Module):
    """ESM2æ•´åˆæ‰©æ•£æ¨¡å‹"""
    
    def __init__(self, device: str = config.device):
        super().__init__()
        self.device = device
        self.vocab_size = config.vocab_size
        
        # æ‰©æ•£è°ƒåº¦å™¨
        self.scheduler = ESM2IntegratedDiffusionScheduler(device=device)
        
        # æ ‡å‡†åµŒå…¥å±‚
        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim)
        self.pos_embedding = nn.Parameter(torch.randn(config.seq_len, config.embedding_dim))
        
        # ESM2ç‰¹å¾æŠ•å½±å±‚
        self.esm2_projection = nn.Linear(config.esm2_embedding_dim, config.embedding_dim)
        
        # æ—¶é—´åµŒå…¥
        self.time_embedding = nn.Sequential(
            nn.Linear(1, config.embedding_dim),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.embedding_dim, config.embedding_dim),
            nn.GELU(),
            nn.Dropout(config.dropout_rate)
        )
        
        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Sequential(
            nn.Linear(config.embedding_dim * 2, config.embedding_dim),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.embedding_dim, config.embedding_dim)
        )
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.embedding_dim,
            nhead=config.attention_heads,
            dim_feedforward=config.embedding_dim * 3,
            dropout=config.dropout_rate,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, config.transformer_layers)
        
        # è¾“å‡ºå±‚
        self.output_proj = nn.Sequential(
            nn.Linear(config.embedding_dim, config.embedding_dim),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.embedding_dim, config.vocab_size)
        )
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(config.embedding_dim)
        
        # åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, std=0.02)
    
    def forward(self, x, t, esm2_features=None):
        batch_size, seq_len = x.shape
        
        # æ ‡å‡†åµŒå…¥
        x_emb = self.embedding(x)
        pos_emb = self.pos_embedding[:seq_len].unsqueeze(0).expand(batch_size, -1, -1)
        
        # æ—¶é—´åµŒå…¥
        t_emb = self.time_embedding(t.float().unsqueeze(-1))
        t_emb = t_emb.unsqueeze(1).expand(batch_size, seq_len, -1)
        
        # ESM2ç‰¹å¾é›†æˆ
        if esm2_features is not None:
            esm2_proj = self.esm2_projection(esm2_features)
            # èåˆæ ‡å‡†åµŒå…¥å’ŒESM2ç‰¹å¾
            combined_emb = torch.cat([x_emb, esm2_proj], dim=-1)
            fused_emb = self.feature_fusion(combined_emb)
        else:
            fused_emb = x_emb
        
        # ç»„åˆæ‰€æœ‰åµŒå…¥
        h = self.layer_norm(fused_emb + pos_emb + t_emb)
        
        # Transformer
        h = self.transformer(h)
        
        # è¾“å‡º
        logits = self.output_proj(h)
        
        return logits
    
    def forward_with_loss(self, x, t, esm2_features=None):
        """å‰å‘ä¼ æ’­å¹¶è®¡ç®—æŸå¤±"""
        # æ·»åŠ å™ªå£°
        x_noisy, noise = self.scheduler.add_noise(x, t)
        
        # é¢„æµ‹
        predicted_logits = self.forward(x_noisy, t, esm2_features)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(predicted_logits.reshape(-1, self.vocab_size),
                              x.reshape(-1), reduction='mean')
        
        return loss, predicted_logits


# ==================== æ•´åˆçš„å¤šæ ·æ€§é‡‡æ ·å™¨ ====================
class ESM2IntegratedDiversitySampler:
    """ESM2æ•´åˆå¤šæ ·æ€§é‡‡æ ·å™¨"""
    
    def __init__(self, config, logger=None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        
        # BBBè‚½ç‰¹å¾ - æ·»åŠ ç¼ºå¤±çš„ hydrophobic_residues å®šä¹‰
        self.cationic_residues = ['R', 'K', 'H']
        self.hydrophobic_residues = ['W', 'F', 'L', 'I', 'V', 'A', 'M', 'Y']  # æ·»åŠ è¿™è¡Œ
        self.aromatic_residues = ['W', 'F', 'Y', 'H']
        
        # å¤šæ ·æ€§è¿½è¸ª
        self.generated_patterns = defaultdict(int)
        self.amino_acid_usage = defaultdict(int)
        self.sequence_lengths = defaultdict(int)
    
    def get_adaptive_temperature(self, step: int, total_steps: int, 
                                diversity_factor: float = 1.0, 
                                bbb_compliance: float = 1.0) -> float:
        """è‡ªé€‚åº”æ¸©åº¦è°ƒèŠ‚ - é’ˆå¯¹ESM2ä¼˜åŒ–"""
        base_temp = self.get_base_temperature(step, total_steps)
        
        # é’ˆå¯¹ESM2ç‰¹å¾çš„æ¸©åº¦è°ƒæ•´
        if diversity_factor < 0.5:
            diversity_boost = 1.5
        elif diversity_factor < 0.7:
            diversity_boost = 1.2
        else:
            diversity_boost = 1.0
        
        # BBBè‚½ç¬¦åˆåº¦è°ƒæ•´
        if bbb_compliance < 0.4:
            bbb_boost = 1.3
        elif bbb_compliance < 0.7:
            bbb_boost = 1.1
        else:
            bbb_boost = 1.0
        
        return base_temp * diversity_boost * bbb_boost
    
    def get_base_temperature(self, step: int, total_steps: int) -> float:
        """åŸºç¡€æ¸©åº¦è°ƒèŠ‚"""
        progress = step / total_steps
        
        if self.config.temperature_schedule == "linear":
            return self.config.temperature_start * (1 - progress) + self.config.temperature_end * progress
        elif self.config.temperature_schedule == "exponential":
            decay_rate = 1.0
            temp = self.config.temperature_start * np.exp(-decay_rate * progress)
            return max(temp, self.config.temperature_end)
        elif self.config.temperature_schedule == "cosine":
            temp = self.config.temperature_end + (self.config.temperature_start - self.config.temperature_end) * \
                   (1 + np.cos(np.pi * progress)) / 2
            return temp
        else:
            return self.config.temperature_start * (1 - progress) + self.config.temperature_end * progress
    
    def apply_bbb_bias(self, logits: torch.Tensor, current_seq: torch.Tensor, 
                      step: int, total_steps: int) -> torch.Tensor:
        """åº”ç”¨BBBè‚½åç½® - é’ˆå¯¹ESM2ä¼˜åŒ–"""
        if step < total_steps * 0.15:  # åœ¨å‰15%çš„æ­¥éª¤ä¸­åº”ç”¨å¼ºBBBåç½®
            return logits
        
        # ç¡®ä¿tensorsåœ¨åŒä¸€è®¾å¤‡
        logits = logits.to(current_seq.device)
        
        # è®¡ç®—å½“å‰åºåˆ—çš„BBBè‚½ç‰¹å¾
        batch_size, seq_len = current_seq.shape
        
        for i in range(batch_size):
            seq_so_far = current_seq[i].cpu().numpy()
            
            # è½¬æ¢ä¸ºæ°¨åŸºé…¸åºåˆ—
            aa_seq = [IDX_TO_AA.get(idx, 'A') for idx in seq_so_far]
            
            # è®¡ç®—å½“å‰ç‰¹å¾
            cationic_count = sum(1 for aa in aa_seq if aa in self.cationic_residues)
            hydrophobic_count = sum(1 for aa in aa_seq if aa in self.hydrophobic_residues)
            current_length = seq_len
            
            # è®¡ç®—éœ€è¦çš„ç‰¹å¾
            cationic_ratio = cationic_count / current_length
            hydrophobic_ratio = hydrophobic_count / current_length
            
            # åº”ç”¨åç½® - ç¡®ä¿åœ¨ç›¸åŒè®¾å¤‡
            for j in range(seq_len):
                # å¦‚æœé˜³ç¦»å­æ®‹åŸºä¸è¶³ï¼Œæé«˜R, K, Hçš„æ¦‚ç‡
                if cationic_ratio < 0.22:
                    logits[i, j, AA_TO_IDX['R']] += 0.7
                    logits[i, j, AA_TO_IDX['K']] += 0.7
                    logits[i, j, AA_TO_IDX['H']] += 0.4
                
                # å¦‚æœç–æ°´æ®‹åŸºä¸è¶³ï¼Œæé«˜ç–æ°´æ®‹åŸºçš„æ¦‚ç‡
                if hydrophobic_ratio < 0.18:
                    for hydro_aa in ['W', 'F', 'L', 'I', 'V']:
                        if hydro_aa in AA_TO_IDX:
                            logits[i, j, AA_TO_IDX[hydro_aa]] += 0.3
                
                # é™ä½è´Ÿç”µè·æ®‹åŸºçš„æ¦‚ç‡
                logits[i, j, AA_TO_IDX['D']] -= 0.25
                logits[i, j, AA_TO_IDX['E']] -= 0.25
                
                # å¦‚æœé˜³ç¦»å­æ®‹åŸºè¿‡å¤šï¼Œé€‚åº¦é™ä½å…¶æ¦‚ç‡
                if cationic_ratio > 0.55:
                    logits[i, j, AA_TO_IDX['R']] -= 0.15
                    logits[i, j, AA_TO_IDX['K']] -= 0.15
        
        return logits
    
    def enhanced_sampling(self, logits: torch.Tensor, input_ids: torch.Tensor, 
                         step: int, total_steps: int, 
                         generated_tokens: List[int] = None,
                         generated_sequences: List[str] = None) -> torch.Tensor:
        """å¢å¼ºçš„æ•´åˆé‡‡æ · - é’ˆå¯¹ESM2ä¼˜åŒ–"""
        batch_size, seq_len, vocab_size = logits.shape
        
        # è®¡ç®—å¤šæ ·æ€§å› å­å’ŒBBBç¬¦åˆåº¦
        diversity_factor = self.calculate_diversity_factor(generated_sequences or [])
        bbb_compliance = self.calculate_bbb_compliance(generated_sequences or [])
        
        # è·å–è‡ªé€‚åº”æ¸©åº¦
        temperature = self.get_adaptive_temperature(step, total_steps, diversity_factor, bbb_compliance)
        
        # åº”ç”¨æ¸©åº¦ - ç¡®ä¿åœ¨ç›¸åŒè®¾å¤‡
        logits = logits.to(input_ids.device)
        scaled_logits = logits / temperature
        
        # åº”ç”¨BBBåç½®
        scaled_logits = self.apply_bbb_bias(scaled_logits, input_ids, step, total_steps)
        
        # åº”ç”¨nucleusé‡‡æ ·
        if self.config.nucleus_sampling:
            scaled_logits = self.nucleus_sampling_enhanced(scaled_logits, self.config.top_p)
        
        # è½¬æ¢ä¸ºæ¦‚ç‡
        probs = F.softmax(scaled_logits, dim=-1)
        
        # åŠ¨æ€top-ké‡‡æ ·
        if self.config.top_k > 0:
            current_top_k = self.config.top_k
            if diversity_factor < 0.6 or bbb_compliance < 0.4:
                current_top_k = min(self.config.top_k + 3, vocab_size)
            
            top_k_probs, top_k_indices = torch.topk(probs, current_top_k, dim=-1)
            top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)
            
            sampled_indices = torch.multinomial(top_k_probs.reshape(-1, current_top_k), 1)
            result = torch.gather(top_k_indices.reshape(-1, current_top_k), 1, sampled_indices)
            result = result.reshape(batch_size, seq_len)
        else:
            result = torch.multinomial(probs.reshape(-1, vocab_size), 1)
            result = result.reshape(batch_size, seq_len)
        
        return result
    
    def nucleus_sampling_enhanced(self, logits: torch.Tensor, p: float = 0.9) -> torch.Tensor:
        """å¢å¼ºçš„nucleusé‡‡æ ·"""
        if not self.config.nucleus_sampling:
            return logits
        
        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        sorted_indices_to_remove = cumulative_probs > p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = False
        
        indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)
        logits[indices_to_remove] = float('-inf')
        
        return logits
    
    def calculate_diversity_factor(self, generated_sequences: List[str]) -> float:
        """è®¡ç®—å¤šæ ·æ€§å› å­"""
        if not generated_sequences:
            return 1.0
        
        # åºåˆ—å”¯ä¸€æ€§
        unique_sequences = set(generated_sequences)
        uniqueness = len(unique_sequences) / len(generated_sequences)
        
        # æ°¨åŸºé…¸ä½¿ç”¨å¤šæ ·æ€§
        aa_counts = defaultdict(int)
        total_aa = 0
        for seq in generated_sequences:
            for aa in seq:
                aa_counts[aa] += 1
                total_aa += 1
        
        aa_entropy = 0
        for count in aa_counts.values():
            if count > 0:
                p = count / total_aa
                aa_entropy -= p * np.log2(p)
        
        max_entropy = np.log2(20)
        aa_diversity = aa_entropy / max_entropy
        
        # é•¿åº¦å¤šæ ·æ€§
        lengths = [len(seq) for seq in generated_sequences]
        unique_lengths = len(set(lengths))
        length_diversity = min(unique_lengths / 10, 1.0)
        
        # ç»¼åˆå¤šæ ·æ€§å› å­
        diversity_factor = (uniqueness * 0.5 + aa_diversity * 0.3 + length_diversity * 0.2)
        return diversity_factor
    
    def calculate_bbb_compliance(self, generated_sequences: List[str]) -> float:
        """è®¡ç®—BBBè‚½ç¬¦åˆåº¦"""
        if not generated_sequences:
            return 1.0
        
        compliant_count = 0
        for seq in generated_sequences:
            # æ£€æŸ¥é•¿åº¦
            if not (self.config.target_length_min <= len(seq) <= self.config.target_length_max):
                continue
            
            # æ£€æŸ¥é˜³ç¦»å­æ®‹åŸº
            cationic_count = sum(1 for aa in seq if aa in self.cationic_residues)
            if not (self.config.min_cationic_residues <= cationic_count <= self.config.max_cationic_residues):
                continue
            
            # æ£€æŸ¥ç–æ°´æ®‹åŸº
            hydrophobic_count = sum(1 for aa in seq if aa in self.hydrophobic_residues)
            if not (self.config.min_hydrophobic_residues <= hydrophobic_count <= self.config.max_hydrophobic_residues):
                continue
            
            # æ£€æŸ¥åˆ†å­é‡
            mw = sum(AA_PROPERTIES[aa]['mw'] for aa in seq if aa in AA_PROPERTIES)
            if not (self.config.target_mw_min <= mw <= self.config.target_mw_max):
                continue
            
            # æ£€æŸ¥å‡€ç”µè·
            net_charge = sum(AA_PROPERTIES[aa]['charge'] for aa in seq if aa in AA_PROPERTIES)
            if not (self.config.target_charge_min <= net_charge <= self.config.target_charge_max):
                continue
            
            compliant_count += 1
        
        return compliant_count / len(generated_sequences)


# ==================== æ•´åˆçš„è´¨é‡è¯„ä¼°å™¨ ====================
class ESM2IntegratedBBBEvaluator:
    """ESM2æ•´åˆBBBè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, config, logger):
        self.config = config
        self.logger = logger
        self.aa_props = AA_PROPERTIES
        
        # BBBè‚½ç‰¹å¾
        self.cationic_residues = ['R', 'K', 'H']
        self.hydrophobic_residues = ['W', 'F', 'L', 'I', 'V', 'A', 'M', 'Y']
        self.aromatic_residues = ['W', 'F', 'Y', 'H']
        
        # BBBç‰¹å¾æ¨¡å¼
        self.bbb_motifs = [
            'RRR', 'KKK', 'RKR', 'KRK', 'RWR', 'KWK', 'RK', 'KR', 
            'RW', 'WR', 'KW', 'WK', 'RF', 'FR', 'KF', 'FK', 'RH', 'HR'
        ]
        
        self.logger.info("ESM2æ•´åˆBBBè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def calculate_overall_score(self, seq: str, existing_sequences: List[str] = None) -> float:
        """è®¡ç®—æ•´åˆçš„æ€»ä½“è¯„åˆ†"""
        # å¿«é€Ÿæ·˜æ±°ä¸ç¬¦åˆåŸºæœ¬è¦æ±‚çš„åºåˆ—
        if len(seq) < self.config.target_length_min or len(seq) > self.config.target_length_max:
            return 0.0
        
        # è®¡ç®—å„é¡¹å¾—åˆ†
        length_score = self.calculate_length_score(seq)
        charge_score = self.calculate_charge_score(seq)
        mw_score = self.calculate_molecular_weight_score(seq)
        cationic_score = self.calculate_cationic_ratio_score(seq)
        hydrophobic_score = self.calculate_hydrophobic_ratio_score(seq)
        motif_score = self.calculate_bbb_motifs_score(seq)
        
        # å¿…éœ€æ¡ä»¶æ£€æŸ¥
        if length_score == 0.0 or charge_score == 0.0 or mw_score == 0.0:
            return 0.0
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = (
            length_score * self.config.property_weights['length_score'] +
            charge_score * self.config.property_weights['charge_score'] +
            mw_score * self.config.property_weights['molecular_weight'] +
            cationic_score * self.config.property_weights['cationic_ratio'] +
            hydrophobic_score * self.config.property_weights['hydrophobic_ratio'] +
            motif_score * self.config.property_weights['bbb_motifs']
        )
        
        return total_score
    
    def calculate_length_score(self, seq: str) -> float:
        """è®¡ç®—é•¿åº¦å¾—åˆ†"""
        length = len(seq)
        
        if length < self.config.target_length_min:
            return 0.0
        elif length > self.config.target_length_max:
            return 0.0
        elif length == self.config.target_length_optimal:
            return 1.0
        else:
            distance = abs(length - self.config.target_length_optimal)
            max_distance = max(
                self.config.target_length_optimal - self.config.target_length_min,
                self.config.target_length_max - self.config.target_length_optimal
            )
            return max(0.0, 1.0 - (distance / max_distance))
    
    def calculate_charge_score(self, seq: str) -> float:
        """è®¡ç®—ç”µè·å¾—åˆ†"""
        net_charge = sum(self.aa_props[aa]['charge'] for aa in seq if aa in self.aa_props)
        
        if net_charge < self.config.target_charge_min:
            return 0.0
        elif net_charge > self.config.target_charge_max:
            return 0.3
        elif net_charge == self.config.target_charge_optimal:
            return 1.0
        else:
            distance = abs(net_charge - self.config.target_charge_optimal)
            max_distance = max(
                self.config.target_charge_optimal - self.config.target_charge_min,
                self.config.target_charge_max - self.config.target_charge_optimal
            )
            return max(0.0, 1.0 - (distance / max_distance) * 0.2)
    
    def calculate_molecular_weight_score(self, seq: str) -> float:
        """è®¡ç®—åˆ†å­é‡å¾—åˆ†"""
        mw = sum(self.aa_props[aa]['mw'] for aa in seq if aa in self.aa_props)
        
        if mw < self.config.target_mw_min or mw > self.config.target_mw_max:
            return 0.0
        elif abs(mw - self.config.target_mw_optimal) <= 120:
            return 1.0
        else:
            distance = abs(mw - self.config.target_mw_optimal)
            max_distance = max(
                self.config.target_mw_optimal - self.config.target_mw_min,
                self.config.target_mw_max - self.config.target_mw_optimal
            )
            return max(0.0, 1.0 - (distance / max_distance) * 0.3)
    
    def calculate_cationic_ratio_score(self, seq: str) -> float:
        """è®¡ç®—é˜³ç¦»å­æ¯”ä¾‹å¾—åˆ†"""
        cationic_count = sum(1 for aa in seq if aa in self.cationic_residues)
        
        if cationic_count < self.config.min_cationic_residues:
            return 0.0
        elif cationic_count > self.config.max_cationic_residues:
            return 0.5
        else:
            ratio = cationic_count / len(seq)
            if 0.20 <= ratio <= 0.50:
                return 1.0
            elif 0.15 <= ratio <= 0.60:
                return 0.8
            else:
                return 0.6
    
    def calculate_hydrophobic_ratio_score(self, seq: str) -> float:
        """è®¡ç®—ç–æ°´æ¯”ä¾‹å¾—åˆ†"""
        hydrophobic_count = sum(1 for aa in seq if aa in self.hydrophobic_residues)
        
        if hydrophobic_count < self.config.min_hydrophobic_residues:
            return 0.0
        elif hydrophobic_count > self.config.max_hydrophobic_residues:
            return 0.5
        else:
            ratio = hydrophobic_count / len(seq)
            if 0.15 <= ratio <= 0.60:
                return 1.0
            elif 0.10 <= ratio <= 0.70:
                return 0.8
            else:
                return 0.6
    
    def calculate_bbb_motifs_score(self, seq: str) -> float:
        """è®¡ç®—BBBæ¨¡å¼å¾—åˆ†"""
        motif_score = 0
        for motif in self.bbb_motifs:
            if motif in seq:
                motif_score += 0.10
        
        return min(motif_score, 1.0)
    
    def analyze_sequence(self, seq: str) -> Dict:
        """åˆ†æåºåˆ—çš„è¯¦ç»†ç‰¹å¾"""
        analysis = {
            'sequence': seq,
            'length': len(seq),
            'molecular_weight': sum(self.aa_props[aa]['mw'] for aa in seq if aa in self.aa_props),
            'net_charge': sum(self.aa_props[aa]['charge'] for aa in seq if aa in self.aa_props),
            'cationic_count': sum(1 for aa in seq if aa in self.cationic_residues),
            'hydrophobic_count': sum(1 for aa in seq if aa in self.hydrophobic_residues),
            'aromatic_count': sum(1 for aa in seq if aa in self.aromatic_residues),
            'cationic_ratio': sum(1 for aa in seq if aa in self.cationic_residues) / len(seq),
            'hydrophobic_ratio': sum(1 for aa in seq if aa in self.hydrophobic_residues) / len(seq),
            'bbb_motifs_found': [motif for motif in self.bbb_motifs if motif in seq],
            'meets_all_requirements': self.meets_all_bbb_requirements(seq),
            'overall_score': self.calculate_overall_score(seq)
        }
        
        return analysis
    
    def meets_all_bbb_requirements(self, seq: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‰€æœ‰BBBè‚½è¦æ±‚"""
        # é•¿åº¦è¦æ±‚
        if not (self.config.target_length_min <= len(seq) <= self.config.target_length_max):
            return False
        
        # ç”µè·è¦æ±‚
        net_charge = sum(self.aa_props[aa]['charge'] for aa in seq if aa in self.aa_props)
        if not (self.config.target_charge_min <= net_charge <= self.config.target_charge_max):
            return False
        
        # åˆ†å­é‡è¦æ±‚
        mw = sum(self.aa_props[aa]['mw'] for aa in seq if aa in self.aa_props)
        if not (self.config.target_mw_min <= mw <= self.config.target_mw_max):
            return False
        
        # é˜³ç¦»å­æ®‹åŸºè¦æ±‚
        cationic_count = sum(1 for aa in seq if aa in self.cationic_residues)
        if not (self.config.min_cationic_residues <= cationic_count <= self.config.max_cationic_residues):
            return False
        
        # ç–æ°´æ®‹åŸºè¦æ±‚
        hydrophobic_count = sum(1 for aa in seq if aa in self.hydrophobic_residues)
        if not (self.config.min_hydrophobic_residues <= hydrophobic_count <= self.config.max_hydrophobic_residues):
            return False
        
        return True


# ==================== å»é‡å™¨ ====================
class ESM2RelaxedSequenceDeduplicator:
    """ESM2ä¼˜åŒ–çš„åºåˆ—å»é‡å™¨"""
    
    def __init__(self, similarity_threshold=0.78, existing_samples=None, logger=None):
        self.similarity_threshold = similarity_threshold
        self.logger = logger or logging.getLogger(__name__)
        self.existing_samples = existing_samples or []
        
        # å­˜å‚¨å·²ç”Ÿæˆçš„åºåˆ—
        self.generated_sequences = set()
        self.sequence_list = []
        self.sequence_hashes = set()
        
        # å°†ç°æœ‰æ ·æœ¬åŠ å…¥å»é‡æ£€æŸ¥
        for seq in self.existing_samples:
            self.generated_sequences.add(seq)
            self.sequence_list.append(seq)
            self.sequence_hashes.add(self.sequence_hash(seq))
        
        # ç›¸ä¼¼åº¦ç¼“å­˜
        self.similarity_cache = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_generated': 0,
            'duplicates_filtered': 0,
            'similar_filtered': 0,
            'unique_sequences': len(self.existing_samples),
            'hash_collisions': 0
        }
    
    def sequence_hash(self, seq: str) -> str:
        """ç”Ÿæˆåºåˆ—çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(seq.encode()).hexdigest()
    
    def calculate_similarity_fast(self, seq1: str, seq2: str) -> float:
        """å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—"""
        cache_key = (seq1, seq2) if seq1 <= seq2 else (seq2, seq1)
        if cache_key in self.similarity_cache:
            return self.similarity_cache[cache_key]
        
        if len(seq1) == 0 or len(seq2) == 0:
            return 0.0
        
        # é’ˆå¯¹çŸ­åºåˆ—ä¼˜åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        len_diff = abs(len(seq1) - len(seq2))
        max_len = max(len(seq1), len(seq2))
        
        if len_diff / max_len > 0.3:
            similarity = 0.0
        else:
            min_len = min(len(seq1), len(seq2))
            matches = sum(1 for i in range(min_len) if seq1[i] == seq2[i])
            
            length_penalty = len_diff / max_len * 0.15
            similarity = (matches / min_len) * (1 - length_penalty)
        
        # ç¼“å­˜ç»“æœ
        self.similarity_cache[cache_key] = similarity
        return similarity
    
    def is_similar_to_existing(self, new_seq: str, check_window: int = 150) -> bool:
        """æ£€æŸ¥æ–°åºåˆ—æ˜¯å¦ä¸ç°æœ‰åºåˆ—ç›¸ä¼¼"""
        if not self.sequence_list:
            return False
        
        # æ£€æŸ¥ä¸ç°æœ‰æ­£æ ·æœ¬çš„ç›¸ä¼¼åº¦
        for existing_seq in self.existing_samples:
            if self.calculate_similarity_fast(new_seq, existing_seq) >= self.similarity_threshold:
                return True
        
        # æ£€æŸ¥ä¸æœ€è¿‘ç”Ÿæˆåºåˆ—çš„ç›¸ä¼¼åº¦
        recent_sequences = self.sequence_list[-check_window:]
        for existing_seq in recent_sequences:
            if self.calculate_similarity_fast(new_seq, existing_seq) >= self.similarity_threshold:
                return True
        
        return False
    
    def add_sequence(self, seq: str) -> bool:
        """æ·»åŠ åºåˆ—ï¼Œå¦‚æœä¸é‡å¤åˆ™è¿”å›True"""
        self.stats['total_generated'] += 1
        
        # å¿«é€Ÿå“ˆå¸Œæ£€æŸ¥
        seq_hash = self.sequence_hash(seq)
        if seq_hash in self.sequence_hashes:
            self.stats['hash_collisions'] += 1
            return False
        
        # å®Œå…¨ç›¸åŒæ£€æŸ¥
        if seq in self.generated_sequences:
            self.stats['duplicates_filtered'] += 1
            return False
        
        # ç›¸ä¼¼åº¦æ£€æŸ¥
        if self.is_similar_to_existing(seq, config.duplicate_check_window):
            self.stats['similar_filtered'] += 1
            return False
        
        # æ·»åŠ åˆ°é›†åˆ
        self.generated_sequences.add(seq)
        self.sequence_list.append(seq)
        self.sequence_hashes.add(seq_hash)
        self.stats['unique_sequences'] += 1
        
        return True
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        new_sequences = stats['unique_sequences'] - len(self.existing_samples)
        if stats['total_generated'] > 0:
            stats['unique_rate'] = new_sequences / stats['total_generated']
            stats['duplicate_rate'] = stats['duplicates_filtered'] / stats['total_generated']
            stats['similar_rate'] = stats['similar_filtered'] / stats['total_generated']
        
        return stats


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•° - ESM2é›†æˆBBBç©¿é€è‚½æ­£æ ·æœ¬ç”Ÿæˆå™¨"""
    print("ğŸ§¬" + "=" * 79)
    print("ğŸ¯ ESM2é›†æˆBBBç©¿é€è‚½æ­£æ ·æœ¬ç”Ÿæˆå™¨")
    print("=" * 80)
    print("ğŸ“‹ ä»»åŠ¡æ¦‚å†µ:")
    print(f"   ğŸ‘¤ ç”¨æˆ·: {config.user_login}")
    print(f"   ğŸ“… æ—¶é—´: {config.current_time}")
    print(f"   ğŸ§¬ æ¶æ„: ESM2ç‰¹å¾æå– + æ‰©æ•£æ¨¡å‹ + Transformerç”Ÿæˆ")
    print(f"   â• ç°æœ‰æ­£æ ·æœ¬: {config.existing_positive_samples} ä¸ª")
    print(f"   â– ç°æœ‰è´Ÿæ ·æœ¬: {config.existing_negative_samples} ä¸ª")
    print(f"   ğŸ¯ éœ€è¦ç”Ÿæˆ: {config.need_to_generate} ä¸ªæ­£æ ·æœ¬")
    print(f"   âš–ï¸  ç›®æ ‡å¹³è¡¡: {config.target_positive_samples} æ­£æ ·æœ¬ vs {config.existing_negative_samples} è´Ÿæ ·æœ¬")
    print("=" * 80)
    print("ğŸ”§ ESM2é›†æˆç‰¹æ€§:")
    print("   âœ… ESM2è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ç‰¹å¾æå–")
    print("   âœ… æ‰©æ•£æ¨¡å‹ + Transformeræ¶æ„")
    print("   âœ… BBBå®é™…éœ€æ±‚: é•¿åº¦6-15aa, åˆ†å­é‡500-2000Da, å‡€ç”µè·+1åˆ°+6")
    print("   âœ… å¤šæ ·æ€§å¢å¼º: å»é‡ä¼˜åŒ–, æ¸©åº¦è‡ªé€‚åº”, æ ¸é‡‡æ ·")
    print("   âœ… è´¨é‡ä¿è¯: é˜³ç¦»å­æ®‹åŸº2-6ä¸ª, ç–æ°´æ®‹åŸº1-8ä¸ª")
    print("   âœ… æ™ºèƒ½é‡‡æ ·: ESM2å¼•å¯¼, BBBåç½®å¼•å¯¼, é‡å¤æƒ©ç½š")
    print("=" * 80)
    
    # æ£€æŸ¥ESM2å¯ç”¨æ€§
    if not ESM2_AVAILABLE:
        print("âŒ ESM2ä¸å¯ç”¨ï¼Œè¯·å®‰è£…ï¼špip install fair-esm")
        return
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(config.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("ğŸš€ å¼€å§‹ESM2é›†æˆBBBç©¿é€è‚½æ­£æ ·æœ¬ç”Ÿæˆ")
    logger.info(f"ğŸ¯ ç›®æ ‡: è¡¥é½æ­£æ ·æœ¬æ•°é‡ä» {config.existing_positive_samples} åˆ° {config.target_positive_samples}")
    logger.info(f"ğŸ“Š éœ€è¦ç”Ÿæˆ: {config.need_to_generate} ä¸ªé«˜è´¨é‡æ­£æ ·æœ¬")
    
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å¤„ç†
        logger.info("ğŸ“Š ç¬¬ä¸€é˜¶æ®µ: æ­£æ ·æœ¬æ•°æ®å¤„ç†")
        existing_samples = read_existing_positive_samples(config.input_file, logger)
        
        # æ›´æ–°é…ç½®
        config.existing_positive_samples = len(existing_samples)
        config.need_to_generate = config.target_positive_samples - len(existing_samples)
        
        if len(existing_samples) >= config.target_positive_samples:
            logger.info("âœ… ç°æœ‰æ­£æ ·æœ¬å·²è¶³å¤Ÿï¼Œæ— éœ€ç”Ÿæˆæ–°æ ·æœ¬")
            return
        
        # ç¬¬äºŒé˜¶æ®µï¼šESM2ç‰¹å¾æå–
        logger.info("ğŸ§¬ ç¬¬äºŒé˜¶æ®µ: ESM2ç‰¹å¾æå–")
        esm2_extractor = ESM2FeatureExtractor(config, logger)
        
        # ä¸ºæ‰€æœ‰æ ·æœ¬æå–ESM2ç‰¹å¾
        logger.info("ğŸ” ä¸ºè®­ç»ƒæ•°æ®æå–ESM2ç‰¹å¾")
        train_esm2_features = esm2_extractor.extract_features(existing_samples)
        
        # åˆ›å»ºæ•°æ®é›†
        logger.info("ğŸ“¦ åˆ›å»ºæ•°æ®é›†")
        dataset = ESM2IntegratedDataset(existing_samples, train_esm2_features, logger)
        
        # åˆ†å‰²æ•°æ®é›†
        n_val = int(len(dataset) * config.val_split)
        n_train = len(dataset) - n_val
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [n_train, n_val], 
            generator=torch.Generator().manual_seed(42)
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        def collate_fn(batch):
            sequences = [item['sequence'] for item in batch]
            encoded_sequences = torch.stack([item['encoded_sequence'] for item in batch])
            esm2_features = torch.stack([item['esm2_features'] for item in batch])
            
            return {
                'sequences': sequences,
                'encoded_sequences': encoded_sequences,
                'esm2_features': esm2_features
            }
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=config.batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=config.num_workers,
            pin_memory=config.pin_memory,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=config.num_workers,
            pin_memory=config.pin_memory,
            drop_last=False
        )
        
        logger.info(f"ğŸ“ˆ è®­ç»ƒé›†: {len(train_dataset)} ä¸ªåºåˆ—")
        logger.info(f"ğŸ“‰ éªŒè¯é›†: {len(val_dataset)} ä¸ªåºåˆ—")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒ
        logger.info("ğŸ¤– ç¬¬ä¸‰é˜¶æ®µ: ESM2é›†æˆæ¨¡å‹è®­ç»ƒ")
        model = ESM2IntegratedDiffusionModel(device=config.device).to(config.device)
        
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"ğŸ”§ æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        
        # è®­ç»ƒæ¨¡å‹
        train_esm2_model(model, train_loader, val_loader, config, logger)
        
        # ä¿å­˜æ¨¡å‹
        model_path = output_dir / "models" / config.model_save_path
        model_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(model.state_dict(), model_path)
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ç¬¬å››é˜¶æ®µï¼šæ­£æ ·æœ¬ç”Ÿæˆ
        logger.info("ğŸŒŸ ç¬¬å››é˜¶æ®µ: ESM2é›†æˆæ­£æ ·æœ¬ç”Ÿæˆ")
        evaluator = ESM2IntegratedBBBEvaluator(config, logger)
        
        all_new_sequences, high_quality_sequences = generate_esm2_positive_samples(
            model, esm2_extractor, evaluator, existing_samples, config, logger
        )
        
        # ç¬¬äº”é˜¶æ®µï¼šç»“æœä¿å­˜
        logger.info("ğŸ’¾ ç¬¬äº”é˜¶æ®µ: ç»“æœä¿å­˜")
        save_esm2_results(all_new_sequences, high_quality_sequences, existing_samples, 
                         evaluator, config, logger, output_dir)
        
        # æœ€ç»ˆç»“æœ
        logger.info("ğŸ†" + "=" * 79)
        logger.info("ğŸ‰ ESM2é›†æˆBBBç©¿é€è‚½æ­£æ ·æœ¬ç”Ÿæˆå®Œæˆ")
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š ç°æœ‰æ­£æ ·æœ¬: {len(existing_samples)}")
        logger.info(f"ğŸ“Š æ–°ç”Ÿæˆæ­£æ ·æœ¬: {len(all_new_sequences)}")
        logger.info(f"â­ é«˜è´¨é‡æ­£æ ·æœ¬: {len(high_quality_sequences)}")
        logger.info(f"ğŸ“Š æ€»æ­£æ ·æœ¬æ•°: {len(existing_samples) + len(all_new_sequences)}")
        logger.info(f"ğŸ“Š è´Ÿæ ·æœ¬æ•°: {config.existing_negative_samples}")
        
        logger.info("ğŸŠ ESM2é›†æˆBBBç©¿é€è‚½æ­£æ ·æœ¬ç”Ÿæˆå™¨æ‰§è¡Œå®Œæˆï¼")
        logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


# ==================== è¾…åŠ©å‡½æ•° ====================
def read_existing_positive_samples(file_path: str, logger) -> List[str]:
    """è¯»å–ç°æœ‰æ­£æ ·æœ¬"""
    sequences = []
    
    possible_files = [
        file_path,
        "train_pos_org.fasta",
        "positive_samples.fasta",
        "existing_positive.fasta"
    ]
    
    file_found = False
    for file in possible_files:
        if os.path.exists(file):
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    sequence = ""
                    for line in f:
                        line = line.strip()
                        if line.startswith('>'):
                            if sequence:
                                sequences.append(sequence)
                            sequence = ""
                        else:
                            sequence += line
                    if sequence:
                        sequences.append(sequence)
                
                logger.info(f"âœ… æˆåŠŸè¯»å–ç°æœ‰æ­£æ ·æœ¬æ–‡ä»¶: {file}")
                logger.info(f"ğŸ“Š ç°æœ‰æ­£æ ·æœ¬æ•°é‡: {len(sequences)}")
                file_found = True
                break
                
            except Exception as e:
                logger.warning(f"âŒ è¯»å–æ–‡ä»¶ {file} å¤±è´¥: {e}")
    
    if not file_found:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ç°æœ‰æ­£æ ·æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤BBBè‚½æ¨¡æ¿")
        sequences = create_default_positive_samples(logger)
    
    return sequences


def create_default_positive_samples(logger) -> List[str]:
    """åˆ›å»ºé»˜è®¤çš„æ­£æ ·æœ¬BBBè‚½"""
    logger.info("ğŸ§¬ åˆ›å»ºé»˜è®¤BBBç©¿é€è‚½æ­£æ ·æœ¬")
    
    default_samples = [
        "GRKKRRQRRR", "YGRKKRRQRRR", "RQIKIWFQNRR", "KLALKLALK",
        "RQARRNRRRR", "RRWWRRWW", "GRKKRRQRR", "YGRKKRRQ",
        "RWKWKW", "FKFKFK", "LRLRLR", "AWAWAW", "QWQWQW", "TWTWTW",
        "HRHRHH", "GKGKGK", "PLPLPL", "CYCYYY", "MEMEME", "SISISS",
        "RWKFLQ", "GKPLCY", "RWKWRW", "KFKFKF", "WKWKWK",
        "AWHRFK", "QWTYLY", "LRCYNE", "VIFKGP", "MESIDQ",
        "KRWKRW", "FRLFRL", "RLRLRL", "KFKFKF", "RWRWRW",
        "FLFLFL", "WLWLWL", "RKRKRK", "FWFWFW", "KYKYKF",
        "RWFRWF", "WFWFWF", "YFYFYF", "HWKHWK", "FKRFKR",
        "WKYWKY", "FRFRFR", "WFKRWF", "YKYKYF", "RWFRFW"
    ]
    
    with open(config.input_file, 'w', encoding='utf-8') as f:
        for i, seq in enumerate(default_samples):
            f.write(f">default_positive_{i + 1}\n{seq}\n")
    
    logger.info(f"ğŸ“ é»˜è®¤æ­£æ ·æœ¬å·²ä¿å­˜åˆ°: {config.input_file}")
    logger.info(f"ğŸ“Š åŒ…å« {len(default_samples)} ä¸ªé«˜è´¨é‡BBBè‚½åºåˆ—")
    return default_samples


def train_esm2_model(model, train_loader, val_loader, config, logger):
    """è®­ç»ƒESM2é›†æˆæ¨¡å‹"""
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒESM2é›†æˆBBBæ­£æ ·æœ¬ç”Ÿæˆæ¨¡å‹")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.lr,
        weight_decay=0.01,
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.n_epochs,
        eta_min=config.lr * 0.01
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None
    
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config.n_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.n_epochs}")):
            encoded_sequences = batch['encoded_sequences'].to(config.device)
            esm2_features = batch['esm2_features'].to(config.device)
            
            batch_size = encoded_sequences.size(0)
            
            # éšæœºæ—¶é—´æ­¥
            t = torch.randint(0, config.n_timesteps, (batch_size,), device=config.device)
            
            # å‰å‘ä¼ æ’­
            if config.mixed_precision:
                with torch.cuda.amp.autocast():
                    loss, _ = model.forward_with_loss(encoded_sequences, t, esm2_features)
            else:
                loss, _ = model.forward_with_loss(encoded_sequences, t, esm2_features)
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / config.gradient_accumulation_steps
            
            # åå‘ä¼ æ’­
            if config.mixed_precision:
                scaler.scale(loss).backward()
                
                if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                loss.backward()
                
                if (batch_idx + 1) % config.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)
                    optimizer.step()
                    optimizer.zero_grad()
            
            train_loss += loss.item() * config.gradient_accumulation_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for batch in val_loader:
                encoded_sequences = batch['encoded_sequences'].to(config.device)
                esm2_features = batch['esm2_features'].to(config.device)
                
                batch_size = encoded_sequences.size(0)
                t = torch.randint(0, config.n_timesteps, (batch_size,), device=config.device)
                
                if config.mixed_precision:
                    with torch.cuda.amp.autocast():
                        loss, _ = model.forward_with_loss(encoded_sequences, t, esm2_features)
                else:
                    loss, _ = model.forward_with_loss(encoded_sequences, t, esm2_features)
                
                val_loss += loss.item()
        
        # è®°å½•æŸå¤±
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # æ—©åœæ£€æŸ¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        # æ‰“å°ä¿¡æ¯
        if (epoch + 1) % 5 == 0:
            logger.info(f"Epoch {epoch + 1}: è®­ç»ƒæŸå¤±={train_loss:.4f}, éªŒè¯æŸå¤±={val_loss:.4f}")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= config.early_stopping_patience:
            logger.info(f"â¹ï¸ æ—©åœè§¦å‘ï¼åœ¨epoch {epoch + 1}åœæ­¢è®­ç»ƒ")
            break
        
        # æ˜¾å­˜ç®¡ç†
        if (epoch + 1) % 10 == 0:
            torch.cuda.empty_cache()
            gc.collect()
    
    logger.info("âœ… ESM2é›†æˆBBBæ­£æ ·æœ¬æ¨¡å‹è®­ç»ƒå®Œæˆ")
    logger.info(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


def generate_esm2_positive_samples(model, esm2_extractor, evaluator, existing_samples, config, logger):
    """ç”ŸæˆESM2é›†æˆæ­£æ ·æœ¬"""
    logger.info(f"ğŸ¯ å¼€å§‹ç”Ÿæˆ {config.need_to_generate} ä¸ªESM2é›†æˆBBBæ­£æ ·æœ¬")
    
    # åˆå§‹åŒ–å»é‡å™¨
    deduplicator = ESM2RelaxedSequenceDeduplicator(
        config.similarity_threshold,
        existing_samples,
        logger
    )
    
    # åˆå§‹åŒ–é‡‡æ ·å™¨
    diversity_sampler = ESM2IntegratedDiversitySampler(config, logger)
    
    model.eval()
    all_new_sequences = []
    high_quality_sequences = []
    
    target_sequences = config.need_to_generate
    batch_size = config.generation_batch_size
    
    with torch.no_grad():
        while len(all_new_sequences) < target_sequences:
            remaining = target_sequences - len(all_new_sequences)
            current_batch_size = min(batch_size, remaining)
            
            # ç”Ÿæˆåºåˆ—
            generated_sequences = sample_esm2_sequences(
                model, diversity_sampler, current_batch_size, config, logger
            )
            
            # è¯„ä¼°å’Œè¿‡æ»¤
            for seq in generated_sequences:
                if config.target_length_min <= len(seq) <= config.target_length_max:
                    # åŸºæœ¬BBBè‚½è¦æ±‚æ£€æŸ¥
                    cationic_count = sum(1 for aa in seq if aa in ['R', 'K', 'H'])
                    net_charge = sum(AA_PROPERTIES[aa]['charge'] for aa in seq if aa in AA_PROPERTIES)
                    
                    if cationic_count >= config.min_cationic_residues and net_charge >= config.target_charge_min:
                        if deduplicator.add_sequence(seq):
                            all_new_sequences.append(seq)
                            
                            # è´¨é‡è¯„ä¼°
                            score = evaluator.calculate_overall_score(seq)
                            if score >= config.quality_threshold:
                                high_quality_sequences.append((seq, score))
            
            # è¿›åº¦æ›´æ–°
            logger.info(f"ç”Ÿæˆè¿›åº¦: {len(all_new_sequences)}/{target_sequences}")
            
            if len(all_new_sequences) >= target_sequences:
                break
    
    # æ’åºé«˜è´¨é‡åºåˆ—
    high_quality_sequences.sort(key=lambda x: x[1], reverse=True)
    
    logger.info(f"ğŸ‰ ESM2é›†æˆæ­£æ ·æœ¬ç”Ÿæˆå®Œæˆ:")
    logger.info(f"ğŸ“Š æ–°ç”Ÿæˆåºåˆ—: {len(all_new_sequences)}")
    logger.info(f"â­ é«˜è´¨é‡åºåˆ—: {len(high_quality_sequences)}")
    
    return all_new_sequences, high_quality_sequences


def sample_esm2_sequences(model, diversity_sampler, batch_size, config, logger):
    """é‡‡æ ·ESM2é›†æˆåºåˆ—"""
    device = config.device
    
    # ä»éšæœºåºåˆ—å¼€å§‹
    x = torch.randint(0, config.vocab_size, (batch_size, config.seq_len), device=device)
    
    # åˆ›å»ºè™šæ‹ŸESM2ç‰¹å¾ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯ä»å‚è€ƒåºåˆ—æˆ–æ¨¡æ¿ç”Ÿæˆçš„ç‰¹å¾ï¼‰
    dummy_esm2_features = torch.randn(batch_size, config.seq_len, config.esm2_embedding_dim, device=device)
    
    # é‡‡æ ·æ­¥éª¤
    sampling_steps = torch.linspace(config.n_timesteps - 1, 0, config.sampling_steps).long()
    
    # é€æ­¥å»å™ª
    for i, t in enumerate(sampling_steps):
        t_tensor = torch.full((batch_size,), t.item(), device=device, dtype=torch.long)
        
        # é¢„æµ‹
        predicted_logits = model.forward(x, t_tensor, dummy_esm2_features)
        
        # ä½¿ç”¨å¤šæ ·æ€§é‡‡æ ·
        x = diversity_sampler.enhanced_sampling(
            predicted_logits, x, i, len(sampling_steps)
        )
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²åºåˆ—
    sequences = []
    for j in range(batch_size):
        seq = indices_to_sequence(x[j].cpu().numpy())
        seq = seq.rstrip('A')  # ç§»é™¤å¡«å……
        sequences.append(seq)
    
    return sequences


def indices_to_sequence(indices: List[int]) -> str:
    """å°†ç´¢å¼•è½¬æ¢ä¸ºåºåˆ—"""
    return ''.join([IDX_TO_AA.get(idx, 'A') for idx in indices])


def save_esm2_results(all_new_sequences, high_quality_sequences, existing_samples, 
                     evaluator, config, logger, output_dir):
    """ä¿å­˜ESM2é›†æˆç»“æœ"""
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = output_dir / "results"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ–°ç”Ÿæˆçš„åºåˆ—
    if all_new_sequences:
        with open(results_dir / "new_positive_sequences.fasta", 'w') as f:
            for i, seq in enumerate(all_new_sequences):
                f.write(f">new_positive_{i+1}\n{seq}\n")
        logger.info(f"ğŸ’¾ æ–°ç”Ÿæˆåºåˆ—å·²ä¿å­˜: {len(all_new_sequences)} ä¸ª")
    
    # ä¿å­˜é«˜è´¨é‡åºåˆ—
    if high_quality_sequences:
        with open(results_dir / "high_quality_positive.fasta", 'w') as f:
            for i, (seq, score) in enumerate(high_quality_sequences):
                f.write(f">high_quality_{i+1}_score_{score:.3f}\n{seq}\n")
        logger.info(f"ğŸ’¾ é«˜è´¨é‡åºåˆ—å·²ä¿å­˜: {len(high_quality_sequences)} ä¸ª")
    
    # ä¿å­˜åˆå¹¶çš„å®Œæ•´æ­£æ ·æœ¬æ•°æ®é›†
    all_positive_sequences = existing_samples + [seq for seq, _ in high_quality_sequences]
    
    with open(results_dir / "all_positive_samples.fasta", 'w') as f:
        for i, seq in enumerate(existing_samples):
            f.write(f">existing_positive_{i+1}\n{seq}\n")
        for i, (seq, score) in enumerate(high_quality_sequences):
            f.write(f">generated_positive_{i+1}_score_{score:.3f}\n{seq}\n")
    
    logger.info(f"ğŸ’¾ å®Œæ•´æ­£æ ·æœ¬æ•°æ®é›†å·²ä¿å­˜: {len(all_positive_sequences)} ä¸ª")
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    analysis_results = []
    for seq, score in high_quality_sequences[:50]:  # åˆ†æå‰50ä¸ª
        analysis = evaluator.analyze_sequence(seq)
        analysis_results.append(analysis)
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    report = {
        'existing_samples': len(existing_samples),
        'new_samples_generated': len(all_new_sequences),
        'high_quality_samples': len(high_quality_sequences),
        'total_positive_samples': len(all_positive_sequences),
        'average_quality_score': np.mean([score for _, score in high_quality_sequences]) if high_quality_sequences else 0,
        'bbb_compliant': sum(1 for a in analysis_results if a['meets_all_requirements']),
        'average_length': np.mean([a['length'] for a in analysis_results]) if analysis_results else 0,
        'average_mw': np.mean([a['molecular_weight'] for a in analysis_results]) if analysis_results else 0,
        'average_charge': np.mean([a['net_charge'] for a in analysis_results]) if analysis_results else 0,
        'generation_time': datetime.now().isoformat(),
        'config_summary': {
            'esm2_model': config.esm2_model_name,
            'target_length_range': f"{config.target_length_min}-{config.target_length_max}",
            'target_mw_range': f"{config.target_mw_min}-{config.target_mw_max}",
            'target_charge_range': f"{config.target_charge_min}-{config.target_charge_max}",
            'quality_threshold': config.quality_threshold,
            'similarity_threshold': config.similarity_threshold
        }
    }
    
    with open(results_dir / "generation_report.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info("ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜")


if __name__ == "__main__":
    main()