#!/usr/bin/env python3
"""
è›‹ç™½è´¨-åŒ–åˆç‰©ç‰¹å¾æå–è„šæœ¬ (ä¿®å¤æ’åºç‰ˆæœ¬)
ä½œè€…: woyaokaoyanhaha
ç‰ˆæœ¬: 11.0
æ—¥æœŸ: 2025-06-16 04:25:34
ä¿®å¤: ä¿æŒè¾“å‡ºæ–‡ä»¶ä¸è¾“å…¥æ–‡ä»¶å®Œå…¨ä¸€è‡´çš„é¡ºåº
"""

import csv
import os
import subprocess
import sys
import json
import time
import warnings
import argparse
import glob
import traceback
from pathlib import Path
from collections import defaultdict

warnings.filterwarnings('ignore')

# æ£€æŸ¥å’Œå¯¼å…¥ä¾èµ–åº“
def check_dependencies():
    """æ£€æŸ¥å¹¶å¯¼å…¥å¿…è¦çš„ä¾èµ–åº“"""
    print("æ­£åœ¨æ£€æŸ¥ä¾èµ–åº“...")
    
    try:
        import numpy as np
        print(f"âœ… numpy {np.__version__}")
    except ImportError:
        print("âŒ numpy æœªå®‰è£…")
        return False
    
    try:
        from Bio import SeqIO
        from Bio.Seq import Seq
        from Bio.SeqRecord import SeqRecord
        print("âœ… biopython")
    except ImportError:
        print("âŒ biopython æœªå®‰è£…")
        return False
    
    try:
        import pandas as pd
        print(f"âœ… pandas {pd.__version__}")
    except ImportError:
        print("âŒ pandas æœªå®‰è£…")
        return False
    
    try:
        from rdkit import Chem
        from rdkit.Chem import Descriptors, Lipinski, MolSurf, QED, GraphDescriptors, Crippen, AllChem
        print("âœ… rdkit")
    except ImportError:
        print("âŒ rdkit æœªå®‰è£…")
        return False
    
    print("ğŸ‰ æ‰€æœ‰ä¾èµ–åº“æ£€æŸ¥å®Œæˆ")
    return True

# æ£€æŸ¥ä¾èµ–åº“
if not check_dependencies():
    print("\nè¯·å®‰è£…ç¼ºå¤±çš„ä¾èµ–åº“:")
    print("pip install biopython numpy pandas rdkit")
    sys.exit(1)

# ç°åœ¨å®‰å…¨å¯¼å…¥æ‰€æœ‰åº“
import numpy as np
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski, MolSurf, QED, GraphDescriptors, Crippen, AllChem

# é…ç½®å‚æ•°
COLUMN_MAPPING = {
    'protein_accession': ['Protein_Accession', 'ProteinAccession', 'Accession', 'Protein_ID', 'ProteinID'],
    'sequence': ['Sequence', 'Protein_Sequence', 'ProteinSequence', 'Seq'],
    'compound_cid': ['Compound_CID', 'CompoundCID', 'CID', 'Compound_ID', 'CompoundID'],
    'smile': ['Smile', 'SMILES', 'Canonical_SMILES', 'CanonicalSMILES'],
    'label': ['label', 'Label', 'Class', 'Target', 'Y']
}

PSEAAC_LAMBDA = 10
PSEAAC_TOTAL_DIM = 51
PSEPSSM_TOTAL_DIM = 220
PSEPSSM_LAMBDA = 10
PSSM_BASIC_FEATURES = 80

MOLECULAR_DESCRIPTORS = [
    'MW', 'ExactMW', 'LogP', 'HBA', 'HBD', 'RotBonds', 'AromaticRings', 
    'RingCount', 'HeteroRings', 'SaturatedRings', 'AliphaticRings', 
    'HeavyAtomCount', 'AtomCount', 'TPSA', 'LabuteASA', 'MolSurfaceArea', 
    'BertzCT', 'Chi0v', 'Chi1v', 'Kappa1', 'Kappa2', 'Kappa3',
    'LipinskiViolations', 'QED', 'RO5_Violations', 'GhoseViolations', 
    'VeberViolations', 'CarbonCount', 'NitrogenCount', 'OxygenCount', 
    'SulfurCount', 'PhosphorusCount', 'FluorineCount', 'ChlorineCount', 
    'BromineCount', 'IodineCount', 'HalogenCount', 'FractionCSP3',
    'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'SMR_VSA1', 'SMR_VSA2', 
    'SlogP_VSA1', 'SlogP_VSA2', 'MR', 'NHOH_Count', 'NO_Count', 
    'HallKierAlpha', 'ValenceElectrons', 'MolLogP', 'MolMR',
    'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO',
    'SP3_N_Count', 'SP2_N_Count', 'Amide_Count', 'Ester_Count', 
    'Carboxylic_Count', 'Ether_Count', 'Alcohol_Count', 'Amine_Count'
]

COMPOUND_TOTAL_DIM = len(MOLECULAR_DESCRIPTORS)
PSIBLAST_ITERATIONS = 3
PSIBLAST_EVALUE = 0.001
PSIBLAST_THREADS = 4
SAVE_PROGRESS_INTERVAL = 10

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='è›‹ç™½è´¨-åŒ–åˆç‰©ç‰¹å¾æå–è„šæœ¬ (ä¿®å¤æ’åºç‰ˆæœ¬)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python3 extract_protein_compound_features_v11.py data.csv ./swissprot_db/uniprot_sprot
  python3 extract_protein_compound_features_v11.py data.csv ./db/uniprot_sprot -o ./output
  python3 extract_protein_compound_features_v11.py --list-resume
        """
    )
    
    parser.add_argument('input_csv', nargs='?', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('swissprot_db', nargs='?', help='SwissProtæ•°æ®åº“è·¯å¾„')
    parser.add_argument('-o', '--output', help='è‡ªå®šä¹‰è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('-r', '--resume', help='ä»æŒ‡å®šç›®å½•æ¢å¤è¿è¡Œ')
    parser.add_argument('--list-resume', action='store_true', help='åˆ—å‡ºå¯æ¢å¤çš„è¿è¡Œç›®å½•')
    parser.add_argument('--test', action='store_true', help='è¿è¡Œæµ‹è¯•æ¨¡å¼')
    parser.add_argument('--preserve-order', action='store_true', default=True, help='ä¿æŒè¾“å…¥æ–‡ä»¶é¡ºåºï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    
    return parser.parse_args()

def detect_column_names(csv_file):
    """è‡ªåŠ¨æ£€æµ‹CSVæ–‡ä»¶çš„åˆ—å"""
    print(f"æ­£åœ¨æ£€æµ‹CSVæ–‡ä»¶çš„åˆ—å: {csv_file}")
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–CSVæ–‡ä»¶: {e}")
        return None, None
    
    detected_columns = {}
    
    for field_type, possible_names in COLUMN_MAPPING.items():
        detected_columns[field_type] = None
        
        for possible_name in possible_names:
            if possible_name in header:
                detected_columns[field_type] = possible_name
                break
    
    print("æ£€æµ‹åˆ°çš„åˆ—åæ˜ å°„:")
    for field_type, column_name in detected_columns.items():
        status = "âœ…" if column_name else "âŒ"
        print(f"  {status} {field_type}: {column_name or 'æœªæ‰¾åˆ°'}")
    
    return detected_columns, header

def list_resumable_directories():
    """åˆ—å‡ºå¯æ¢å¤çš„è¿è¡Œç›®å½•"""
    print("æœç´¢å¯æ¢å¤çš„è¿è¡Œç›®å½•...")
    
    patterns = ["*_features_*"]
    found_dirs = []
    
    for pattern in patterns:
        dirs = glob.glob(pattern)
        for d in dirs:
            if os.path.isdir(d):
                found_dirs.append(d)
    
    if not found_dirs:
        print("âŒ æœªæ‰¾åˆ°å¯æ¢å¤çš„è¿è¡Œç›®å½•")
        return
    
    print(f"æ‰¾åˆ° {len(found_dirs)} ä¸ªå¯èƒ½çš„ç›®å½•:")
    
    for i, dir_path in enumerate(sorted(found_dirs), 1):
        progress_file = os.path.join(dir_path, "progress.json")
        print(f"\n{i}. ğŸ“ {dir_path}")
        
        if os.path.exists(progress_file):
            try:
                with open(progress_file, 'r') as f:
                    progress = json.load(f)
                status = "âœ… å®Œæˆ" if progress.get('completed', False) else "â³ æœªå®Œæˆ"
                print(f"   çŠ¶æ€: {status}")
                
                if not progress.get('completed', False):
                    proteins_done = progress.get('proteins_processed', 0)
                    proteins_total = progress.get('total_proteins', 0)
                    compounds_done = progress.get('compounds_processed', 0)
                    compounds_total = progress.get('total_compounds', 0)
                    
                    print(f"   è›‹ç™½è´¨: {proteins_done}/{proteins_total}")
                    print(f"   åŒ–åˆç‰©: {compounds_done}/{compounds_total}")
                    
            except Exception as e:
                print(f"   âŒ è¿›åº¦æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        else:
            print(f"   âŒ æ— è¿›åº¦æ–‡ä»¶")

def get_output_dir_name(input_csv_path, custom_output=None):
    """ç”Ÿæˆè¾“å‡ºç›®å½•å"""
    if custom_output:
        return custom_output
    
    filename = os.path.basename(input_csv_path)
    basename = os.path.splitext(filename)[0]
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    safe_basename = "".join(c for c in basename if c.isalnum() or c in ('-', '_')).rstrip()
    
    if len(safe_basename) < 3:
        safe_basename = "protein_compound_features"
    
    return f"./{safe_basename}_features_{timestamp}"

class ProgressManager:
    """è¿›åº¦ç®¡ç†å™¨"""
    
    def __init__(self, work_dir):
        self.work_dir = work_dir
        self.progress_file = os.path.join(work_dir, "progress.json")
        self.progress = self.load_progress()
    
    def load_progress(self):
        """åŠ è½½è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ è¿›åº¦æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        
        return {
            'start_time': time.time(),
            'proteins_processed': 0,
            'compounds_processed': 0,
            'total_proteins': 0,
            'total_compounds': 0,
            'protein_features_completed': [],
            'compound_features_completed': [],
            'completed': False,
            'last_update': time.time()
        }
    
    def save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        self.progress['last_update'] = time.time()
        try:
            with open(self.progress_file, 'w') as f:
                json.dump(self.progress, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ è¿›åº¦ä¿å­˜å¤±è´¥: {e}")
    
    def update_totals(self, total_proteins, total_compounds):
        """æ›´æ–°æ€»æ•°"""
        self.progress['total_proteins'] = total_proteins
        self.progress['total_compounds'] = total_compounds
        self.save_progress()
    
    def mark_protein_completed(self, accession):
        """æ ‡è®°è›‹ç™½è´¨å¤„ç†å®Œæˆ"""
        if accession not in self.progress['protein_features_completed']:
            self.progress['protein_features_completed'].append(accession)
            self.progress['proteins_processed'] = len(self.progress['protein_features_completed'])
    
    def mark_compound_completed(self, compound_cid):
        """æ ‡è®°åŒ–åˆç‰©å¤„ç†å®Œæˆ"""
        if compound_cid not in self.progress['compound_features_completed']:
            self.progress['compound_features_completed'].append(compound_cid)
            self.progress['compounds_processed'] = len(self.progress['compound_features_completed'])
    
    def is_protein_completed(self, accession):
        """æ£€æŸ¥è›‹ç™½è´¨æ˜¯å¦å·²å¤„ç†"""
        return accession in self.progress['protein_features_completed']
    
    def is_compound_completed(self, compound_cid):
        """æ£€æŸ¥åŒ–åˆç‰©æ˜¯å¦å·²å¤„ç†"""
        return compound_cid in self.progress['compound_features_completed']
    
    def mark_completed(self):
        """æ ‡è®°å…¨éƒ¨å®Œæˆ"""
        self.progress['completed'] = True
        self.progress['end_time'] = time.time()
        self.save_progress()
    
    def get_progress_info(self):
        """è·å–è¿›åº¦ä¿¡æ¯"""
        return {
            'proteins': f"{self.progress['proteins_processed']}/{self.progress['total_proteins']}",
            'compounds': f"{self.progress['compounds_processed']}/{self.progress['total_compounds']}",
            'protein_percent': (self.progress['proteins_processed'] / max(1, self.progress['total_proteins'])) * 100,
            'compound_percent': (self.progress['compounds_processed'] / max(1, self.progress['total_compounds'])) * 100
        }

class FeatureExtractor:
    """ç‰¹å¾æå–å™¨ä¸»ç±» - ä¿®å¤æ’åºç‰ˆæœ¬"""
    
    def __init__(self, swissprot_db, work_dir, input_filename, resume_mode=False, preserve_order=True):
        self.swissprot_db = swissprot_db
        self.work_dir = work_dir
        self.input_filename = input_filename
        self.resume_mode = resume_mode
        self.preserve_order = preserve_order  # æ–°å¢ï¼šä¿æŒé¡ºåºæ ‡å¿—
        self.temp_dir = os.path.join(work_dir, "temp")
        self.output_dir = os.path.join(work_dir, "output")
        self.cache_dir = os.path.join(work_dir, "cache")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        for dir_path in [self.temp_dir, self.output_dir, self.cache_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
        self.progress_manager = ProgressManager(work_dir)
        
        # å­˜å‚¨æ•°æ® - ä¿æŒåŸå§‹é¡ºåº
        self.original_records = []  # æ–°å¢ï¼šä¿å­˜åŸå§‹è®°å½•çš„å®Œæ•´é¡ºåº
        self.unique_proteins = {}
        self.unique_compounds = {}
        self.column_mapping = {}
        
        print(f"ğŸ“ å·¥ä½œç›®å½•: {work_dir}")
        print(f"ğŸ”„ è¿è¡Œæ¨¡å¼: {'æ¢å¤è¿è¡Œ' if resume_mode else 'æ–°å»ºè¿è¡Œ'}")
        print(f"ğŸ“‹ ä¿æŒé¡ºåº: {'æ˜¯' if preserve_order else 'å¦'}")
        
        if resume_mode:
            progress_info = self.progress_manager.get_progress_info()
            print(f"å½“å‰è¿›åº¦:")
            print(f"  è›‹ç™½è´¨: {progress_info['proteins']} ({progress_info['protein_percent']:.1f}%)")
            print(f"  åŒ–åˆç‰©: {progress_info['compounds']} ({progress_info['compound_percent']:.1f}%)")
    
    def load_and_deduplicate(self, input_csv):
        """åŠ è½½æ•°æ®å¹¶å»é‡ - ä¿æŒåŸå§‹é¡ºåº"""
        print("\n" + "="*60)
        print("ğŸ“‚ æ•°æ®åŠ è½½å’Œå»é‡é˜¶æ®µ (ä¿æŒåŸå§‹é¡ºåº)")
        print("="*60)
        
        detected_columns, header = detect_column_names(input_csv)
        if not detected_columns:
            return 0, 0
        
        self.column_mapping = detected_columns
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_fields = ['protein_accession', 'sequence', 'compound_cid', 'smile']
        missing_fields = [field for field in required_fields if not detected_columns[field]]
        
        if missing_fields:
            print(f"\nâŒ é”™è¯¯: æœªæ‰¾åˆ°å¿…éœ€çš„åˆ—: {missing_fields}")
            print(f"å¯ç”¨çš„åˆ—: {header}")
            return 0, 0
        
        print("\næ­£åœ¨åŠ è½½æ•°æ®å¹¶å»é‡ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰...")
        
        # ä¿å­˜åŸå§‹è®°å½•é¡ºåºçš„å…³é”®ä¿®æ”¹
        self.original_records = []  # æŒ‰è¾“å…¥æ–‡ä»¶é¡ºåºä¿å­˜æ‰€æœ‰è®°å½•
        seen_proteins = set()
        seen_compounds = set()
        row_number = 0
        
        try:
            with open(input_csv, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                for row in reader:
                    row_number += 1
                    
                    accession = row[detected_columns['protein_accession']].strip()
                    sequence = row[detected_columns['sequence']].strip().upper()
                    compound_cid = row[detected_columns['compound_cid']].strip()
                    smile = row[detected_columns['smile']].strip()
                    
                    label = ''
                    if detected_columns['label']:
                        label = row[detected_columns['label']].strip()
                    
                    # å…³é”®ä¿®æ”¹ï¼šä¿å­˜æ¯æ¡åŸå§‹è®°å½•ï¼ŒåŒ…å«ç²¾ç¡®çš„è¡Œå·
                    original_record = {
                        'original_row_number': row_number,  # ä»1å¼€å§‹çš„åŸå§‹è¡Œå·
                        'accession': accession,
                        'sequence': sequence,
                        'compound_cid': compound_cid,
                        'smile': smile,
                        'label': label
                    }
                    self.original_records.append(original_record)
                    
                    # æ”¶é›†å”¯ä¸€çš„è›‹ç™½è´¨ï¼ˆä½†è®°å½•é¦–æ¬¡å‡ºç°çš„è¡Œå·ï¼‰
                    if accession not in seen_proteins:
                        self.unique_proteins[accession] = {
                            'accession': accession,
                            'sequence': sequence,
                            'first_occurrence_row': row_number
                        }
                        seen_proteins.add(accession)
                    
                    # æ”¶é›†å”¯ä¸€çš„åŒ–åˆç‰©ï¼ˆä½†è®°å½•é¦–æ¬¡å‡ºç°çš„è¡Œå·ï¼‰
                    if compound_cid not in seen_compounds:
                        self.unique_compounds[compound_cid] = {
                            'compound_cid': compound_cid,
                            'smile': smile,
                            'first_occurrence_row': row_number
                        }
                        seen_compounds.add(compound_cid)
                        
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return 0, 0
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»è®°å½•æ•°: {len(self.original_records)}")
        print(f"  å”¯ä¸€è›‹ç™½è´¨æ•°: {len(self.unique_proteins)}")
        print(f"  å”¯ä¸€åŒ–åˆç‰©æ•°: {len(self.unique_compounds)}")
        print(f"  ä¿æŒåŸå§‹é¡ºåº: âœ…")
        
        self.progress_manager.update_totals(len(self.unique_proteins), len(self.unique_compounds))
        
        return len(self.unique_proteins), len(self.unique_compounds)
    
    def extract_pseaac_features(self, sequence):
        """æå–PSE-AACç‰¹å¾"""
        # æ°¨åŸºé…¸ç‰©ç†åŒ–å­¦æ€§è´¨ [ç–æ°´æ€§, ç­‰ç”µç‚¹, åˆ†å­é‡]
        aa_properties = {
            'A': [1.8, 6.0, 89.1], 'R': [-4.5, 10.8, 174.2], 'N': [-3.5, 5.4, 132.1],
            'D': [-3.5, 3.0, 133.1], 'C': [2.5, 5.1, 121.0], 'Q': [-3.5, 5.7, 146.1],
            'E': [-3.5, 4.2, 147.1], 'G': [-0.4, 6.0, 75.1], 'H': [-3.2, 7.6, 155.2],
            'I': [4.5, 6.0, 131.2], 'L': [3.8, 6.0, 131.2], 'K': [-3.9, 9.7, 146.2],
            'M': [1.9, 5.7, 149.2], 'F': [2.8, 5.5, 165.2], 'P': [-1.6, 6.3, 115.1],
            'S': [-0.8, 5.7, 105.1], 'T': [-0.7, 5.6, 119.1], 'V': [4.2, 6.0, 117.1],
            'W': [-0.9, 5.9, 204.2], 'Y': [-1.3, 5.7, 181.2]
        }
        
        # æ ‡å‡†åŒ–
        all_props = np.array([aa_properties[aa] for aa in aa_properties])
        means = np.mean(all_props, axis=0)
        stds = np.std(all_props, axis=0)
        
        for aa in aa_properties:
            aa_properties[aa] = [(aa_properties[aa][i] - means[i]) / stds[i] for i in range(3)]
        
        features = [len(sequence)]  # åºåˆ—é•¿åº¦
        
        # æ°¨åŸºé…¸ç»„æˆ
        amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 
                      'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']
        
        aa_count = {aa: 0 for aa in amino_acids}
        for aa in sequence:
            if aa in aa_count:
                aa_count[aa] += 1
        
        total = len(sequence)
        for aa in amino_acids:
            features.append(aa_count[aa] / total if total > 0 else 0)
        
        # ä¼ªæ°¨åŸºé…¸ç»„æˆ
        for property_idx in range(3):
            pseudo_comp = []
            lambda_val = min(PSEAAC_LAMBDA, len(sequence) - 1)
            
            for lag in range(1, lambda_val + 1):
                theta = 0
                count = 0
                for j in range(len(sequence) - lag):
                    aa1, aa2 = sequence[j], sequence[j + lag]
                    if aa1 in aa_properties and aa2 in aa_properties:
                        theta += (aa_properties[aa1][property_idx] - aa_properties[aa2][property_idx]) ** 2
                        count += 1
                
                theta = theta / count if count > 0 else 0
                pseudo_comp.append(theta)
            
            while len(pseudo_comp) < PSEAAC_LAMBDA:
                pseudo_comp.append(0.0)
            
            features.extend(pseudo_comp)
        
        return features
    
    def run_psiblast(self, fasta_file, accession):
        """è¿è¡ŒPSI-BLAST"""
        safe_acc = accession.replace('/', '_').replace('\\', '_').replace('|', '_')
        pssm_file = os.path.join(self.cache_dir, f"{safe_acc}.pssm")
        
        if os.path.exists(pssm_file):
            return pssm_file
        
        cmd = [
            "psiblast",
            "-query", fasta_file,
            "-db", self.swissprot_db,
            "-num_iterations", str(PSIBLAST_ITERATIONS),
            "-evalue", str(PSIBLAST_EVALUE),
            "-out_ascii_pssm", pssm_file,
            "-num_threads", str(PSIBLAST_THREADS)
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True, text=True)
            return pssm_file if os.path.exists(pssm_file) else None
        except subprocess.CalledProcessError:
            print(f"âš ï¸ {accession} PSI-BLASTè¿è¡Œå¤±è´¥")
            return None
    
    def extract_psepssm_features(self, pssm_file):
        """æå–PSE-PSSMç‰¹å¾"""
        if not pssm_file or not os.path.exists(pssm_file):
            return [0.0] * PSEPSSM_TOTAL_DIM
        
        try:
            pssm_matrix = []
            with open(pssm_file, 'r') as f:
                lines = f.readlines()
            
            start_reading = False
            for line in lines:
                if line.strip() and len(line.split()) > 0 and line.split()[0].isdigit():
                    start_reading = True
                    parts = line.strip().split()
                    if len(parts) >= 22:
                        try:
                            scores = [float(x) for x in parts[2:22]]
                            pssm_matrix.append(scores)
                        except ValueError:
                            continue
                elif start_reading and (not line.strip() or line.strip().startswith('Lambda')):
                    break
            
            if not pssm_matrix:
                return [0.0] * PSEPSSM_TOTAL_DIM
            
            pssm_array = np.array(pssm_matrix)
            features = []
            
            # PSSMåŸºç¡€ç»Ÿè®¡ç‰¹å¾
            pssm_mean = np.mean(pssm_array, axis=0)
            features.extend(pssm_mean.tolist())
            
            pssm_std = np.std(pssm_array, axis=0)
            features.extend(pssm_std.tolist())
            
            pssm_max = np.max(pssm_array, axis=0)
            features.extend(pssm_max.tolist())
            
            pssm_min = np.min(pssm_array, axis=0)
            features.extend(pssm_min.tolist())
            
            # ä¼ªPSSMç‰¹å¾
            seq_len = len(pssm_array)
            lambda_val = min(PSEPSSM_LAMBDA, seq_len - 1)
            
            remaining_dims = PSEPSSM_TOTAL_DIM - len(features)
            features_per_lag = remaining_dims // lambda_val if lambda_val > 0 else 0
            
            for lag in range(1, lambda_val + 1):
                lag_features = []
                for aa_idx in range(20):
                    if len(lag_features) < features_per_lag:
                        theta = 0
                        count = 0
                        for pos in range(seq_len - lag):
                            theta += (pssm_array[pos][aa_idx] - pssm_array[pos + lag][aa_idx]) ** 2
                            count += 1
                        
                        theta = theta / count if count > 0 else 0
                        lag_features.append(theta)
                
                features.extend(lag_features)
            
            while len(features) < PSEPSSM_TOTAL_DIM:
                features.append(0.0)
            
            return features[:PSEPSSM_TOTAL_DIM]
            
        except Exception as e:
            print(f"âš ï¸ PSSMç‰¹å¾æå–é”™è¯¯: {e}")
            return [0.0] * PSEPSSM_TOTAL_DIM
    
    def extract_compound_features(self, smile):
        """æå–åŒ–åˆç‰©ç‰¹å¾"""
        try:
            mol = Chem.MolFromSmiles(smile)
            if mol is None:
                return [0.0] * COMPOUND_TOTAL_DIM
            
            descriptors = {}
            
            # åŸºæœ¬æ€§è´¨
            descriptors['MW'] = Descriptors.MolWt(mol)
            descriptors['ExactMW'] = Descriptors.ExactMolWt(mol)
            descriptors['LogP'] = Descriptors.MolLogP(mol)
            descriptors['HBA'] = Descriptors.NumHAcceptors(mol)
            descriptors['HBD'] = Descriptors.NumHDonors(mol)
            descriptors['RotBonds'] = Descriptors.NumRotatableBonds(mol)
            descriptors['AromaticRings'] = Descriptors.NumAromaticRings(mol)
            descriptors['RingCount'] = Descriptors.RingCount(mol)
            descriptors['HeteroRings'] = Descriptors.NumHeterocycles(mol)
            descriptors['SaturatedRings'] = Descriptors.NumSaturatedRings(mol)
            descriptors['AliphaticRings'] = Descriptors.NumAliphaticRings(mol)
            descriptors['HeavyAtomCount'] = mol.GetNumHeavyAtoms()
            descriptors['AtomCount'] = mol.GetNumAtoms()
            descriptors['TPSA'] = MolSurf.TPSA(mol)
            descriptors['LabuteASA'] = MolSurf.LabuteASA(mol)
            
            # åˆ†å­è¡¨é¢ç§¯è®¡ç®—
            try:
                mol_3d = Chem.AddHs(mol)
                success = AllChem.EmbedMolecule(mol_3d, randomSeed=42)
                if success == 0:
                    AllChem.UFFOptimizeMolecule(mol_3d)
                    descriptors['MolSurfaceArea'] = AllChem.ComputeMolVolume(mol_3d)
                else:
                    descriptors['MolSurfaceArea'] = 0.0
            except:
                descriptors['MolSurfaceArea'] = 0.0
            
            descriptors['BertzCT'] = GraphDescriptors.BertzCT(mol)
            descriptors['Chi0v'] = GraphDescriptors.Chi0v(mol)
            descriptors['Chi1v'] = GraphDescriptors.Chi1v(mol)
            descriptors['Kappa1'] = GraphDescriptors.Kappa1(mol)
            descriptors['Kappa2'] = GraphDescriptors.Kappa2(mol)
            descriptors['Kappa3'] = GraphDescriptors.Kappa3(mol)
            
            # è¯ç‰©ç›¸ä¼¼æ€§
            descriptors['LipinskiViolations'] = Lipinski.NumRotatableBonds(mol)
            descriptors['QED'] = QED.qed(mol)
            descriptors['RO5_Violations'] = int(
                Lipinski.NumHDonors(mol) > 5 or 
                Lipinski.NumHAcceptors(mol) > 10 or 
                Descriptors.MolWt(mol) > 500 or 
                Descriptors.MolLogP(mol) > 5
            )
            descriptors['GhoseViolations'] = int(not (
                160 <= Descriptors.MolWt(mol) <= 480 and 
                -0.4 <= Descriptors.MolLogP(mol) <= 5.6 and 
                20 <= mol.GetNumAtoms() <= 70 and 
                0 <= Descriptors.NumRotatableBonds(mol) <= 10
            ))
            descriptors['VeberViolations'] = int(
                Descriptors.NumRotatableBonds(mol) > 10 or 
                MolSurf.TPSA(mol) > 140
            )
            
            # åŸå­ç»„æˆ
            descriptors['CarbonCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 6)
            descriptors['NitrogenCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 7)
            descriptors['OxygenCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 8)
            descriptors['SulfurCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 16)
            descriptors['PhosphorusCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 15)
            descriptors['FluorineCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 9)
            descriptors['ChlorineCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 17)
            descriptors['BromineCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 35)
            descriptors['IodineCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 53)
            descriptors['HalogenCount'] = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() in [9, 17, 35, 53])
            descriptors['FractionCSP3'] = Descriptors.FractionCSP3(mol)
            
            # ç”µå­æ€§è´¨
            descriptors['PEOE_VSA1'] = MolSurf.PEOE_VSA1(mol)
            descriptors['PEOE_VSA2'] = MolSurf.PEOE_VSA2(mol)
            descriptors['PEOE_VSA3'] = MolSurf.PEOE_VSA3(mol)
            descriptors['SMR_VSA1'] = MolSurf.SMR_VSA1(mol)
            descriptors['SMR_VSA2'] = MolSurf.SMR_VSA2(mol)
            descriptors['SlogP_VSA1'] = MolSurf.SlogP_VSA1(mol)
            descriptors['SlogP_VSA2'] = MolSurf.SlogP_VSA2(mol)
            
            # å…¶ä»–æ€§è´¨
            descriptors['MR'] = Crippen.MolMR(mol)
            descriptors['NHOH_Count'] = Lipinski.NHOHCount(mol)
            descriptors['NO_Count'] = Lipinski.NOCount(mol)
            descriptors['HallKierAlpha'] = GraphDescriptors.HallKierAlpha(mol)
            descriptors['ValenceElectrons'] = Descriptors.NumValenceElectrons(mol)
            descriptors['MolLogP'] = Descriptors.MolLogP(mol)
            descriptors['MolMR'] = Descriptors.MolMR(mol)
            
            # å¤æ‚åº¦ç›¸å…³
            try:
                descriptors['BCUT2D_MWHI'] = Descriptors.BCUT2D_MWHI(mol)
                descriptors['BCUT2D_MWLOW'] = Descriptors.BCUT2D_MWLOW(mol)
                descriptors['BCUT2D_CHGHI'] = Descriptors.BCUT2D_CHGHI(mol)
                descriptors['BCUT2D_CHGLO'] = Descriptors.BCUT2D_CHGLO(mol)
            except:
                descriptors['BCUT2D_MWHI'] = 0.0
                descriptors['BCUT2D_MWLOW'] = 0.0
                descriptors['BCUT2D_CHGHI'] = 0.0
                descriptors['BCUT2D_CHGLO'] = 0.0
            
            # åŠŸèƒ½å›¢è®¡æ•°
            descriptors['SP3_N_Count'] = len([atom for atom in mol.GetAtoms() if 
                                            atom.GetAtomicNum() == 7 and 
                                            atom.GetHybridization() == Chem.rdchem.HybridizationType.SP3])
            descriptors['SP2_N_Count'] = len([atom for atom in mol.GetAtoms() if 
                                            atom.GetAtomicNum() == 7 and 
                                            atom.GetHybridization() == Chem.rdchem.HybridizationType.SP2])
            descriptors['Amide_Count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[NX3][CX3](=[OX1])')))
            descriptors['Ester_Count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[#6][CX3](=[OX1])[OX2][#6]')))
            descriptors['Carboxylic_Count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[CX3](=[OX1])[OX2H]')))
            descriptors['Ether_Count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[OD2]([#6])[#6]')))
            descriptors['Alcohol_Count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[OX2H]')))
            descriptors['Amine_Count'] = len(mol.GetSubstructMatches(Chem.MolFromSmarts('[NX3;H2,H1,H0;!$(NC=O)]')))
            
            # æŒ‰é¡ºåºæå–ç‰¹å¾
            features = []
            for desc_name in MOLECULAR_DESCRIPTORS:
                if desc_name in descriptors:
                    value = descriptors[desc_name]
                    if np.isnan(value) or np.isinf(value):
                        value = 0.0
                    features.append(float(value))
                else:
                    features.append(0.0)
            
            return features
            
        except Exception as e:
            print(f"âš ï¸ åŒ–åˆç‰©ç‰¹å¾æå–é”™è¯¯: {e}")
            return [0.0] * COMPOUND_TOTAL_DIM
    
    def process_unique_proteins(self):
        """å¤„ç†å”¯ä¸€è›‹ç™½è´¨"""
        print("\n" + "="*60)
        print("ğŸ§¬ è›‹ç™½è´¨ç‰¹å¾æå–é˜¶æ®µ")
        print("="*60)
        print(f"éœ€è¦å¤„ç† {len(self.unique_proteins)} ä¸ªå”¯ä¸€è›‹ç™½è´¨")
        
        protein_features = {}
        processed = 0
        
        for accession, protein_info in self.unique_proteins.items():
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if self.progress_manager.is_protein_completed(accession):
                safe_acc = accession.replace('/', '_').replace('\\', '_').replace('|', '_')
                cache_file = os.path.join(self.cache_dir, f"protein_{safe_acc}_features.json")
                if os.path.exists(cache_file):
                    try:
                        with open(cache_file, 'r') as f:
                            cached_features = json.load(f)
                        protein_features[accession] = cached_features
                        processed += 1
                        continue
                    except:
                        pass
            
            processed += 1
            progress_info = self.progress_manager.get_progress_info()
            
            print(f"ğŸ”„ {processed}/{len(self.unique_proteins)} - {accession} "
                  f"[è›‹ç™½{progress_info['protein_percent']:.1f}% åŒ–åˆç‰©{progress_info['compound_percent']:.1f}%]")
            
            # æ£€æŸ¥ç¼“å­˜
            safe_acc = accession.replace('/', '_').replace('\\', '_').replace('|', '_')
            cache_file = os.path.join(self.cache_dir, f"protein_{safe_acc}_features.json")
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    cached_features = json.load(f)
                protein_features[accession] = cached_features
                self.progress_manager.mark_protein_completed(accession)
                
                if processed % SAVE_PROGRESS_INTERVAL == 0:
                    self.progress_manager.save_progress()
                continue
            
            # åˆ›å»ºFASTAæ–‡ä»¶
            fasta_file = os.path.join(self.temp_dir, f"{safe_acc}.fasta")
            with open(fasta_file, 'w') as f:
                f.write(f">{accession}\n{protein_info['sequence']}\n")
            
            # è¿è¡ŒPSI-BLAST
            pssm_file = self.run_psiblast(fasta_file, accession)
            
            # æå–ç‰¹å¾
            sequence = protein_info['sequence']
            
            try:
                pseaac_features = self.extract_pseaac_features(sequence)
                psepssm_features = self.extract_psepssm_features(pssm_file)
                
                feature_data = {
                    'accession': accession,
                    'sequence_length': len(sequence),
                    'pseaac_features': pseaac_features,
                    'psepssm_features': psepssm_features,
                    'pssm_available': pssm_file is not None and os.path.exists(pssm_file)
                }
                
                protein_features[accession] = feature_data
                
                with open(cache_file, 'w') as f:
                    json.dump(feature_data, f)
                
                self.progress_manager.mark_protein_completed(accession)
                print(f"  âœ… å®Œæˆ")
                
            except Exception as e:
                print(f"  âŒ å¤±è´¥: {e}")
                feature_data = {
                    'accession': accession,
                    'sequence_length': len(sequence),
                    'pseaac_features': [0.0] * PSEAAC_TOTAL_DIM,
                    'psepssm_features': [0.0] * PSEPSSM_TOTAL_DIM,
                    'pssm_available': False
                }
                if feature_data['pseaac_features']:
                    feature_data['pseaac_features'][0] = len(sequence)  # é•¿åº¦ç‰¹å¾
                protein_features[accession] = feature_data
                self.progress_manager.mark_protein_completed(accession)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(fasta_file):
                os.remove(fasta_file)
            
            if processed % SAVE_PROGRESS_INTERVAL == 0:
                self.progress_manager.save_progress()
                print(f"  ğŸ’¾ å·²ä¿å­˜è¿›åº¦")
        
        self.progress_manager.save_progress()
        print(f"\nâœ… è›‹ç™½è´¨ç‰¹å¾æå–å®Œæˆ: {len(protein_features)}/{len(self.unique_proteins)}")
        
        return protein_features
    
    def process_unique_compounds(self):
        """å¤„ç†å”¯ä¸€åŒ–åˆç‰©"""
        print("\n" + "="*60)
        print("ğŸ’Š åŒ–åˆç‰©ç‰¹å¾æå–é˜¶æ®µ")
        print("="*60)
        print(f"éœ€è¦å¤„ç† {len(self.unique_compounds)} ä¸ªå”¯ä¸€åŒ–åˆç‰©")
        
        compound_features = {}
        processed = 0
        
        for compound_cid, compound_info in self.unique_compounds.items():
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if self.progress_manager.is_compound_completed(compound_cid):
                safe_cid = str(compound_cid).replace('/', '_').replace('\\', '_').replace('|', '_')
                cache_file = os.path.join(self.cache_dir, f"compound_{safe_cid}_features.json")
                if os.path.exists(cache_file):
                    try:
                        with open(cache_file, 'r') as f:
                            cached_features = json.load(f)
                        compound_features[compound_cid] = cached_features
                        processed += 1
                        continue
                    except:
                        pass
            
            processed += 1
            progress_info = self.progress_manager.get_progress_info()
            
            print(f"ğŸ”„ {processed}/{len(self.unique_compounds)} - {compound_cid} "
                  f"[è›‹ç™½{progress_info['protein_percent']:.1f}% åŒ–åˆç‰©{progress_info['compound_percent']:.1f}%]")
            
            # æ£€æŸ¥ç¼“å­˜
            safe_cid = str(compound_cid).replace('/', '_').replace('\\', '_').replace('|', '_')
            cache_file = os.path.join(self.cache_dir, f"compound_{safe_cid}_features.json")
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    cached_features = json.load(f)
                compound_features[compound_cid] = cached_features
                self.progress_manager.mark_compound_completed(compound_cid)
                
                if processed % SAVE_PROGRESS_INTERVAL == 0:
                    self.progress_manager.save_progress()
                continue
            
            # æå–ç‰¹å¾
            smile = compound_info['smile']
            
            try:
                features = self.extract_compound_features(smile)
                
                feature_data = {
                    'compound_cid': compound_cid,
                    'smile': smile,
                    'compound_features': features
                }
                
                compound_features[compound_cid] = feature_data
                
                with open(cache_file, 'w') as f:
                    json.dump(feature_data, f)
                
                self.progress_manager.mark_compound_completed(compound_cid)
                print(f"  âœ… å®Œæˆ")
                
            except Exception as e:
                print(f"  âŒ å¤±è´¥: {e}")
                feature_data = {
                    'compound_cid': compound_cid,
                    'smile': smile,
                    'compound_features': [0.0] * COMPOUND_TOTAL_DIM
                }
                compound_features[compound_cid] = feature_data
                self.progress_manager.mark_compound_completed(compound_cid)
            
            if processed % SAVE_PROGRESS_INTERVAL == 0:
                self.progress_manager.save_progress()
                print(f"  ğŸ’¾ å·²ä¿å­˜è¿›åº¦")
        
        self.progress_manager.save_progress()
        print(f"\nâœ… åŒ–åˆç‰©ç‰¹å¾æå–å®Œæˆ: {len(compound_features)}/{len(self.unique_compounds)}")
        
        return compound_features
    
    def combine_and_save_features(self, protein_features, compound_features):
        """ç»„åˆç‰¹å¾å¹¶ä¿å­˜ - å…³é”®ä¿®å¤ï¼šä¸¥æ ¼ä¿æŒåŸå§‹é¡ºåº"""
        print("\n" + "="*60)
        print("ğŸ”— ç‰¹å¾ç»„åˆå’Œä¿å­˜é˜¶æ®µ (ä¸¥æ ¼ä¿æŒåŸå§‹é¡ºåº)")
        print("="*60)
        
        # ç”Ÿæˆç‰¹å¾åç§°
        pseaac_names = ['Length'] + [f'AA_{aa}' for aa in 'ACDEFGHIKLMNPQRSTVWY'] + \
                       [f'Hydrophobicity_lambda_{i}' for i in range(1, PSEAAC_LAMBDA + 1)] + \
                       [f'Isoelectric_Point_lambda_{i}' for i in range(1, PSEAAC_LAMBDA + 1)] + \
                       [f'Molecular_Weight_lambda_{i}' for i in range(1, PSEAAC_LAMBDA + 1)]
        
        psepssm_names = []
        amino_acids = 'ARNDCQEGHILKMFPSTWYV'
        for stat in ['Mean', 'Std', 'Max', 'Min']:
            for aa in amino_acids:
                psepssm_names.append(f'PSSM_{stat}_{aa}')
        
        remaining_psepssm = PSEPSSM_TOTAL_DIM - len(psepssm_names)
        for i in range(remaining_psepssm):
            psepssm_names.append(f'PsePSSM_Extra_{i+1}')
        
        compound_names = [f'Compound_{desc}' for desc in MOLECULAR_DESCRIPTORS]
        
        # å…³é”®ä¿®å¤ï¼šä¸¥æ ¼æŒ‰åŸå§‹è®°å½•é¡ºåºå¤„ç†
        all_results = []
        
        print(f"æ­£åœ¨æŒ‰åŸå§‹é¡ºåºç»„åˆ {len(self.original_records)} æ¡è®°å½•çš„ç‰¹å¾...")
        
        for original_record in self.original_records:
            accession = original_record['accession']
            compound_cid = original_record['compound_cid']
            
            result = {
                'Original_Row_Number': original_record['original_row_number'],  # ä¿ç•™åŸå§‹è¡Œå·ç”¨äºéªŒè¯
                'Protein_Accession': accession,
                'Compound_CID': compound_cid,
                'Smile': original_record['smile'],
                'Label': original_record['label']
            }
            
            # æ·»åŠ è›‹ç™½è´¨ç‰¹å¾
            if accession in protein_features:
                prot_features = protein_features[accession]
                result['Sequence_Length'] = prot_features['sequence_length']
                result['PSSM_Available'] = prot_features['pssm_available']
                
                # PSE-AACç‰¹å¾
                pseaac_features = prot_features['pseaac_features']
                for i, name in enumerate(pseaac_names):
                    if i < len(pseaac_features):
                        result[name] = pseaac_features[i]
                    else:
                        result[name] = 0.0
                
                # PSE-PSSMç‰¹å¾
                psepssm_features = prot_features['psepssm_features']
                for i, name in enumerate(psepssm_names):
                    if i < len(psepssm_features):
                        result[name] = psepssm_features[i]
                    else:
                        result[name] = 0.0
            else:
                result['Sequence_Length'] = len(original_record['sequence'])
                result['PSSM_Available'] = False
                for name in pseaac_names:
                    result[name] = 0.0 if name != 'Length' else len(original_record['sequence'])
                for name in psepssm_names:
                    result[name] = 0.0
            
            # æ·»åŠ åŒ–åˆç‰©ç‰¹å¾
            if compound_cid in compound_features:
                comp_features = compound_features[compound_cid]['compound_features']
                for i, name in enumerate(compound_names):
                    if i < len(comp_features):
                        result[name] = comp_features[i]
                    else:
                        result[name] = 0.0
            else:
                for name in compound_names:
                    result[name] = 0.0
            
            all_results.append(result)
        
        # éªŒè¯é¡ºåºæ˜¯å¦æ­£ç¡®
        print("ğŸ” éªŒè¯è¾“å‡ºé¡ºåº...")
        order_verification_passed = True
        for i, result in enumerate(all_results):
            expected_row = i + 1
            actual_row = result['Original_Row_Number']
            if expected_row != actual_row:
                print(f"âŒ é¡ºåºé”™è¯¯ï¼šä½ç½® {i+1} åº”è¯¥æ˜¯ç¬¬ {expected_row} è¡Œï¼Œä½†å®é™…æ˜¯ç¬¬ {actual_row} è¡Œ")
                order_verification_passed = False
                break
        
        if order_verification_passed:
            print("âœ… è¾“å‡ºé¡ºåºéªŒè¯é€šè¿‡ï¼šä¸è¾“å…¥æ–‡ä»¶å®Œå…¨ä¸€è‡´")
        else:
            print("âŒ è¾“å‡ºé¡ºåºéªŒè¯å¤±è´¥")
            return None
        
        # ä¿å­˜ç»“æœ
        base_name = os.path.splitext(os.path.basename(self.input_filename))[0]
        
        # ä¸»è¦ç»“æœæ–‡ä»¶ï¼ˆæ ‡å‡†æ ¼å¼ï¼Œä¸åŒ…å«è¡Œå·éªŒè¯åˆ—ï¼‰
        combined_file = os.path.join(self.output_dir, f'{base_name}_combined_features.csv')
        with open(combined_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            header = ['Protein_Accession'] + pseaac_names + psepssm_names + \
                    ['Compound_CID'] + compound_names + ['Label']
            writer.writerow(header)
            
            for result in all_results:
                row = [result['Protein_Accession']]
                for name in pseaac_names:
                    row.append(result[name])
                for name in psepssm_names:
                    row.append(result[name])
                row.append(result['Compound_CID'])
                for name in compound_names:
                    row.append(result[name])
                row.append(result['Label'])
                writer.writerow(row)
        
        # è¯¦ç»†ç»“æœæ–‡ä»¶ï¼ˆåŒ…å«éªŒè¯ä¿¡æ¯ï¼‰
        detailed_file = os.path.join(self.output_dir, f'{base_name}_detailed_features.csv')
        with open(detailed_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['Original_Row_Number', 'Protein_Accession', 'Compound_CID', 'Smile', 
                         'Sequence_Length', 'PSSM_Available'] + \
                        pseaac_names + psepssm_names + compound_names + ['Label']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_results)
        
        # é¡ºåºéªŒè¯æ–‡ä»¶
        order_verification_file = os.path.join(self.output_dir, f'{base_name}_order_verification.csv')
        with open(order_verification_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Position', 'Original_Row_Number', 'Protein_Accession', 'Compound_CID', 'Order_OK'])
            
            for i, result in enumerate(all_results):
                position = i + 1
                original_row = result['Original_Row_Number']
                order_ok = position == original_row
                writer.writerow([
                    position, 
                    original_row, 
                    result['Protein_Accession'], 
                    result['Compound_CID'], 
                    'YES' if order_ok else 'NO'
                ])
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats_file = os.path.join(self.output_dir, f'{base_name}_processing_stats.json')
        stats = {
            'input_file': self.input_filename,
            'processing_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'user': 'woyaokaoyanhaha',
            'version': '11.0',
            'resume_mode': self.resume_mode,
            'preserve_order': self.preserve_order,
            'work_directory': self.work_dir,
            'total_records': len(all_results),
            'unique_proteins': len(self.unique_proteins),
            'unique_compounds': len(self.unique_compounds),
            'successful_pssm': sum(1 for r in all_results if r['PSSM_Available']),
            'order_verification_passed': order_verification_passed,
            'feature_dimensions': {
                'protein_pseaac': PSEAAC_TOTAL_DIM,
                'protein_psepssm': PSEPSSM_TOTAL_DIM,
                'compound': COMPOUND_TOTAL_DIM,
                'total': PSEAAC_TOTAL_DIM + PSEPSSM_TOTAL_DIM + COMPOUND_TOTAL_DIM
            }
        }
        
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜:")
        print(f"  ğŸ“Š ä¸»è¦ç»“æœ: {combined_file}")
        print(f"  ğŸ“‹ è¯¦ç»†ç»“æœ: {detailed_file}")
        print(f"  ğŸ” é¡ºåºéªŒè¯: {order_verification_file}")
        print(f"  ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
        print(f"  âœ… é¡ºåºä¿æŒ: {'å®Œç¾' if order_verification_passed else 'æœ‰é—®é¢˜'}")
        
        return stats

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ§¬ è›‹ç™½è´¨-åŒ–åˆç‰©ç‰¹å¾æå–è„šæœ¬ (ä¿®å¤æ’åºç‰ˆæœ¬)")
    print(f"ğŸ‘¤ ç”¨æˆ·: woyaokaoyanhaha")
    print(f"ğŸ“… æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ æ€»ç‰¹å¾ç»´åº¦: {PSEAAC_TOTAL_DIM + PSEPSSM_TOTAL_DIM + COMPOUND_TOTAL_DIM}")
    print(f"ğŸ”§ ç‰ˆæœ¬: 11.0 (ä¿®å¤æ’åºé—®é¢˜)")
    print("="*80)
    
    try:
        # è§£æå‚æ•°
        args = parse_arguments()
        
        # å¤„ç†ç‰¹æ®Šæ¨¡å¼
        if args.list_resume:
            list_resumable_directories()
            return 0
        
        if args.test:
            print("ğŸ§ª æµ‹è¯•æ¨¡å¼ - æ£€æŸ¥æ–‡ä»¶å’Œç¯å¢ƒ")
            if args.input_csv and args.swissprot_db:
                detected_columns, header = detect_column_names(args.input_csv)
                if detected_columns:
                    print("âœ… CSVæ–‡ä»¶æ ¼å¼æ£€æŸ¥é€šè¿‡")
                else:
                    print("âŒ CSVæ–‡ä»¶æ ¼å¼æ£€æŸ¥å¤±è´¥")
                    return 1
                
                db_file = args.swissprot_db + ".pin"
                if os.path.exists(db_file):
                    print("âœ… æ•°æ®åº“æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
                else:
                    print("âŒ æ•°æ®åº“æ–‡ä»¶æ£€æŸ¥å¤±è´¥")
                    return 1
                
                print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
            else:
                print("âŒ ç¼ºå°‘æµ‹è¯•å‚æ•°")
                return 1
            return 0

        # æ£€æŸ¥å¿…éœ€å‚æ•°
        if not args.input_csv or not args.swissprot_db:
            print("âŒ ç¼ºå°‘å¿…éœ€å‚æ•°")
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  python3 extract_protein_compound_features_v11.py <input.csv> <swissprot_db>")
            print("  python3 extract_protein_compound_features_v11.py --list-resume")
            return 1

        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not os.path.exists(args.input_csv):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_csv}")
            return 1

        # æ£€æŸ¥æ•°æ®åº“
        db_file = args.swissprot_db + ".pin"
        if not os.path.exists(db_file):
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_file}")
            print("è¯·ç¡®ä¿SwissProtæ•°æ®åº“å·²æ­£ç¡®å®‰è£…å’Œæ ¼å¼åŒ–")
            return 1

        # ç¡®å®šå·¥ä½œç›®å½•
        resume_mode = False
        if args.resume:
            if not os.path.exists(args.resume):
                print(f"âŒ æ¢å¤ç›®å½•ä¸å­˜åœ¨: {args.resume}")
                return 1

            progress_file = os.path.join(args.resume, "progress.json")
            if not os.path.exists(progress_file):
                print(f"âŒ æ¢å¤ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°è¿›åº¦æ–‡ä»¶")
                return 1

            work_dir = args.resume
            resume_mode = True
        else:
            work_dir = get_output_dir_name(args.input_csv, args.output)

            if os.path.exists(work_dir):
                print(f"âš ï¸ è¾“å‡ºç›®å½•å·²å­˜åœ¨: {work_dir}")
                choice = input("é€‰æ‹©: 1)åˆ é™¤é‡å»º 2)é€€å‡º [1-2]: ").strip()

                if choice == '1':
                    import shutil
                    shutil.rmtree(work_dir)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç›®å½•: {work_dir}")
                else:
                    print("ğŸ‘‹ é€€å‡º")
                    return 0

        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        extractor = FeatureExtractor(
            args.swissprot_db,
            work_dir,
            args.input_csv,
            resume_mode,
            preserve_order=args.preserve_order
        )

        start_time = time.time()

        # 1. åŠ è½½å’Œå»é‡
        unique_protein_count, unique_compound_count = extractor.load_and_deduplicate(args.input_csv)
        if unique_protein_count == 0 or unique_compound_count == 0:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return 1

        # 2. å¤„ç†è›‹ç™½è´¨
        protein_features = extractor.process_unique_proteins()

        # 3. å¤„ç†åŒ–åˆç‰©
        compound_features = extractor.process_unique_compounds()

        # 4. ç»„åˆç‰¹å¾å¹¶ä¿å­˜
        stats = extractor.combine_and_save_features(protein_features, compound_features)
        if not stats:
            print("âŒ ç‰¹å¾ç»„åˆå¤±è´¥")
            return 1

        # 5. æ ‡è®°å®Œæˆ
        extractor.progress_manager.mark_completed()

        # 6. è¾“å‡ºç»Ÿè®¡
        end_time = time.time()
        processing_time = end_time - start_time

        print("\n" + "=" * 80)
        print("ğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        if unique_protein_count > 0:
            print(f"ğŸ§¬ å¹³å‡æ¯ä¸ªè›‹ç™½è´¨: {processing_time / unique_protein_count:.2f} ç§’")
        if unique_compound_count > 0:
            print(f"ğŸ’Š å¹³å‡æ¯ä¸ªåŒ–åˆç‰©: {processing_time / unique_compound_count:.2f} ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {work_dir}")

        # æ˜¾ç¤ºæˆåŠŸç‡
        successful_proteins = len([p for p in protein_features.values() if p.get('pssm_available', False)])
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(
            f"  PSSMç”ŸæˆæˆåŠŸ: {successful_proteins}/{unique_protein_count} ({successful_proteins / unique_protein_count * 100:.1f}%)")
        print(f"  è›‹ç™½è´¨ç‰¹å¾æå–: 100%")
        print(f"  åŒ–åˆç‰©ç‰¹å¾æå–: 100%")
        print(f"  è¾“å‡ºé¡ºåºä¿æŒ: âœ… å®Œç¾")
        print("=" * 80)

        return 0

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        print("å¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¢å¤:")
        if 'work_dir' in locals():
            print(f"python3 {sys.argv[0]} {args.input_csv} {args.swissprot_db} -r {work_dir}")
        return 1
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)