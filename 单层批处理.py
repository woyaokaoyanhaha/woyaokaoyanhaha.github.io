# =============================================================================
# å¼ºåˆ¶è®¾ç½®ä¸´æ—¶æ–‡ä»¶åˆ°ä»£ç åŒç›®å½•
# =============================================================================
import os
import tempfile
import glob
from datetime import datetime

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
LOCAL_TEMP_DIR = os.path.join(SCRIPT_DIR, "temp")

# åˆ›å»ºæœ¬åœ°ä¸´æ—¶ç›®å½•
try:
    os.makedirs(LOCAL_TEMP_DIR, exist_ok=True)
    tempfile.tempdir = LOCAL_TEMP_DIR
    os.environ['TEMP'] = LOCAL_TEMP_DIR
    os.environ['TMP'] = LOCAL_TEMP_DIR
    print(f"ğŸ¯ ä¸´æ—¶æ–‡ä»¶ç›®å½•è®¾ç½®ä¸º: {LOCAL_TEMP_DIR}")
except Exception as e:
    print(f"âŒ è®¾ç½®æœ¬åœ°ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
    print(f"   å°†ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸´æ—¶ç›®å½•")

# =============================================================================
# ç”¨æˆ·é…ç½®å‚æ•° - å¤ç”¨ä¸´æ—¶æ–‡ä»¶ç‰ˆ
# =============================================================================

# è¾“å…¥æ–‡ä»¶é…ç½®
PROTEIN_FEATURE_FILE = "protein_features.csv"
COMPOUND_FEATURE_FILE = "69ä¸‡å…¨éƒ¨ç‰¹å¾.csv"

# ä¸´æ—¶æ–‡ä»¶å¤ç”¨é…ç½®
REUSE_EXISTING_CHUNKS = True  # æ˜¯å¦å¤ç”¨ç°æœ‰çš„åˆ†å—æ–‡ä»¶
AUTO_DETECT_CHUNKS = True  # è‡ªåŠ¨æ£€æµ‹ç°æœ‰åˆ†å—æ–‡ä»¶
SPECIFIC_CHUNK_DIR = None  # æŒ‡å®šç‰¹å®šçš„åˆ†å—ç›®å½•ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹

# æ¨¡å‹ç›®å½•é…ç½®
MODEL_BASE_DIR = "æ–°è›‹ç™½ç‰¹å¾æ ¸å—ä½“Combine_BioAssayæ–°ç‰¹å¾"
SELECTED_MODEL_TYPES = ["æ ‡å‡†éšæœºåˆ†å‰²"]

# å†…å­˜ä¼˜åŒ–é…ç½®
COMPOUND_CHUNK_SIZE = 100000  # å¦‚æœéœ€è¦é‡æ–°åˆ†å—æ—¶ä½¿ç”¨
MEMORY_BATCH_SIZE = 5000  # å†…å­˜æ‰¹å¤„ç†å¤§å°
MAX_MEMORY_GB = 8  # é™ä½å†…å­˜é™åˆ¶
ENABLE_MEMORY_MONITORING = True  # å¯ç”¨å†…å­˜ç›‘æ§

# å…·ä½“æ¨¡å‹é€‰æ‹©
SELECTED_MODELS = {
    "æ ‡å‡†éšæœºåˆ†å‰²": ["å †å åˆ†ç±»å™¨_æ ‡å‡†éšæœºåˆ†å‰²", "éšæœºæ£®æ—", "æç«¯éšæœºæ ‘"],
}

# è¾“å‡ºé…ç½®
OUTPUT_BASE_DIR = "å…±è¯†é¢„æµ‹69ä¸‡æ¡"
DETAILED_OUTPUT = True
SAVE_PROBABILITIES = True
SEPARATE_MODEL_RESULTS = True

# é¢„æµ‹è®¾ç½®
USE_ENSEMBLE_MODELS = False
CONFIDENCE_THRESHOLD = 0.5
PREDICTION_MODE = "all_combinations"

# äº¤äº’å¼æ¨¡å‹é€‰æ‹©
INTERACTIVE_MODEL_SELECTION = False

# è¿›åº¦æ˜¾ç¤º
SHOW_PROGRESS = True

# è·³è¿‡æœ‰é—®é¢˜çš„æ¨¡å‹
SKIP_ENSEMBLE_MODELS = False
SKIP_MEMORY_INTENSIVE_MODELS = False

# å…±è¯†é¢„æµ‹é…ç½®
ENABLE_CONSENSUS_ANALYSIS = True
MIN_CONSENSUS_MODELS = 2
CONSENSUS_PROBABILITY_THRESHOLD = 0.5
SAVE_CONSENSUS_RESULTS = True
CONSENSUS_OUTPUT_DETAILED = True

# å…±è¯†åˆ†æç±»å‹
CONSENSUS_ANALYSIS_TYPES = [
    "all_positive",
]

# =============================================================================
# ç»§ç»­å¯¼å…¥å…¶ä»–åº“
# =============================================================================

import sys
import numpy as np
import pandas as pd
import joblib
import warnings
import json
import gc
from tqdm import tqdm
from sklearn.base import clone
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold
from joblib import Parallel, delayed
from collections import defaultdict, Counter
import shutil

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')


# =============================================================================
# å†…å­˜ç›‘æ§å·¥å…·
# =============================================================================

def get_memory_usage():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
    try:
        import psutil
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': process.memory_percent()
        }
    except ImportError:
        return {'rss_mb': 0, 'vms_mb': 0, 'percent': 0}


def monitor_memory(operation_name="æ“ä½œ"):
    """å†…å­˜ç›‘æ§è£…é¥°å™¨"""

    def decorator(func):
        def wrapper(*args, **kwargs):
            if ENABLE_MEMORY_MONITORING:
                before = get_memory_usage()
                print(f"ğŸ” {operation_name}å‰: {before['rss_mb']:.1f}MB ({before['percent']:.1f}%)")

            try:
                result = func(*args, **kwargs)

                if ENABLE_MEMORY_MONITORING:
                    after = get_memory_usage()
                    print(f"ğŸ” {operation_name}å: {after['rss_mb']:.1f}MB ({after['percent']:.1f}%)")
                    print(f"ğŸ“Š å†…å­˜å˜åŒ–: {after['rss_mb'] - before['rss_mb']:+.1f}MB")

                return result

            except MemoryError as e:
                print(f"âŒ {operation_name}å†…å­˜ä¸è¶³: {e}")
                gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
                raise
            except Exception as e:
                print(f"âŒ {operation_name}å¤±è´¥: {e}")
                raise

        return wrapper

    return decorator


def optimize_batch_size_by_memory():
    """æ ¹æ®å¯ç”¨å†…å­˜ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°"""
    try:
        import psutil
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024 ** 3)

        if available_gb >= 16:
            return 10000  # å……è¶³å†…å­˜
        elif available_gb >= 8:
            return 5000  # ä¸­ç­‰å†…å­˜
        elif available_gb >= 4:
            return 2000  # è¾ƒå°‘å†…å­˜
        else:
            return 1000  # æœ€å°å†…å­˜
    except:
        return MEMORY_BATCH_SIZE  # é»˜è®¤å€¼


# =============================================================================
# ç°æœ‰åˆ†å—æ–‡ä»¶æ£€æµ‹å’Œç®¡ç†å™¨
# =============================================================================

class ExistingChunkManager:
    """ç°æœ‰åˆ†å—æ–‡ä»¶æ£€æµ‹å’Œç®¡ç†å™¨"""

    def __init__(self):
        self.temp_base_dir = LOCAL_TEMP_DIR
        self.existing_chunk_dirs = []
        self.selected_chunk_dir = None

    def scan_existing_chunks(self):
        """æ‰«æç°æœ‰çš„åˆ†å—ç›®å½•"""
        print(f"ğŸ” æ‰«æç°æœ‰åˆ†å—æ–‡ä»¶...")
        print(f"   æ‰«æç›®å½•: {self.temp_base_dir}")

        if not os.path.exists(self.temp_base_dir):
            print(f"   ä¸´æ—¶ç›®å½•ä¸å­˜åœ¨")
            return []

        # æŸ¥æ‰¾æ‰€æœ‰compound_chunks_*ç›®å½•
        pattern = os.path.join(self.temp_base_dir, "compound_chunks_*")
        chunk_dirs = glob.glob(pattern)

        valid_chunk_dirs = []

        for chunk_dir in chunk_dirs:
            if os.path.isdir(chunk_dir):
                # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰.csvæ–‡ä»¶
                csv_files = glob.glob(os.path.join(chunk_dir, "compound_chunk_*.csv"))
                if csv_files:
                    # è·å–ç›®å½•ä¿¡æ¯
                    dir_name = os.path.basename(chunk_dir)
                    timestamp_str = dir_name.replace("compound_chunks_", "")

                    # è®¡ç®—æ–‡ä»¶å¤§å°
                    total_size = 0
                    file_count = len(csv_files)

                    for csv_file in csv_files:
                        try:
                            total_size += os.path.getsize(csv_file)
                        except:
                            pass

                    total_size_gb = total_size / (1024 ** 3)

                    chunk_info = {
                        'dir_path': chunk_dir,
                        'dir_name': dir_name,
                        'timestamp': timestamp_str,
                        'file_count': file_count,
                        'total_size_gb': total_size_gb,
                        'csv_files': sorted(csv_files)
                    }

                    valid_chunk_dirs.append(chunk_info)
                    print(f"   âœ“ å‘ç°: {dir_name}")
                    print(f"     - æ–‡ä»¶æ•°: {file_count}")
                    print(f"     - æ€»å¤§å°: {total_size_gb:.2f} GB")
                    print(f"     - æ—¶é—´æˆ³: {timestamp_str}")

        self.existing_chunk_dirs = sorted(valid_chunk_dirs, key=lambda x: x['timestamp'], reverse=True)

        if valid_chunk_dirs:
            print(f"\nğŸ“Š å…±å‘ç° {len(valid_chunk_dirs)} ä¸ªæœ‰æ•ˆåˆ†å—ç›®å½•")
        else:
            print(f"   æœªå‘ç°æœ‰æ•ˆçš„åˆ†å—æ–‡ä»¶")

        return self.existing_chunk_dirs

    def select_chunk_directory(self):
        """é€‰æ‹©è¦ä½¿ç”¨çš„åˆ†å—ç›®å½•"""
        if not self.existing_chunk_dirs:
            print(f"âŒ æ²¡æœ‰å¯ç”¨çš„ç°æœ‰åˆ†å—æ–‡ä»¶")
            return None

        if len(self.existing_chunk_dirs) == 1:
            # åªæœ‰ä¸€ä¸ªé€‰æ‹©ï¼Œç›´æ¥ä½¿ç”¨
            selected = self.existing_chunk_dirs[0]
            print(f"ğŸ¯ è‡ªåŠ¨é€‰æ‹©å”¯ä¸€çš„åˆ†å—ç›®å½•: {selected['dir_name']}")
            self.selected_chunk_dir = selected
            return selected

        # å¤šä¸ªé€‰æ‹©ï¼Œæ˜¾ç¤ºåˆ—è¡¨è®©ç”¨æˆ·é€‰æ‹©
        print(f"\nğŸ“‹ å‘ç°å¤šä¸ªåˆ†å—ç›®å½•ï¼Œè¯·é€‰æ‹©:")
        for i, chunk_info in enumerate(self.existing_chunk_dirs):
            print(f"  {i + 1}. {chunk_info['dir_name']}")
            print(f"     æ—¶é—´: {chunk_info['timestamp']}")
            print(f"     æ–‡ä»¶: {chunk_info['file_count']}ä¸ª")
            print(f"     å¤§å°: {chunk_info['total_size_gb']:.2f}GB")
            print()

        while True:
            try:
                choice = input(f"è¯·é€‰æ‹©åˆ†å—ç›®å½• (1-{len(self.existing_chunk_dirs)}) æˆ– 'n' é‡æ–°åˆ†å—: ").strip()

                if choice.lower() == 'n':
                    print(f"ğŸ’« ç”¨æˆ·é€‰æ‹©é‡æ–°åˆ†å—")
                    return None

                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(self.existing_chunk_dirs):
                    selected = self.existing_chunk_dirs[choice_idx]
                    print(f"ğŸ¯ ç”¨æˆ·é€‰æ‹©: {selected['dir_name']}")
                    self.selected_chunk_dir = selected
                    return selected
                else:
                    print(f"âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-{len(self.existing_chunk_dirs)} æˆ– 'n'")

            except ValueError:
                print(f"âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥æ•°å­—æˆ– 'n'")
                continue

    def validate_chunk_files(self, chunk_info):
        """éªŒè¯åˆ†å—æ–‡ä»¶çš„å®Œæ•´æ€§"""
        print(f"ğŸ”§ éªŒè¯åˆ†å—æ–‡ä»¶å®Œæ•´æ€§...")

        csv_files = chunk_info['csv_files']
        valid_files = []
        total_rows = 0

        for csv_file in csv_files:
            try:
                # å°è¯•è¯»å–æ–‡ä»¶å¤´éƒ¨
                df_sample = pd.read_csv(csv_file, nrows=5)

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(csv_file)
                if file_size < 1024:  # å°äº1KBè®¤ä¸ºæ–‡ä»¶æœ‰é—®é¢˜
                    print(f"   âš ï¸ æ–‡ä»¶è¿‡å°: {os.path.basename(csv_file)} ({file_size}å­—èŠ‚)")
                    continue

                # ä¼°ç®—è¡Œæ•°
                with open(csv_file, 'r') as f:
                    line_count = sum(1 for line in f) - 1  # å‡å»æ ‡é¢˜è¡Œ

                valid_files.append({
                    'file_path': csv_file,
                    'file_name': os.path.basename(csv_file),
                    'rows': line_count,
                    'columns': len(df_sample.columns),
                    'file_size_mb': file_size / (1024 * 1024)
                })

                total_rows += line_count
                print(f"   âœ“ {os.path.basename(csv_file)}: {line_count:,}è¡Œ, {len(df_sample.columns)}åˆ—")

            except Exception as e:
                print(f"   âŒ æ–‡ä»¶æŸå: {os.path.basename(csv_file)} - {e}")
                continue

        if valid_files:
            print(f"   ğŸ“Š éªŒè¯å®Œæˆ: {len(valid_files)}/{len(csv_files)} æ–‡ä»¶æœ‰æ•ˆ")
            print(f"   ğŸ“Š æ€»è¡Œæ•°: {total_rows:,}")
            return valid_files
        else:
            print(f"   âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†å—æ–‡ä»¶")
            return []

    def get_chunk_file_info(self):
        """è·å–é€‰ä¸­åˆ†å—ç›®å½•çš„æ–‡ä»¶ä¿¡æ¯"""
        if not self.selected_chunk_dir:
            return None

        valid_files = self.validate_chunk_files(self.selected_chunk_dir)

        if not valid_files:
            return None

        return {
            'chunk_dir': self.selected_chunk_dir['dir_path'],
            'chunk_files': [f['file_path'] for f in valid_files],
            'chunk_info': valid_files,
            'total_files': len(valid_files),
            'total_rows': sum(f['rows'] for f in valid_files),
            'reused': True
        }


# =============================================================================
# JSONåºåˆ—åŒ–è¾…åŠ©å‡½æ•°
# =============================================================================

def convert_numpy_types(obj):
    """è½¬æ¢NumPyæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj


def safe_json_dump(obj, file_path, **kwargs):
    """å®‰å…¨JSONä¿å­˜"""
    try:
        converted_obj = convert_numpy_types(obj)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(converted_obj, f, **kwargs)
        return True
    except Exception as e:
        print(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        return False


# =============================================================================
# è‡ªå®šä¹‰åˆ†ç±»å™¨ç±»å®šä¹‰ï¼ˆä¿æŒä¸å˜ï¼‰
# =============================================================================

class CustomVotingClassifier:
    """è‡ªå®šä¹‰æŠ•ç¥¨åˆ†ç±»å™¨"""

    def __init__(self, estimators, voting='soft'):
        self.estimators = estimators
        self.voting = voting
        self.classes_ = None
        self.named_estimators_ = dict(estimators)

    def get_params(self, deep=True):
        params = {'voting': self.voting}
        if deep:
            estimators_params = {}
            for name, estimator in self.estimators:
                if hasattr(estimator, 'get_params'):
                    for key, val in estimator.get_params(deep).items():
                        estimators_params[f'{name}__{key}'] = val
            params.update(estimators_params)
        params['estimators'] = self.estimators
        return params

    def set_params(self, **params):
        for key, value in params.items():
            if key == 'estimators':
                self.estimators = value
                self.named_estimators_ = dict(value)
            elif key == 'voting':
                self.voting = value
            else:
                try:
                    est_name, param_name = key.split('__', 1)
                    for i, (name, est) in enumerate(self.estimators):
                        if name == est_name and hasattr(est, 'set_params'):
                            self.estimators[i] = (name, est.set_params(**{param_name: value}))
                            break
                except:
                    pass
        return self

    def fit(self, X, y):
        self.classes_ = np.unique(y)
        for name, est in self.estimators:
            try:
                est.fit(X, y)
            except Exception as e:
                print(f"è®­ç»ƒåˆ†ç±»å™¨ {name} æ—¶å‡ºé”™: {e}")
        return self

    def predict(self, X):
        if self.voting == 'hard':
            predictions = np.array([clf.predict(X) for _, clf in self.estimators])
            maj = np.apply_along_axis(
                lambda x: np.argmax(np.bincount(x, minlength=len(self.classes_))),
                axis=0, arr=predictions)
            return self.classes_[maj]
        else:
            predictions = self._collect_probas(X)
            avg = np.average(predictions, axis=0)
            return self.classes_[np.argmax(avg, axis=1)]

    def predict_proba(self, X):
        if self.voting == 'hard':
            raise AttributeError("predict_proba is not available when voting='hard'")
        return np.average(self._collect_probas(X), axis=0)

    def _collect_probas(self, X):
        probas = []
        for name, clf in self.estimators:
            try:
                proba = clf.predict_proba(X)
                probas.append(proba)
            except Exception as e:
                print(f"è·å–åˆ†ç±»å™¨ {name} æ¦‚ç‡æ—¶å‡ºé”™: {e}")
                probas.append(np.zeros((X.shape[0], len(self.classes_))))
        return np.asarray(probas)


class CustomStackingClassifier:
    """è‡ªå®šä¹‰å †å åˆ†ç±»å™¨"""

    def __init__(self, estimators, final_estimator, cv=5):
        self.estimators = estimators
        self.final_estimator = final_estimator
        self.cv = cv
        self.named_estimators_ = dict(estimators)
        self.classes_ = None

    def get_params(self, deep=True):
        params = {'cv': self.cv}
        params['estimators'] = self.estimators
        params['final_estimator'] = self.final_estimator
        if deep:
            estimators_params = {}
            for name, estimator in self.estimators:
                if hasattr(estimator, 'get_params'):
                    for key, val in estimator.get_params(deep).items():
                        estimators_params[f'{name}__{key}'] = val
            params.update(estimators_params)
            if hasattr(self.final_estimator, 'get_params'):
                for key, val in self.final_estimator.get_params(deep).items():
                    params[f'final_estimator__{key}'] = val
        return params

    def set_params(self, **params):
        for key, value in params.items():
            if key == 'estimators':
                self.estimators = value
                self.named_estimators_ = dict(value)
            elif key == 'final_estimator':
                self.final_estimator = value
            elif key == 'cv':
                self.cv = value
            elif key.startswith('final_estimator__'):
                if hasattr(self.final_estimator, 'set_params'):
                    param_name = key.split('__', 1)[1]
                    self.final_estimator.set_params(**{param_name: value})
            else:
                try:
                    est_name, param_name = key.split('__', 1)
                    for i, (name, est) in enumerate(self.estimators):
                        if name == est_name and hasattr(est, 'set_params'):
                            self.estimators[i] = (name, est.set_params(**{param_name: value}))
                            break
                except:
                    pass
        return self

    def fit(self, X, y):
        self.classes_ = np.unique(y)
        kf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=42)
        meta_features = np.zeros((X.shape[0], len(self.estimators) * len(self.classes_)))

        for i, (name, est) in enumerate(self.estimators):
            for train_idx, val_idx in kf.split(X, y):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]

                try:
                    est_clone = clone(est)
                    est_clone.fit(X_train, y_train)
                    prob = est_clone.predict_proba(X_val)
                    meta_features[val_idx, i * len(self.classes_):(i + 1) * len(self.classes_)] = prob
                except Exception as e:
                    print(f"è®­ç»ƒåŸºç¡€åˆ†ç±»å™¨ {name} æ—¶å‡ºé”™: {e}")

        for name, est in self.estimators:
            try:
                est.fit(X, y)
            except Exception as e:
                print(f"è®­ç»ƒåŸºç¡€åˆ†ç±»å™¨ {name} æœ€ç»ˆç‰ˆæœ¬æ—¶å‡ºé”™: {e}")

        try:
            self.final_estimator.fit(meta_features, y)
        except Exception as e:
            print(f"è®­ç»ƒå…ƒåˆ†ç±»å™¨æ—¶å‡ºé”™: {e}")

        return self

    def predict(self, X):
        meta_features = self._make_meta_features(X)
        return self.final_estimator.predict(meta_features)

    def predict_proba(self, X):
        meta_features = self._make_meta_features(X)
        return self.final_estimator.predict_proba(meta_features)

    def _make_meta_features(self, X):
        meta_features = np.zeros((X.shape[0], len(self.estimators) * len(self.classes_)))

        for i, (name, est) in enumerate(self.estimators):
            try:
                prob = est.predict_proba(X)
                meta_features[:, i * len(self.classes_):(i + 1) * len(self.classes_)] = prob
            except Exception as e:
                print(f"è·å–å…ƒç‰¹å¾æ—¶åˆ†ç±»å™¨ {name} å‡ºé”™: {e}")

        return meta_features


# =============================================================================
# å…±è¯†åˆ†æç±»ï¼ˆä¿æŒä¸å˜ï¼‰
# =============================================================================

class ConsensusAnalyzer:
    """å…±è¯†é¢„æµ‹åˆ†æå™¨"""

    def __init__(self, min_consensus_models=2, probability_threshold=0.6):
        self.min_consensus_models = min_consensus_models
        self.probability_threshold = probability_threshold
        self.consensus_results = defaultdict(list)

    def analyze_consensus(self, all_results):
        """åˆ†ææ‰€æœ‰æ¨¡å‹çš„å…±è¯†é¢„æµ‹"""
        print(f"\nå¼€å§‹å…±è¯†åˆ†æ...")
        print(f"å…±è¯†è¦æ±‚: è‡³å°‘{self.min_consensus_models}ä¸ªæ¨¡å‹åŒæ„")
        print(f"æ¦‚ç‡é˜ˆå€¼: {self.probability_threshold}")

        # æŒ‰åŒ–åˆç‰©-è›‹ç™½è´¨å¯¹åˆ†ç»„
        compound_predictions = defaultdict(list)

        for result in all_results:
            key = f"{result['protein_id']}_{result['compound_id']}"
            compound_predictions[key].append(result)

        print(f"å…±åˆ†æ {len(compound_predictions)} ä¸ªåŒ–åˆç‰©-è›‹ç™½è´¨å¯¹")

        # åˆ†æä¸åŒç±»å‹çš„å…±è¯†
        consensus_stats = {}

        if "all_positive" in CONSENSUS_ANALYSIS_TYPES:
            all_positive = self._find_all_positive_consensus(compound_predictions)
            consensus_stats["all_positive"] = all_positive
            print(f"æ‰€æœ‰æ¨¡å‹éƒ½é¢„æµ‹ä¸ºæ­£ä¾‹: {len(all_positive)} ä¸ª")

        return consensus_stats

    def _find_all_positive_consensus(self, compound_predictions):
        """æ‰¾åˆ°æ‰€æœ‰æ¨¡å‹éƒ½é¢„æµ‹ä¸ºæ­£ä¾‹çš„åŒ–åˆç‰©"""
        all_positive = []

        for compound_key, predictions in compound_predictions.items():
            if len(predictions) < self.min_consensus_models:
                continue

            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å‹éƒ½é¢„æµ‹ä¸ºæ­£ä¾‹
            all_positive_pred = all(pred['prediction'] == 1 for pred in predictions)

            if all_positive_pred:
                # è®¡ç®—å¹³å‡æ¦‚ç‡
                avg_prob_0 = float(np.mean([pred.get('probability_0', 0.5) for pred in predictions]))
                avg_prob_1 = float(np.mean([pred.get('probability_1', 0.5) for pred in predictions]))
                avg_confidence = float(np.mean([pred.get('confidence', 0.5) for pred in predictions]))

                consensus_info = {
                    'protein_id': str(predictions[0]['protein_id']),
                    'compound_id': str(predictions[0]['compound_id']),
                    'num_models': int(len(predictions)),
                    'consensus_type': 'all_positive',
                    'avg_probability_0': avg_prob_0,
                    'avg_probability_1': avg_prob_1,
                    'avg_confidence': avg_confidence,
                    'model_details': []
                }

                # æ·»åŠ æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
                for pred in predictions:
                    model_detail = {
                        'model_type': str(pred['model_type']),
                        'model_name': str(pred['model_name']),
                        'prediction': int(pred['prediction']),
                        'prediction_label': str(pred['prediction_label']),
                        'probability_0': float(pred.get('probability_0', 0.5)),
                        'probability_1': float(pred.get('probability_1', 0.5)),
                        'confidence': float(pred.get('confidence', 0.5))
                    }
                    consensus_info['model_details'].append(model_detail)

                all_positive.append(consensus_info)

        return all_positive

    def save_consensus_results(self, consensus_stats, output_dir):
        """ä¿å­˜å…±è¯†åˆ†æç»“æœ"""
        if not SAVE_CONSENSUS_RESULTS:
            return

        print(f"\nä¿å­˜å…±è¯†åˆ†æç»“æœ...")

        # åˆ›å»ºå…±è¯†åˆ†æå­ç›®å½•
        consensus_dir = os.path.join(output_dir, "consensus_analysis")
        os.makedirs(consensus_dir, exist_ok=True)

        for consensus_type, results in consensus_stats.items():
            if not results:
                continue

            print(f"  ä¿å­˜ {consensus_type} ç»“æœ: {len(results)} ä¸ªåŒ–åˆç‰©")

            # åˆ›å»ºç®€åŒ–çš„æ•°æ®è¡¨æ ¼
            simplified_data = []

            for result in results:
                simplified_row = {
                    'protein_id': result['protein_id'],
                    'compound_id': result['compound_id'],
                    'num_models': result['num_models'],
                    'consensus_type': result['consensus_type'],
                    'avg_probability_0': f"{result['avg_probability_0']:.4f}",
                    'avg_probability_1': f"{result['avg_probability_1']:.4f}",
                    'avg_confidence': f"{result['avg_confidence']:.4f}"
                }
                simplified_data.append(simplified_row)

            # ä¿å­˜ç®€åŒ–è¡¨æ ¼
            if simplified_data:
                try:
                    simplified_df = pd.DataFrame(simplified_data)
                    simplified_file = os.path.join(consensus_dir, f"{consensus_type}_summary.csv")
                    simplified_df.to_csv(simplified_file, index=False, encoding='utf-8-sig')
                    print(f"    âœ“ ç®€åŒ–è¡¨æ ¼: {simplified_file}")
                except Exception as e:
                    print(f"    âœ— ä¿å­˜ç®€åŒ–è¡¨æ ¼å¤±è´¥: {e}")

            # ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ä¿¡æ¯
            json_file = os.path.join(consensus_dir, f"{consensus_type}_complete.json")
            if safe_json_dump(results, json_file, indent=2, ensure_ascii=False):
                print(f"    âœ“ å®Œæ•´JSON: {json_file}")
            else:
                print(f"    âœ— å®Œæ•´JSONä¿å­˜å¤±è´¥: {json_file}")


# =============================================================================
# å¤ç”¨ä¸´æ—¶æ–‡ä»¶çš„é¢„æµ‹å™¨ç±»
# =============================================================================

class ReuseChunkPredictor:
    """å¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶çš„é¢„æµ‹å™¨"""

    def __init__(self, model_base_dir, selected_model_types=None, selected_models=None):
        self.model_base_dir = model_base_dir
        self.selected_model_types = selected_model_types or SELECTED_MODEL_TYPES
        self.selected_models = selected_models or SELECTED_MODELS
        self.models = {}
        self.feature_pipelines = {}
        self.available_models = {}

        # ç°æœ‰åˆ†å—ç®¡ç†å™¨
        self.chunk_manager = ExistingChunkManager()

        # åŠ¨æ€ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
        self.dynamic_batch_size = optimize_batch_size_by_memory()
        print(f"ğŸ¯ åŠ¨æ€æ‰¹å¤„ç†å¤§å°: {self.dynamic_batch_size:,}")

        # å…±è¯†åˆ†æå™¨
        if ENABLE_CONSENSUS_ANALYSIS:
            self.consensus_analyzer = ConsensusAnalyzer(
                min_consensus_models=MIN_CONSENSUS_MODELS,
                probability_threshold=CONSENSUS_PROBABILITY_THRESHOLD
            )

    def scan_available_models(self):
        """æ‰«ææ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        print("æ‰«æå¯ç”¨æ¨¡å‹...")

        self.available_models = {}

        for model_type in self.selected_model_types:
            model_dir = os.path.join(self.model_base_dir, model_type)

            if not os.path.exists(model_dir):
                print(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
                continue

            pipeline_path = os.path.join(model_dir, 'feature_pipeline.pkl')
            if not os.path.exists(pipeline_path):
                print(f"ç‰¹å¾ç®¡é“æ–‡ä»¶ä¸å­˜åœ¨: {pipeline_path}")
                continue

            model_files = [f for f in os.listdir(model_dir) if f.endswith('_model.pkl')]

            if model_files:
                available_model_names = []
                for f in model_files:
                    model_name = f.replace('_model.pkl', '')
                    available_model_names.append(model_name)

                if available_model_names:
                    self.available_models[model_type] = available_model_names
                    print(f"  {model_type}: {len(available_model_names)} ä¸ªå¯ç”¨æ¨¡å‹")
                    for model_name in available_model_names:
                        print(f"    - {model_name}")

        total_available = sum(len(models) for models in self.available_models.values())
        print(f"\næ€»å…±æ‰«æåˆ° {total_available} ä¸ªå¯ç”¨æ¨¡å‹")

        return total_available > 0

    @monitor_memory("æ¨¡å‹åŠ è½½")
    def load_selected_models(self):
        """åŠ è½½é€‰æ‹©çš„æ¨¡å‹"""
        print("\nå¼€å§‹åŠ è½½é€‰æ‹©çš„æ¨¡å‹...")

        loaded_count = 0

        for model_type in self.selected_model_types:
            if model_type not in self.available_models:
                continue

            model_dir = os.path.join(self.model_base_dir, model_type)
            print(f"\nåŠ è½½ {model_type} æ¨¡å‹...")

            # åŠ è½½ç‰¹å¾ç®¡é“
            pipeline_path = os.path.join(model_dir, 'feature_pipeline.pkl')
            try:
                self.feature_pipelines[model_type] = joblib.load(pipeline_path)
                print(f"  âœ“ ç‰¹å¾ç®¡é“å·²åŠ è½½")
            except Exception as e:
                print(f"  âœ— åŠ è½½ç‰¹å¾ç®¡é“å¤±è´¥: {e}")
                continue

            # ç¡®å®šè¦åŠ è½½çš„æ¨¡å‹åˆ—è¡¨
            if model_type in self.selected_models:
                models_to_load = self.selected_models[model_type]
            else:
                models_to_load = self.available_models[model_type]

            # åŠ è½½æ¨¡å‹
            self.models[model_type] = {}

            for model_name in models_to_load:
                model_path = os.path.join(model_dir, f'{model_name}_model.pkl')

                if os.path.exists(model_path):
                    try:
                        print(f"  æ­£åœ¨åŠ è½½ {model_name}...")
                        model = joblib.load(model_path)
                        self.models[model_type][model_name] = model
                        print(f"  âœ“ {model_name} æ¨¡å‹å·²åŠ è½½")
                        loaded_count += 1
                    except Exception as e:
                        print(f"  âœ— åŠ è½½ {model_name} æ¨¡å‹å¤±è´¥: {e}")
                else:
                    print(f"  âœ— {model_name} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        print(f"\næ¨¡å‹åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {loaded_count} ä¸ªæ¨¡å‹")
        return loaded_count > 0

    def load_and_prepare_features_reuse(self, protein_file, compound_file):
        """å¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶çš„ç‰¹å¾æ•°æ®åŠ è½½"""
        print(f"\nåŠ è½½ç‰¹å¾æ–‡ä»¶ï¼ˆå¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶ç‰ˆï¼‰...")

        # åŠ è½½è›‹ç™½è´¨ç‰¹å¾
        try:
            protein_df = pd.read_csv(protein_file)
            print(f"  è›‹ç™½è´¨ç‰¹å¾æ–‡ä»¶: {protein_df.shape}")
        except Exception as e:
            raise ValueError(f"åŠ è½½è›‹ç™½è´¨ç‰¹å¾æ–‡ä»¶å¤±è´¥: {e}")

        # å°è¯•å¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶
        chunk_info = None
        if REUSE_EXISTING_CHUNKS:
            print(f"\nğŸ”„ å°è¯•å¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶...")

            # æ‰«æç°æœ‰åˆ†å—
            existing_chunks = self.chunk_manager.scan_existing_chunks()

            if existing_chunks:
                # é€‰æ‹©åˆ†å—ç›®å½•
                if SPECIFIC_CHUNK_DIR:
                    # ä½¿ç”¨æŒ‡å®šç›®å½•
                    for chunk in existing_chunks:
                        if chunk['dir_path'] == SPECIFIC_CHUNK_DIR:
                            selected_chunk = chunk
                            break
                    else:
                        print(f"âŒ æŒ‡å®šçš„åˆ†å—ç›®å½•ä¸å­˜åœ¨: {SPECIFIC_CHUNK_DIR}")
                        selected_chunk = None
                else:
                    # è‡ªåŠ¨é€‰æ‹©æˆ–äº¤äº’é€‰æ‹©
                    selected_chunk = self.chunk_manager.select_chunk_directory()

                if selected_chunk:
                    chunk_info = self.chunk_manager.get_chunk_file_info()

                    if chunk_info:
                        print(f"âœ… æˆåŠŸå¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶:")
                        print(f"   åˆ†å—ç›®å½•: {chunk_info['chunk_dir']}")
                        print(f"   æ–‡ä»¶æ•°é‡: {chunk_info['total_files']}")
                        print(f"   æ€»è¡Œæ•°: {chunk_info['total_rows']:,}")
                    else:
                        print(f"âŒ ç°æœ‰åˆ†å—æ–‡ä»¶éªŒè¯å¤±è´¥")
                        chunk_info = None

            if not chunk_info:
                print(f"ğŸ’« å°†åˆ›å»ºæ–°çš„åˆ†å—æ–‡ä»¶...")

        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„ç°æœ‰åˆ†å—ï¼Œåˆ›å»ºæ–°çš„åˆ†å—
        if not chunk_info:
            print(f"âš ï¸ æ— æ³•å¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶ï¼Œéœ€è¦é‡æ–°åˆ†å—")
            print(f"   è¿™å°†éœ€è¦é¢å¤–çš„æ—¶é—´å’Œç£ç›˜ç©ºé—´")

            # è¿™é‡Œå¯ä»¥é€‰æ‹©æ˜¯å¦ç»§ç»­æˆ–é€€å‡º
            choice = input(f"æ˜¯å¦ç»§ç»­é‡æ–°åˆ†å—ï¼Ÿ(y/n): ").strip().lower()
            if choice != 'y':
                raise RuntimeError("ç”¨æˆ·å–æ¶ˆé‡æ–°åˆ†å—")

            # é‡æ–°åˆ†å—é€»è¾‘ï¼ˆå¯ä»¥è°ƒç”¨åŸæ¥çš„åˆ†å—å‡½æ•°ï¼‰
            raise NotImplementedError("é‡æ–°åˆ†å—åŠŸèƒ½éœ€è¦å•ç‹¬å®ç°")

        # å¤„ç†è›‹ç™½è´¨ç‰¹å¾æ•°æ®
        if protein_df.shape[1] > 1:
            protein_ids = protein_df.iloc[:, 0].values
            protein_features_matrix = protein_df.iloc[:, 1:].values
        else:
            protein_ids = [f"Protein_{i + 1}" for i in range(len(protein_df))]
            protein_features_matrix = protein_df.values

        # è®¡ç®—æ€»ç»„åˆæ•°
        total_combinations = len(protein_ids) * chunk_info['total_rows']
        print(f"  æ€»ç»„åˆæ•°: {len(protein_ids)} Ã— {chunk_info['total_rows']:,} = {total_combinations:,}")

        return {
            'protein_ids': protein_ids,
            'protein_features': protein_features_matrix,
            'compound_chunk_files': chunk_info['chunk_files'],
            'compound_chunk_info': chunk_info['chunk_info'],
            'total_combinations': total_combinations,
            'use_chunked_processing': True,
            'dynamic_batch_size': self.dynamic_batch_size,
            'reused_chunks': True,
            'chunk_dir': chunk_info['chunk_dir']
        }

    def apply_feature_pipeline(self, features, model_type):
        """åº”ç”¨ç‰¹å¾å·¥ç¨‹ç®¡é“"""
        if model_type not in self.feature_pipelines:
            raise ValueError(f"æœªæ‰¾åˆ° {model_type} çš„ç‰¹å¾ç®¡é“")

        pipeline = self.feature_pipelines[model_type]

        # ç¡®ä¿è¾“å…¥æ˜¯2Dæ•°ç»„
        if features.ndim == 1:
            features = features.reshape(1, -1)

        # åº”ç”¨å¡«å……å™¨
        if 'imputer' in pipeline and pipeline['imputer'] is not None:
            try:
                features = pipeline['imputer'].transform(features)
            except Exception as e:
                print(f"åº”ç”¨å¡«å……å™¨å¤±è´¥: {e}")

        # åº”ç”¨ç‰¹å¾é€‰æ‹©
        if 'selected_features' in pipeline and pipeline['selected_features']:
            try:
                selected_count = len(pipeline['selected_features'])
                if features.shape[1] >= selected_count:
                    features = features[:, :selected_count]
                else:
                    padding = np.zeros((features.shape[0], selected_count - features.shape[1]))
                    features = np.hstack([features, padding])
            except Exception as e:
                print(f"åº”ç”¨ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")

        # åº”ç”¨é™ç»´å™¨
        if 'reducer' in pipeline and pipeline['reducer'] is not None:
            try:
                features = pipeline['reducer'].transform(features)
            except Exception as e:
                print(f"åº”ç”¨é™ç»´å™¨å¤±è´¥: {e}")

        # åº”ç”¨ç¼©æ”¾å™¨
        if 'scaler' in pipeline and pipeline['scaler'] is not None:
            try:
                features = pipeline['scaler'].transform(features)
            except Exception as e:
                print(f"åº”ç”¨ç¼©æ”¾å™¨å¤±è´¥: {e}")

        # å¤„ç†å¯èƒ½çš„NaNå’Œinfå€¼
        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)

        return features

    @monitor_memory("å¤ç”¨åˆ†å—é¢„æµ‹")
    def predict_chunk_reuse(self, data_info, model_type, model_name, model):
        """ä½¿ç”¨å¤ç”¨åˆ†å—æ–‡ä»¶çš„é¢„æµ‹"""
        print(f"    å¼€å§‹å¤ç”¨åˆ†å—é¢„æµ‹: {model_name}")

        protein_ids = data_info['protein_ids']
        protein_features = data_info['protein_features']
        chunk_files = data_info['compound_chunk_files']
        batch_size = data_info['dynamic_batch_size']

        results = []
        total_processed = 0

        try:
            # éªŒè¯ä¸´æ—¶æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for chunk_file in chunk_files:
                if not os.path.exists(chunk_file):
                    raise FileNotFoundError(f"å¤ç”¨çš„åˆ†å—æ–‡ä»¶ä¸å­˜åœ¨: {chunk_file}")

            print(f"      âœ“ éªŒè¯é€šè¿‡ï¼Œæ‰€æœ‰ {len(chunk_files)} ä¸ªå¤ç”¨åˆ†å—æ–‡ä»¶å­˜åœ¨")
            print(f"      ğŸ“Š ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {batch_size:,}")
            print(f"      ğŸ”„ å¤ç”¨åˆ†å—ç›®å½•: {data_info['chunk_dir']}")

            # éå†æ¯ä¸ªåŒ–åˆç‰©å—æ–‡ä»¶
            for chunk_idx, chunk_file in enumerate(chunk_files):
                print(f"      å¤„ç†åŒ–åˆç‰©å— {chunk_idx + 1}/{len(chunk_files)}")

                try:
                    # åŠ è½½å½“å‰åŒ–åˆç‰©å—
                    compound_chunk_df = pd.read_csv(chunk_file)
                    print(f"        å¤ç”¨å—æ–‡ä»¶: {len(compound_chunk_df):,} è¡Œ")

                    # å¤„ç†åŒ–åˆç‰©ç‰¹å¾æ•°æ®
                    if compound_chunk_df.shape[1] > 1:
                        compound_ids = compound_chunk_df.iloc[:, 0].values
                        compound_features_matrix = compound_chunk_df.iloc[:, 1:].values
                    else:
                        compound_ids = [f"Compound_chunk{chunk_idx}_{i + 1}" for i in range(len(compound_chunk_df))]
                        compound_features_matrix = compound_chunk_df.values

                    # éå†æ¯ä¸ªè›‹ç™½è´¨
                    for protein_idx, protein_id in enumerate(protein_ids):
                        protein_feature = protein_features[protein_idx]

                        # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å°æ‰¹æ¬¡å¤„ç†åŒ–åˆç‰©
                        for start_idx in range(0, len(compound_ids), batch_size):
                            end_idx = min(start_idx + batch_size, len(compound_ids))

                            if ENABLE_MEMORY_MONITORING and start_idx % (batch_size * 5) == 0:
                                memory_info = get_memory_usage()
                                print(
                                    f"          æ‰¹æ¬¡ {start_idx // batch_size + 1}: å†…å­˜ {memory_info['rss_mb']:.1f}MB")

                            # ç»„åˆå½“å‰æ‰¹æ¬¡çš„ç‰¹å¾
                            batch_features = []
                            batch_combinations = []

                            for compound_idx in range(start_idx, end_idx):
                                compound_id = compound_ids[compound_idx]
                                compound_feature = compound_features_matrix[compound_idx]
                                combined_feature = np.concatenate([protein_feature, compound_feature])

                                batch_features.append(combined_feature)
                                batch_combinations.append((protein_id, compound_id))

                            if not batch_features:
                                continue

                            # è½¬æ¢ä¸ºnumpyæ•°ç»„
                            batch_features = np.array(batch_features)

                            try:
                                # åº”ç”¨ç‰¹å¾å·¥ç¨‹ç®¡é“
                                features_processed = self.apply_feature_pipeline(batch_features, model_type)

                                # è¿›è¡Œé¢„æµ‹
                                predictions = model.predict(features_processed)

                                # è·å–æ¦‚ç‡
                                probabilities = None
                                if hasattr(model, 'predict_proba'):
                                    try:
                                        probabilities = model.predict_proba(features_processed)
                                    except Exception as e:
                                        print(f"          è·å–æ¦‚ç‡å¤±è´¥: {e}")
                                        probabilities = None

                                # æ„å»ºç»“æœ
                                for k, (prot_id, comp_id) in enumerate(batch_combinations):
                                    result = {
                                        'protein_id': str(prot_id),
                                        'compound_id': str(comp_id),
                                        'model_type': str(model_type),
                                        'model_name': str(model_name),
                                        'prediction': int(predictions[k]),
                                        'prediction_label': 'ç›¸äº’ä½œç”¨' if int(predictions[k]) == 1 else 'æ— ç›¸äº’ä½œç”¨'
                                    }

                                    if probabilities is not None:
                                        result['probability_0'] = float(probabilities[k][0])
                                        result['probability_1'] = float(probabilities[k][1])
                                        result['confidence'] = float(max(probabilities[k]))
                                    else:
                                        result['probability_0'] = 0.5
                                        result['probability_1'] = 0.5
                                        result['confidence'] = 0.5

                                    results.append(result)

                                total_processed += len(batch_combinations)

                            except MemoryError as e:
                                print(f"          âŒ æ‰¹æ¬¡å¤„ç†å†…å­˜ä¸è¶³: {e}")
                                print(f"          ğŸ”„ è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡")
                                gc.collect()
                                continue
                            except Exception as e:
                                print(f"          âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                                continue
                            finally:
                                # ä¸»åŠ¨æ¸…ç†å†…å­˜
                                del batch_features, features_processed, predictions
                                if probabilities is not None:
                                    del probabilities
                                gc.collect()

                        # è›‹ç™½è´¨å¤„ç†å®Œæˆåçš„å†…å­˜æ¸…ç†
                        if protein_idx % 5 == 0:
                            gc.collect()

                    # æ¸…ç†å½“å‰åŒ–åˆç‰©å—
                    del compound_chunk_df, compound_ids, compound_features_matrix
                    gc.collect()

                    print(f"        å®Œæˆå— {chunk_idx + 1}ï¼Œå·²å¤„ç† {total_processed:,} ä¸ªç»„åˆ")

                except MemoryError as e:
                    print(f"      âŒ å¤„ç†åŒ–åˆç‰©å— {chunk_idx} å†…å­˜ä¸è¶³: {e}")
                    gc.collect()
                    continue
                except Exception as e:
                    print(f"      âŒ å¤„ç†åŒ–åˆç‰©å— {chunk_idx} å¤±è´¥: {e}")
                    continue

        except Exception as e:
            print(f"    âœ— {model_name} é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")

        print(f"    âœ“ {model_name} é¢„æµ‹å®Œæˆï¼Œå…± {len(results)} ä¸ªç»“æœ")
        return results

    def predict_and_save_reuse(self, data_info):
        """ä½¿ç”¨å¤ç”¨åˆ†å—æ–‡ä»¶çš„é¢„æµ‹å’Œä¿å­˜"""
        print(f"\nå¼€å§‹å¤ç”¨åˆ†å—æ–‡ä»¶é¢„æµ‹å¹¶ä¿å­˜ç»“æœ...")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = os.path.join(SCRIPT_DIR, OUTPUT_BASE_DIR, f"reuse_chunks_{timestamp}")
        os.makedirs(output_dir, exist_ok=True)

        all_results = []
        model_summary = {}

        # è®°å½•å¤ç”¨ä¿¡æ¯
        reuse_info = {
            'reused_chunks': data_info.get('reused_chunks', False),
            'chunk_dir': data_info.get('chunk_dir', ''),
            'total_files': len(data_info['compound_chunk_files']),
            'total_combinations': data_info['total_combinations'],
            'dynamic_batch_size': data_info['dynamic_batch_size']
        }

        try:
            for model_type in self.models:
                if not self.models[model_type]:
                    continue

                print(f"\nä½¿ç”¨ {model_type} æ¨¡å‹è¿›è¡Œé¢„æµ‹...")

                # ä¸ºæ¯ä¸ªæ¨¡å‹ç±»å‹åˆ›å»ºå­ç›®å½•
                type_dir = os.path.join(output_dir, model_type.replace('/', '_'))
                os.makedirs(type_dir, exist_ok=True)

                model_summary[model_type] = {}

                for model_name, model in self.models[model_type].items():
                    print(f"  é¢„æµ‹æ¨¡å‹: {model_name}")

                    # ä½¿ç”¨å¤ç”¨åˆ†å—æ–‡ä»¶çš„é¢„æµ‹
                    model_results = self.predict_chunk_reuse(data_info, model_type, model_name, model)

                    if model_results:
                        # ä¿å­˜ç»“æœ
                        model_file = os.path.join(type_dir, f"{model_name}_prediction.csv")

                        try:
                            results_df = pd.DataFrame(model_results)
                            results_df.to_csv(model_file, index=False, encoding='utf-8-sig')
                            print(f"    âœ“ ç»“æœå·²ä¿å­˜: {model_file}")
                        except Exception as e:
                            print(f"    âœ— ä¿å­˜ç»“æœå¤±è´¥: {e}")

                        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                        positive_count = sum(1 for r in model_results if r['prediction'] == 1)
                        negative_count = len(model_results) - positive_count

                        model_summary[model_type][model_name] = {
                            'total_predictions': len(model_results),
                            'positive_predictions': positive_count,
                            'negative_predictions': negative_count,
                            'positive_ratio': positive_count / len(model_results) if model_results else 0,
                            'output_file': model_file
                        }

                        print(f"    é¢„æµ‹ç»“æœ: {positive_count:,} ä¸ªç›¸äº’ä½œç”¨, {negative_count:,} ä¸ªæ— ç›¸äº’ä½œç”¨")

                        # æ”¶é›†ç”¨äºå…±è¯†åˆ†æ
                        if ENABLE_CONSENSUS_ANALYSIS:
                            all_results.extend(model_results)

                        # æ¸…ç†å†…å­˜
                        del model_results
                        gc.collect()

                    else:
                        print(f"    âœ— é¢„æµ‹å¤±è´¥")

            # è¿›è¡Œå…±è¯†åˆ†æ
            if ENABLE_CONSENSUS_ANALYSIS and all_results:
                print(f"\nå¼€å§‹å…±è¯†åˆ†æï¼Œæ€»å…± {len(all_results)} ä¸ªé¢„æµ‹ç»“æœ...")
                try:
                    consensus_stats = self.consensus_analyzer.analyze_consensus(all_results)
                    self.consensus_analyzer.save_consensus_results(consensus_stats, output_dir)
                except Exception as e:
                    print(f"å…±è¯†åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")

        finally:
            # æ³¨æ„ï¼šå¤ç”¨çš„åˆ†å—æ–‡ä»¶ä¸éœ€è¦æ¸…ç†ï¼Œå®ƒä»¬å¯èƒ½è¿˜ä¼šè¢«å…¶ä»–ä»»åŠ¡ä½¿ç”¨
            print(f"\nğŸ“ å¤ç”¨çš„åˆ†å—æ–‡ä»¶ä¿ç•™åœ¨: {data_info.get('chunk_dir', 'æœªçŸ¥')}")

        # ä¿å­˜å¤ç”¨ä¿¡æ¯
        reuse_info_file = os.path.join(output_dir, "reuse_info.json")
        if safe_json_dump(reuse_info, reuse_info_file, indent=2, ensure_ascii=False):
            print(f"å¤ç”¨ä¿¡æ¯å·²ä¿å­˜: {reuse_info_file}")

        # ä¿å­˜æ¨¡å‹å¯¹æ¯”ç»“æœ
        comparison_data = []
        for model_type, models in model_summary.items():
            for model_name, stats in models.items():
                comparison_data.append({
                    'model_type': model_type,
                    'model_name': model_name,
                    'total_predictions': stats['total_predictions'],
                    'positive_predictions': stats['positive_predictions'],
                    'negative_predictions': stats['negative_predictions'],
                    'positive_ratio': f"{stats['positive_ratio']:.4f}"
                })

        if comparison_data:
            try:
                comparison_df = pd.DataFrame(comparison_data)
                comparison_file = os.path.join(output_dir, "model_comparison.csv")
                comparison_df.to_csv(comparison_file, index=False, encoding='utf-8-sig')
                print(f"\næ¨¡å‹å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_file}")
            except Exception as e:
                print(f"ä¿å­˜æ¨¡å‹å¯¹æ¯”ç»“æœå¤±è´¥: {e}")

        # ä¿å­˜é¢„æµ‹æ‘˜è¦
        summary_info = {
            'model_summary': model_summary,
            'reuse_info': reuse_info
        }
        summary_file = os.path.join(output_dir, "prediction_summary.json")
        if safe_json_dump(summary_info, summary_file, indent=2, ensure_ascii=False):
            print(f"é¢„æµ‹æ‘˜è¦å·²ä¿å­˜: {summary_file}")

        print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨ç›®å½•: {output_dir}")

        return all_results, model_summary, output_dir


def main():
    """å¤ç”¨ä¸´æ—¶æ–‡ä»¶ç‰ˆæœ¬çš„ä¸»å‡½æ•°"""
    try:
        print(f"è›‹ç™½è´¨-åŒ–åˆç‰©ç›¸äº’ä½œç”¨é¢„æµ‹å™¨ï¼ˆå¤ç”¨ä¸´æ—¶æ–‡ä»¶ç‰ˆï¼‰")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å½“å‰ç”¨æˆ·: woyaokaoyanhaha")
        print(f"è„šæœ¬ç›®å½•: {SCRIPT_DIR}")
        print("=" * 80)

        # æ˜¾ç¤ºå¤ç”¨é…ç½®
        print(f"å¤ç”¨ä¸´æ—¶æ–‡ä»¶é…ç½®:")
        print(f"  å¤ç”¨ç°æœ‰åˆ†å—: {REUSE_EXISTING_CHUNKS}")
        print(f"  è‡ªåŠ¨æ£€æµ‹åˆ†å—: {AUTO_DETECT_CHUNKS}")
        print(f"  æŒ‡å®šåˆ†å—ç›®å½•: {SPECIFIC_CHUNK_DIR or 'è‡ªåŠ¨æ£€æµ‹'}")
        print(f"  å†…å­˜æ‰¹å¤„ç†å¤§å°: {MEMORY_BATCH_SIZE:,}")
        print(f"  æœ€å¤§å†…å­˜é™åˆ¶: {MAX_MEMORY_GB} GB")
        print(f"  å†…å­˜ç›‘æ§: {ENABLE_MEMORY_MONITORING}")
        print(f"  å…±è¯†åˆ†æ: {ENABLE_CONSENSUS_ANALYSIS}")
        print(f"  æœ¬åœ°ä¸´æ—¶ç›®å½•: {LOCAL_TEMP_DIR}")

        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not os.path.exists(PROTEIN_FEATURE_FILE):
            raise FileNotFoundError(f"è›‹ç™½è´¨ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {PROTEIN_FEATURE_FILE}")

        if not os.path.exists(COMPOUND_FEATURE_FILE):
            raise FileNotFoundError(f"åŒ–åˆç‰©ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {COMPOUND_FEATURE_FILE}")

        if not os.path.exists(MODEL_BASE_DIR):
            raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_BASE_DIR}")

        # åˆå§‹åŒ–å¤ç”¨é¢„æµ‹å™¨
        predictor = ReuseChunkPredictor(MODEL_BASE_DIR, SELECTED_MODEL_TYPES, SELECTED_MODELS)

        # æ‰«æå’ŒåŠ è½½æ¨¡å‹
        if not predictor.scan_available_models():
            raise RuntimeError("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹")

        if not predictor.load_selected_models():
            raise RuntimeError("æœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹")

        # åŠ è½½å’Œå‡†å¤‡ç‰¹å¾æ•°æ®ï¼ˆå¤ç”¨ç‰ˆï¼‰
        data_info = predictor.load_and_prepare_features_reuse(PROTEIN_FEATURE_FILE, COMPOUND_FEATURE_FILE)

        print(f"\nå‡†å¤‡å¼€å§‹é¢„æµ‹...")
        print(f"æ•°æ®ä¿¡æ¯:")
        print(f"  æ€»ç»„åˆæ•°: {data_info['total_combinations']:,}")
        print(f"  åŒ–åˆç‰©å—æ•°: {len(data_info['compound_chunk_files'])}")
        print(f"  åŠ¨æ€æ‰¹å¤„ç†å¤§å°: {data_info['dynamic_batch_size']:,}")
        print(f"  ä½¿ç”¨å¤ç”¨åˆ†å—: {'æ˜¯' if data_info.get('reused_chunks') else 'å¦'}")
        if data_info.get('reused_chunks'):
            print(f"  å¤ç”¨åˆ†å—ç›®å½•: {data_info.get('chunk_dir', 'æœªçŸ¥')}")

        # ç¡®è®¤å¼€å§‹é¢„æµ‹
        confirm = input(f"\nç¡®è®¤å¼€å§‹é¢„æµ‹å—ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆé¢„æµ‹")
            return

        # è¿›è¡Œå¤ç”¨åˆ†å—é¢„æµ‹
        all_results, model_summary, output_dir = predictor.predict_and_save_reuse(data_info)

        print(f"\nğŸ‰ é¢„æµ‹å®Œæˆï¼")
        print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")

        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        total_models = sum(len(models) for models in model_summary.values())
        total_predictions = sum(
            stats['total_predictions']
            for model_type_stats in model_summary.values()
            for stats in model_type_stats.values()
        )

        print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
        print(f"  ä½¿ç”¨æ¨¡å‹æ•°: {total_models}")
        print(f"  æ€»é¢„æµ‹æ•°: {total_predictions:,}")
        print(f"  å¤ç”¨åˆ†å—æ–‡ä»¶: {'æˆåŠŸ' if data_info.get('reused_chunks') else 'æœªä½¿ç”¨'} âœ…")
        print(f"  å†…å­˜ä¼˜åŒ–: æˆåŠŸé¿å…å†…å­˜æº¢å‡º âœ…")

        return output_dir

    except Exception as e:
        print(f"âŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    print(f"ğŸš€ å¯åŠ¨å¤ç”¨ä¸´æ—¶æ–‡ä»¶é¢„æµ‹å™¨")
    print(f"ğŸ“… å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ‘¤ å½“å‰ç”¨æˆ·: woyaokaoyanhaha")
    print(f"ğŸ“‚ è„šæœ¬ä½ç½®: {SCRIPT_DIR}")
    print(f"ğŸ“ æœ¬åœ°ä¸´æ—¶ç›®å½•: {LOCAL_TEMP_DIR}")
    print(f"ğŸ”„ åŠŸèƒ½ç‰¹ç‚¹: å¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶ï¼Œé¿å…é‡å¤åˆ†å—")
    print("=" * 80)

    result_dir = main()

    if result_dir:
        print(f"\nğŸŒŸ é¢„æµ‹æˆåŠŸå®Œæˆï¼")
        print(f"ğŸŒŸ ç»“æœç›®å½•: {result_dir}")
        print(f"ğŸ”„ æˆåŠŸå¤ç”¨ç°æœ‰åˆ†å—æ–‡ä»¶ï¼ŒèŠ‚çœæ—¶é—´å’Œç£ç›˜ç©ºé—´")
    else:
        print(f"\nâŒ é¢„æµ‹å¤±è´¥")

    print(f"\nâ° ç¨‹åºç»“æŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")