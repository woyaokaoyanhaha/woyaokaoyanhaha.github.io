#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BBBè‚½é¢„æµ‹ç³»ç»Ÿ - åŸºäºESM2ç‰¹å¾æå–å’Œå¤šç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆç‰¹å¾ç‹¬ç«‹å­˜å‚¨ç‰ˆï¼‰
æ”¯æŒdataæ–‡ä»¶å¤¹ç»Ÿä¸€ç®¡ç†å’ŒESM2ç‰¹å¾æ–‡ä»¶ç‹¬ç«‹å­˜å‚¨
Author: AI Assistant
Date: 2025-01-27
"""

import os
import sys
import numpy as np
import pandas as pd
import warnings
import joblib
import json
import hashlib
from datetime import datetime
from collections import Counter
import multiprocessing as mp
from Bio import SeqIO
import torch
import torch.nn.functional as F

# ESM2æ¨¡å‹ç›¸å…³
try:
    import esm
    ESM_AVAILABLE = True
    print("âœ… ESMæ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError:
    ESM_AVAILABLE = False
    print("âš ï¸ ESMæ¨¡å—æœªå®‰è£…ï¼Œè¯·ä½¿ç”¨: pip install fair-esm")

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import (
    train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix,
    roc_curve, precision_recall_curve
)

# æ¨¡å‹ç®—æ³•
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.ensemble import VotingClassifier, StackingClassifier

# å‚æ•°åˆ†å¸ƒ
from scipy.stats import randint, uniform

# å¯è§†åŒ–
try:
    import matplotlib.pyplot as plt
    import matplotlib
    from matplotlib.font_manager import FontProperties
    import seaborn as sns
    VISUALIZATION_ENABLED = True
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    matplotlib.rcParams['font.family'] = ['SimHei'] if os.name == 'nt' else ['Arial Unicode MS']
    
except ImportError:
    print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œå°†è·³è¿‡å¯è§†åŒ–åŠŸèƒ½")
    VISUALIZATION_ENABLED = False

# é«˜çº§æ¨¡å‹åº“
try:
    import xgboost as xgb
    xgb_installed = True
except ImportError:
    print("âš ï¸ XGBoostæœªå®‰è£…")
    xgb_installed = False

try:
    import lightgbm as lgb
    lgbm_installed = True
except ImportError:
    print("âš ï¸ LightGBMæœªå®‰è£…")
    lgbm_installed = False

try:
    import catboost as cb
    catboost_installed = True
except ImportError:
    print("âš ï¸ CatBoostæœªå®‰è£…")
    catboost_installed = False

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# =============================================================================
# é…ç½®å‚æ•°
# =============================================================================

# ğŸ“ æ–‡ä»¶è·¯å¾„é…ç½® - ç»Ÿä¸€æ”¾åœ¨dataæ–‡ä»¶å¤¹
DATA_DIR = "data"
POS_TEST_FILE = os.path.join(DATA_DIR, "pos_test.fasta")
TRAIN_POS_ORG_FILE = os.path.join(DATA_DIR, "train_pos_org.fasta")
NEG_TRAIN_FILE = os.path.join(DATA_DIR, "neg_train.fasta")
NEG_TEST_FILE = os.path.join(DATA_DIR, "neg_test.fasta")

# ä¼ªæ­£æ ·æœ¬æ–‡ä»¶é…ç½® - æ”¯æŒå¤šç§æ ¼å¼
PSEUDO_POS_FILES = [
    os.path.join(DATA_DIR, "df_all_final_1000.csv"),  # CSVæ ¼å¼
    os.path.join(DATA_DIR, "pseudo_positive.fasta"),   # FASTAæ ¼å¼ï¼ˆå¤‡é€‰ï¼‰
]

# ğŸ—‚ï¸ ESM2ç‰¹å¾å­˜å‚¨ç›®å½•é…ç½®
FEATURES_DIR = "esm2_features"
TRAIN_FEATURES_DIR = os.path.join(FEATURES_DIR, "train")
TEST_FEATURES_DIR = os.path.join(FEATURES_DIR, "test")
FEATURE_METADATA_FILE = os.path.join(FEATURES_DIR, "feature_metadata.json")

# ğŸ”® ESM2é…ç½®
ESM2_MODEL_NAME = "esm2_t30_150M_UR50D"  # å¯é€‰: esm2_t33_650M_UR50D, esm2_t36_3B_UR50D
ESM2_REPRESENTATION_LAYER = 30  # å¯¹åº”æ¨¡å‹çš„æœ€åä¸€å±‚
ESM2_BATCH_SIZE = 4
ESM2_MAX_LENGTH = 1022  # ESM2æœ€å¤§åºåˆ—é•¿åº¦

# ğŸ“Š è®­ç»ƒé…ç½®
TEST_SIZE = 0.2
RANDOM_STATE = 42
CV_FOLDS = 5
OPTIMIZATION_TARGET = 'roc_auc'

# ğŸ” å‚æ•°æœç´¢é…ç½®
USE_RANDOMIZED_SEARCH = True
RANDOMIZED_SEARCH_ITERATIONS = 50
EXTENDED_SEARCH_ENABLED = True

# ğŸ“ˆ å¯è§†åŒ–é…ç½®
PLOT_DPI = 300
SAVE_PLOTS = True

# âš¡ æ€§èƒ½é…ç½®
CPU_COUNT = min(4, mp.cpu_count())

# ğŸ¨ è¾“å‡ºé…ç½®
OUTPUT_DIR = "bbb_peptide_results"

# =============================================================================
# ç‰¹å¾ç›®å½•ç®¡ç†
# =============================================================================

def create_feature_directories():
    """åˆ›å»ºç‰¹å¾å­˜å‚¨ç›®å½•ç»“æ„"""
    directories = [FEATURES_DIR, TRAIN_FEATURES_DIR, TEST_FEATURES_DIR]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"ğŸ“ åˆ›å»º/ç¡®è®¤ç›®å½•: {directory}")

def generate_feature_hash(sequences, dataset_type):
    """ä¸ºåºåˆ—é›†åˆç”Ÿæˆå”¯ä¸€çš„å“ˆå¸Œæ ‡è¯†"""
    sequences_str = ''.join(sequences) + dataset_type
    hash_obj = hashlib.md5(sequences_str.encode())
    return hash_obj.hexdigest()[:12]

def get_feature_filename(dataset_type, sequences, model_name=ESM2_MODEL_NAME):
    """ç”Ÿæˆç‰¹å¾æ–‡ä»¶å"""
    hash_id = generate_feature_hash(sequences, dataset_type)
    return f"{dataset_type}_{model_name}_{len(sequences)}seq_{hash_id}.npy"

def save_feature_metadata(metadata):
    """ä¿å­˜ç‰¹å¾å…ƒæ•°æ®"""
    with open(FEATURE_METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
    print(f"ğŸ’¾ ç‰¹å¾å…ƒæ•°æ®å·²ä¿å­˜: {FEATURE_METADATA_FILE}")

def load_feature_metadata():
    """åŠ è½½ç‰¹å¾å…ƒæ•°æ®"""
    if os.path.exists(FEATURE_METADATA_FILE):
        with open(FEATURE_METADATA_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

# =============================================================================
# æ•°æ®åŠ è½½å™¨ç±»
# =============================================================================

class BBBDataLoader:
    """BBBè‚½æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼"""
    
    def __init__(self, data_dir=DATA_DIR):
        self.data_dir = data_dir
        self.ensure_data_dir()
    
    def ensure_data_dir(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            print(f"ğŸ“ åˆ›å»ºæ•°æ®ç›®å½•: {self.data_dir}")
    
    def read_fasta_file(self, file_path):
        """è¯»å–FASTAæ–‡ä»¶"""
        sequences = []
        sequence_ids = []
        
        try:
            print(f"ğŸ“– è¯»å–FASTAæ–‡ä»¶: {file_path}")
            
            if not os.path.exists(file_path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return [], []
            
            with open(file_path, 'r') as handle:
                for record in SeqIO.parse(handle, "fasta"):
                    sequences.append(str(record.seq))
                    sequence_ids.append(record.id)
            
            print(f"âœ… è¯»å–å®Œæˆï¼Œå…± {len(sequences)} ä¸ªåºåˆ—")
            return sequences, sequence_ids
            
        except Exception as e:
            print(f"âŒ è¯»å–FASTAæ–‡ä»¶å¤±è´¥: {e}")
            return [], []
    
    def read_csv_peptides(self, file_path, peptide_column="peptide"):
        """è¯»å–CSVæ–‡ä»¶ä¸­çš„è‚½æ®µåºåˆ—"""
        sequences = []
        sequence_ids = []
        
        try:
            print(f"ğŸ“– è¯»å–CSVæ–‡ä»¶: {file_path}")
            
            if not os.path.exists(file_path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return [], []
            
            df = pd.read_csv(file_path)
            print(f"ğŸ“Š CSVæ–‡ä»¶å½¢çŠ¶: {df.shape}")
            print(f"ğŸ“‹ å¯ç”¨åˆ—å: {list(df.columns)}")
            
            # æ™ºèƒ½æŸ¥æ‰¾è‚½æ®µåˆ—
            possible_columns = [peptide_column, 'peptide', 'Peptide', 'sequence', 'Sequence', 'seq']
            peptide_col = None
            
            for col in possible_columns:
                if col in df.columns:
                    peptide_col = col
                    break
            
            if peptide_col is None:
                print(f"âŒ æœªæ‰¾åˆ°è‚½æ®µåˆ—ï¼Œå°è¯•è¿‡çš„åˆ—å: {possible_columns}")
                print(f"ğŸ“‹ å®é™…åˆ—å: {list(df.columns)}")
                return [], []
            
            print(f"âœ… ä½¿ç”¨åˆ—å: {peptide_col}")
            
            # æå–åºåˆ—å¹¶è¿‡æ»¤ç©ºå€¼
            sequences_raw = df[peptide_col].dropna().tolist()
            sequences = [str(seq).strip() for seq in sequences_raw if str(seq).strip()]
            sequence_ids = [f"csv_seq_{i+1}" for i in range(len(sequences))]
            
            print(f"âœ… è¯»å–å®Œæˆï¼Œå…± {len(sequences)} ä¸ªæœ‰æ•ˆåºåˆ—")
            return sequences, sequence_ids
            
        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            return [], []
    
    def load_pseudo_positive_samples(self):
        """æ™ºèƒ½åŠ è½½ä¼ªæ­£æ ·æœ¬æ•°æ®ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼"""
        all_sequences = []
        all_ids = []
        
        print("ğŸ” æœç´¢ä¼ªæ­£æ ·æœ¬æ–‡ä»¶...")
        
        for file_path in PSEUDO_POS_FILES:
            if not os.path.exists(file_path):
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.csv':
                sequences, seq_ids = self.read_csv_peptides(file_path)
            elif file_ext in ['.fasta', '.fa']:
                sequences, seq_ids = self.read_fasta_file(file_path)
            else:
                print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
                continue
            
            if sequences:
                all_sequences.extend(sequences)
                all_ids.extend(seq_ids)
                print(f"âœ… ä» {os.path.basename(file_path)} åŠ è½½ {len(sequences)} ä¸ªåºåˆ—")
                break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ–‡ä»¶å°±åœæ­¢
        
        if not all_sequences:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¼ªæ­£æ ·æœ¬æ–‡ä»¶")
            # å°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å«"pseudo"æˆ–æ•°å­—çš„æ–‡ä»¶
            print("ğŸ” å°è¯•åœ¨dataç›®å½•ä¸­æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ä¼ªæ­£æ ·æœ¬æ–‡ä»¶...")
            for filename in os.listdir(self.data_dir):
                if any(keyword in filename.lower() for keyword in ['pseudo', 'final', '1000']):
                    file_path = os.path.join(self.data_dir, filename)
                    print(f"ğŸ” å°è¯•æ–‡ä»¶: {file_path}")
                    
                    file_ext = os.path.splitext(filename)[1].lower()
                    if file_ext == '.csv':
                        sequences, seq_ids = self.read_csv_peptides(file_path)
                    elif file_ext in ['.fasta', '.fa']:
                        sequences, seq_ids = self.read_fasta_file(file_path)
                    else:
                        continue
                    
                    if sequences:
                        all_sequences.extend(sequences)
                        all_ids.extend(seq_ids)
                        print(f"âœ… æˆåŠŸä» {filename} åŠ è½½ {len(sequences)} ä¸ªåºåˆ—")
                        break
        
        print(f"ğŸ“Š ä¼ªæ­£æ ·æœ¬æ•°æ®æ±‡æ€»: {len(all_sequences)} ä¸ªåºåˆ—")
        return all_sequences, all_ids
    
    def load_all_training_data(self):
        """åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®")
        print("="*60)
        
        all_sequences = []
        all_labels = []
        all_ids = []
        all_sources = []
        
        # 1. åŠ è½½æ­£æ ·æœ¬è®­ç»ƒæ•°æ®
        pos_train_sequences, pos_train_ids = self.read_fasta_file(TRAIN_POS_ORG_FILE)
        if pos_train_sequences:
            all_sequences.extend(pos_train_sequences)
            all_labels.extend([1] * len(pos_train_sequences))
            all_ids.extend(pos_train_ids)
            all_sources.extend(['pos_train'] * len(pos_train_sequences))
            print(f"âœ… æ­£æ ·æœ¬è®­ç»ƒæ•°æ®: {len(pos_train_sequences)} ä¸ª")
        
        # 2. åŠ è½½ä¼ªæ­£æ ·æœ¬æ•°æ®
        pseudo_pos_sequences, pseudo_pos_ids = self.load_pseudo_positive_samples()
        if pseudo_pos_sequences:
            all_sequences.extend(pseudo_pos_sequences)
            all_labels.extend([1] * len(pseudo_pos_sequences))
            all_ids.extend(pseudo_pos_ids)
            all_sources.extend(['pseudo_pos'] * len(pseudo_pos_sequences))
            print(f"âœ… ä¼ªæ­£æ ·æœ¬æ•°æ®: {len(pseudo_pos_sequences)} ä¸ª")
        
        # 3. åŠ è½½è´Ÿæ ·æœ¬è®­ç»ƒæ•°æ®
        neg_train_sequences, neg_train_ids = self.read_fasta_file(NEG_TRAIN_FILE)
        if neg_train_sequences:
            all_sequences.extend(neg_train_sequences)
            all_labels.extend([0] * len(neg_train_sequences))
            all_ids.extend(neg_train_ids)
            all_sources.extend(['neg_train'] * len(neg_train_sequences))
            print(f"âœ… è´Ÿæ ·æœ¬è®­ç»ƒæ•°æ®: {len(neg_train_sequences)} ä¸ª")
        
        # æ•°æ®ç»Ÿè®¡
        print(f"\nğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»åºåˆ—æ•°: {len(all_sequences)}")
        if all_labels:
            pos_count = sum(all_labels)
            neg_count = len(all_labels) - pos_count
            print(f"  æ­£æ ·æœ¬æ•°: {pos_count} ({pos_count/len(all_labels)*100:.1f}%)")
            print(f"  è´Ÿæ ·æœ¬æ•°: {neg_count} ({neg_count/len(all_labels)*100:.1f}%)")
        
        return all_sequences, all_labels, all_ids, all_sources
    
    def load_all_test_data(self):
        """åŠ è½½æ‰€æœ‰æµ‹è¯•æ•°æ®"""
        print("\n" + "="*30)
        print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®")
        print("="*30)
        
        test_sequences = []
        test_labels = []
        test_ids = []
        test_sources = []
        
        # 1. åŠ è½½æ­£æ ·æœ¬æµ‹è¯•æ•°æ®
        pos_test_sequences, pos_test_ids = self.read_fasta_file(POS_TEST_FILE)
        if pos_test_sequences:
            test_sequences.extend(pos_test_sequences)
            test_labels.extend([1] * len(pos_test_sequences))
            test_ids.extend(pos_test_ids)
            test_sources.extend(['pos_test'] * len(pos_test_sequences))
            print(f"âœ… æ­£æ ·æœ¬æµ‹è¯•æ•°æ®: {len(pos_test_sequences)} ä¸ª")
        
        # 2. åŠ è½½è´Ÿæ ·æœ¬æµ‹è¯•æ•°æ®
        neg_test_sequences, neg_test_ids = self.read_fasta_file(NEG_TEST_FILE)
        if neg_test_sequences:
            test_sequences.extend(neg_test_sequences)
            test_labels.extend([0] * len(neg_test_sequences))
            test_ids.extend(neg_test_ids)
            test_sources.extend(['neg_test'] * len(neg_test_sequences))
            print(f"âœ… è´Ÿæ ·æœ¬æµ‹è¯•æ•°æ®: {len(neg_test_sequences)} ä¸ª")
        
        # æ•°æ®ç»Ÿè®¡
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»åºåˆ—æ•°: {len(test_sequences)}")
        if test_labels:
            pos_count = sum(test_labels)
            neg_count = len(test_labels) - pos_count
            print(f"  æ­£æ ·æœ¬æ•°: {pos_count} ({pos_count/len(test_labels)*100:.1f}%)")
            print(f"  è´Ÿæ ·æœ¬æ•°: {neg_count} ({neg_count/len(test_labels)*100:.1f}%)")
        
        return test_sequences, test_labels, test_ids, test_sources

# =============================================================================
# ESM2ç‰¹å¾æå–å™¨ - æ”¯æŒç‰¹å¾ç¼“å­˜
# =============================================================================

class ESM2FeatureExtractor:
    """ESM2ç‰¹å¾æå–å™¨ - æ”¯æŒç‰¹å¾ç¼“å­˜å’Œç‹¬ç«‹å­˜å‚¨"""
    
    def __init__(self, model_name=ESM2_MODEL_NAME, representation_layer=ESM2_REPRESENTATION_LAYER):
        self.model_name = model_name
        self.representation_layer = representation_layer
        self.model = None
        self.alphabet = None
        self.batch_converter = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def load_model(self):
        """åŠ è½½ESM2æ¨¡å‹"""
        if not ESM_AVAILABLE:
            raise ImportError("ESMæ¨¡å—æœªå®‰è£…ï¼Œè¯·ä½¿ç”¨: pip install fair-esm")
        
        print(f"ğŸ”¬ åŠ è½½ESM2æ¨¡å‹: {self.model_name}")
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        try:
            self.model, self.alphabet = esm.pretrained.load_model_and_alphabet(self.model_name)
            self.model = self.model.to(self.device)
            self.model.eval()
            self.batch_converter = self.alphabet.get_batch_converter()
            print(f"âœ… ESM2æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ ESM2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def check_cached_features(self, sequences, dataset_type):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„ç‰¹å¾æ–‡ä»¶"""
        feature_filename = get_feature_filename(dataset_type, sequences, self.model_name)
        
        if dataset_type == 'train':
            feature_path = os.path.join(TRAIN_FEATURES_DIR, feature_filename)
        else:
            feature_path = os.path.join(TEST_FEATURES_DIR, feature_filename)
            
        if os.path.exists(feature_path):
            print(f"ğŸ¯ å‘ç°ç¼“å­˜ç‰¹å¾æ–‡ä»¶: {feature_path}")
            try:
                features = np.load(feature_path)
                print(f"âœ… æˆåŠŸåŠ è½½ç¼“å­˜ç‰¹å¾ï¼Œå½¢çŠ¶: {features.shape}")
                return features
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç¼“å­˜ç‰¹å¾å¤±è´¥: {e}")
                return None
        
        return None
    
    def save_features(self, features, sequences, dataset_type, labels=None, seq_ids=None):
        """ä¿å­˜ç‰¹å¾æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•"""
        feature_filename = get_feature_filename(dataset_type, sequences, self.model_name)
        
        if dataset_type == 'train':
            feature_path = os.path.join(TRAIN_FEATURES_DIR, feature_filename)
        else:
            feature_path = os.path.join(TEST_FEATURES_DIR, feature_filename)
        
        try:
            # ä¿å­˜ç‰¹å¾æ–‡ä»¶
            np.save(feature_path, features)
            print(f"ğŸ’¾ ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜: {feature_path}")
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata = load_feature_metadata()
            feature_info = {
                'filename': feature_filename,
                'dataset_type': dataset_type,
                'model_name': self.model_name,
                'representation_layer': self.representation_layer,
                'sequence_count': len(sequences),
                'feature_dim': features.shape[1],
                'creation_time': datetime.now().isoformat(),
                'file_size_mb': os.path.getsize(feature_path) / (1024 * 1024)
            }
            
            if labels is not None:
                feature_info['label_distribution'] = {
                    'positive': int(sum(labels)),
                    'negative': int(len(labels) - sum(labels))
                }
            
            metadata[feature_filename] = feature_info
            save_feature_metadata(metadata)
            
            return feature_path
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç‰¹å¾æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def extract_features(self, sequences, dataset_type='unknown', labels=None, seq_ids=None, batch_size=ESM2_BATCH_SIZE):
        """æå–ESM2ç‰¹å¾ï¼Œæ”¯æŒç¼“å­˜æœºåˆ¶"""
        print(f"ğŸ§¬ å¤„ç† {dataset_type} æ•°æ®é›†çš„ESM2ç‰¹å¾...")
        
        # æ£€æŸ¥ç¼“å­˜
        cached_features = self.check_cached_features(sequences, dataset_type)
        if cached_features is not None:
            return cached_features
        
        # åŠ è½½æ¨¡å‹
        if self.model is None:
            if not self.load_model():
                return None
        
        print(f"ğŸ”¬ å¼€å§‹æå– {len(sequences)} ä¸ªåºåˆ—çš„ESM2ç‰¹å¾...")
        print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        all_features = []
        
        try:
            with torch.no_grad():
                for i in range(0, len(sequences), batch_size):
                    batch_sequences = sequences[i:i + batch_size]
                    
                    # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
                    batch_labels, batch_strs, batch_tokens = self.batch_converter([
                        (f"seq_{j}", seq[:ESM2_MAX_LENGTH]) 
                        for j, seq in enumerate(batch_sequences)
                    ])
                    
                    batch_tokens = batch_tokens.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    results = self.model(batch_tokens, repr_layers=[self.representation_layer])
                    
                    # æå–ç‰¹å¾ (ä½¿ç”¨åºåˆ—çº§åˆ«çš„å¹³å‡æ± åŒ–)
                    representations = results["representations"][self.representation_layer]
                    
                    # å¯¹æ¯ä¸ªåºåˆ—è®¡ç®—å¹³å‡ç‰¹å¾ (æ’é™¤ç‰¹æ®Štoken)
                    for j, seq in enumerate(batch_sequences):
                        seq_len = min(len(seq), ESM2_MAX_LENGTH)
                        # å–ä¸­é—´éƒ¨åˆ†ï¼Œæ’é™¤ [CLS] å’Œ [SEP] token
                        seq_repr = representations[j, 1:seq_len+1].mean(dim=0)
                        all_features.append(seq_repr.cpu().numpy())
                    
                    if (i // batch_size + 1) % 10 == 0:
                        print(f"  å·²å¤„ç†: {min(i + batch_size, len(sequences))}/{len(sequences)}")
            
            features_array = np.array(all_features)
            print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {features_array.shape}")
            
            # ä¿å­˜ç‰¹å¾æ–‡ä»¶
            self.save_features(features_array, sequences, dataset_type, labels, seq_ids)
            
            return features_array
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            return None

# =============================================================================
# æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ (ä¿æŒåŸæœ‰åŠŸèƒ½)
# =============================================================================

def get_extended_param_grids():
    """è·å–æ‰©å±•çš„å‚æ•°ç½‘æ ¼"""
    param_grids = {}
    
    # SVM
    if USE_RANDOMIZED_SEARCH:
        param_grids['SVM'] = {
            'C': uniform(0.01, 99.99),
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
            'kernel': ['rbf', 'poly', 'sigmoid'],
            'probability': [True]
        }
    else:
        param_grids['SVM'] = {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
            'kernel': ['rbf', 'linear'],
            'probability': [True]
        }
    
    # éšæœºæ£®æ—
    if USE_RANDOMIZED_SEARCH:
        param_grids['éšæœºæ£®æ—'] = {
            'n_estimators': randint(50, 500),
            'max_depth': [None, 5, 10, 15, 20, 25, 30],
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10),
            'max_features': ['sqrt', 'log2', None]
        }
    else:
        param_grids['éšæœºæ£®æ—'] = {
            'n_estimators': [50, 100, 200, 300],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10],
            'max_features': ['sqrt', 'log2']
        }
    
    # æ¢¯åº¦æå‡
    if USE_RANDOMIZED_SEARCH:
        param_grids['æ¢¯åº¦æå‡'] = {
            'n_estimators': randint(50, 300),
            'learning_rate': uniform(0.01, 0.29),
            'max_depth': randint(3, 12),
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10)
        }
    else:
        param_grids['æ¢¯åº¦æå‡'] = {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'max_depth': [3, 5, 7, 9]
        }
    
    # é€»è¾‘å›å½’
    if USE_RANDOMIZED_SEARCH:
        param_grids['é€»è¾‘å›å½’'] = {
            'C': uniform(0.01, 99.99),
            'penalty': ['l1', 'l2', None],
            'solver': ['liblinear', 'lbfgs', 'saga'],
            'max_iter': [1000, 2000, 3000, 4000, 5000]
        }
    else:
        param_grids['é€»è¾‘å›å½’'] = {
            'C': [0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'lbfgs'],
            'max_iter': [1000]
        }
    
    # Kè¿‘é‚»
    if USE_RANDOMIZED_SEARCH:
        param_grids['Kè¿‘é‚»'] = {
            'n_neighbors': randint(3, 30),
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],
            'p': [1, 2, 3, 4, 5]
        }
    else:
        param_grids['Kè¿‘é‚»'] = {
            'n_neighbors': [3, 5, 7, 9, 11],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }
    
    # æç«¯éšæœºæ ‘
    if USE_RANDOMIZED_SEARCH:
        param_grids['æç«¯éšæœºæ ‘'] = {
            'n_estimators': randint(50, 500),
            'max_depth': [None, 5, 10, 15, 20, 25, 30],
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10),
            'max_features': ['sqrt', 'log2', None]
        }
    else:
        param_grids['æç«¯éšæœºæ ‘'] = {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10]
        }
    
    # æœ´ç´ è´å¶æ–¯
    param_grids['æœ´ç´ è´å¶æ–¯'] = {
        'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]
    }
    
    # XGBoost
    if xgb_installed:
        if USE_RANDOMIZED_SEARCH:
            param_grids['XGBoost'] = {
                'n_estimators': randint(50, 500),
                'learning_rate': uniform(0.01, 0.29),
                'max_depth': randint(3, 12),
                'min_child_weight': randint(1, 10),
                'subsample': uniform(0.6, 0.4),
                'colsample_bytree': uniform(0.6, 0.4),
                'reg_alpha': uniform(0, 0.1),
                'reg_lambda': uniform(0, 0.1)
            }
        else:
            param_grids['XGBoost'] = {
                'n_estimators': [50, 100, 200, 300],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'max_depth': [3, 5, 7, 9]
            }
    
    # LightGBM
    if lgbm_installed:
        if USE_RANDOMIZED_SEARCH:
            param_grids['LightGBM'] = {
                'n_estimators': randint(50, 500),
                'learning_rate': uniform(0.01, 0.29),
                'max_depth': randint(3, 12),
                'num_leaves': randint(10, 100),
                'min_child_samples': randint(5, 50),
                'subsample': uniform(0.6, 0.4),
                'colsample_bytree': uniform(0.6, 0.4),
                'reg_alpha': uniform(0, 0.1),
                'reg_lambda': uniform(0, 0.1)
            }
        else:
            param_grids['LightGBM'] = {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'num_leaves': [10, 31, 50]
            }
    
    # CatBoost
    if catboost_installed:
        if USE_RANDOMIZED_SEARCH:
            param_grids['CatBoost'] = {
                'iterations': randint(50, 500),
                'learning_rate': uniform(0.01, 0.29),
                'depth': randint(3, 10),
                'l2_leaf_reg': uniform(0.1, 9.9),
                'border_count': [32, 64, 128, 200, 254],
                'verbose': [False]
            }
        else:
            param_grids['CatBoost'] = {
                'iterations': [50, 100, 200],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'depth': [3, 5, 7],
                'verbose': [False]
            }
    
    return param_grids

def get_model_instance(model_name):
    """æ ¹æ®æ¨¡å‹åç§°è·å–æ¨¡å‹å®ä¾‹"""
    if model_name == 'SVM':
        return SVC(random_state=RANDOM_STATE)
    elif model_name == 'éšæœºæ£®æ—':
        return RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)
    elif model_name == 'æ¢¯åº¦æå‡':
        return GradientBoostingClassifier(random_state=RANDOM_STATE)
    elif model_name == 'é€»è¾‘å›å½’':
        return LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)
    elif model_name == 'Kè¿‘é‚»':
        return KNeighborsClassifier(n_jobs=-1)
    elif model_name == 'æç«¯éšæœºæ ‘':
        return ExtraTreesClassifier(random_state=RANDOM_STATE, n_jobs=-1)
    elif model_name == 'æœ´ç´ è´å¶æ–¯':
        return GaussianNB()
    elif model_name == 'XGBoost' and xgb_installed:
        return xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss', n_jobs=-1)
    elif model_name == 'LightGBM' and lgbm_installed:
        return lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)
    elif model_name == 'CatBoost' and catboost_installed:
        return cb.CatBoostClassifier(random_state=RANDOM_STATE, verbose=False, thread_count=-1)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

def train_and_evaluate_model(model_name, param_grid, X_train, y_train, X_test, y_test, output_dir):
    """è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªæ¨¡å‹"""
    try:
        print(f"\nğŸ¤– å¼€å§‹è®­ç»ƒ: {model_name}")
        
        model = get_model_instance(model_name)
        
        # å‚æ•°æœç´¢
        if USE_RANDOMIZED_SEARCH:
            search = RandomizedSearchCV(
                model, param_grid, 
                n_iter=RANDOMIZED_SEARCH_ITERATIONS,
                cv=CV_FOLDS, 
                scoring=OPTIMIZATION_TARGET,
                n_jobs=CPU_COUNT,
                random_state=RANDOM_STATE,
                verbose=0
            )
        else:
            search = GridSearchCV(
                model, param_grid,
                cv=CV_FOLDS,
                scoring=OPTIMIZATION_TARGET,
                n_jobs=CPU_COUNT,
                verbose=0
            )
        
        # è®­ç»ƒ
        search.fit(X_train, y_train)
        best_model = search.best_estimator_
        best_params = search.best_params_
        
        print(f"âœ… {model_name} å‚æ•°æœç´¢å®Œæˆ")
        print(f"ğŸ¯ æœ€ä¼˜å‚æ•°: {best_params}")
        
        # é¢„æµ‹
        y_pred = best_model.predict(X_test)
        
        if hasattr(best_model, 'predict_proba'):
            y_pred_proba = best_model.predict_proba(X_test)[:, 1]
        elif hasattr(best_model, 'decision_function'):
            y_pred_proba = best_model.decision_function(X_test)
        else:
            y_pred_proba = y_pred.astype(float)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0),
            'auc': roc_auc_score(y_test, y_pred_proba),
            'aupr': average_precision_score(y_test, y_pred_proba),
            'best_params': best_params
        }
        
        print(f"ğŸ“Š {model_name} æ€§èƒ½æŒ‡æ ‡:")
        print(f"  AUC: {metrics['auc']:.4f}")
        print(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
        print(f"  å¬å›ç‡: {metrics['recall']:.4f}")
        print(f"  F1åˆ†æ•°: {metrics['f1']:.4f}")
        print(f"  AUPR: {metrics['aupr']:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(output_dir, f'{model_name}_model.pkl')
        joblib.dump(best_model, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ç”Ÿæˆå¯è§†åŒ–
        if VISUALIZATION_ENABLED and SAVE_PLOTS:
            generate_model_visualizations(y_test, y_pred, y_pred_proba, model_name, output_dir)
        
        return model_name, best_model, metrics
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒ {model_name} å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def generate_model_visualizations(y_test, y_pred, y_pred_proba, model_name, output_dir):
    """ç”Ÿæˆæ¨¡å‹å¯è§†åŒ–å›¾è¡¨"""
    try:
        # 1. æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['éBBBè‚½', 'BBBè‚½'], yticklabels=['éBBBè‚½', 'BBBè‚½'])
        plt.title(f'{model_name} - æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f'{model_name}_confusion_matrix.png'), 
                   dpi=PLOT_DPI, bbox_inches='tight')
        plt.close()
        
        # 2. ROCæ›²çº¿
        plt.figure(figsize=(8, 6))
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        auc = roc_auc_score(y_test, y_pred_proba)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('å‡æ­£ä¾‹ç‡')
        plt.ylabel('çœŸæ­£ä¾‹ç‡')
        plt.title(f'{model_name} - ROCæ›²çº¿')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f'{model_name}_roc_curve.png'), 
                   dpi=PLOT_DPI, bbox_inches='tight')
        plt.close()
        
        # 3. PRæ›²çº¿
        plt.figure(figsize=(8, 6))
        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
        aupr = average_precision_score(y_test, y_pred_proba)
        plt.plot(recall, precision, label=f'{model_name} (AUPR = {aupr:.3f})', linewidth=2)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('å¬å›ç‡')
        plt.ylabel('ç²¾ç¡®ç‡')
        plt.title(f'{model_name} - PRæ›²çº¿')
        plt.legend(loc="lower left")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f'{model_name}_pr_curve.png'), 
                   dpi=PLOT_DPI, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ {model_name} å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜")
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆ {model_name} å¯è§†åŒ–å¤±è´¥: {e}")

def generate_model_comparison_report(results, output_dir):
    """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
    if not results:
        print("âŒ æ²¡æœ‰ç»“æœå¯ä¾›æ¯”è¾ƒ")
        return

    try:
        # åˆ›å»ºå¯¹æ¯”æ•°æ®
        comparison_data = []
        for model_name, metrics in results.items():
            comparison_data.append({
                'æ¨¡å‹': model_name,
                'AUC': f"{metrics['auc']:.4f}",
                'å‡†ç¡®ç‡': f"{metrics['accuracy']:.4f}",
                'ç²¾ç¡®ç‡': f"{metrics['precision']:.4f}",
                'å¬å›ç‡': f"{metrics['recall']:.4f}",
                'F1åˆ†æ•°': f"{metrics['f1']:.4f}",
                'AUPR': f"{metrics['aupr']:.4f}"
            })

        # æŒ‰AUCæ’åº
        comparison_data.sort(key=lambda x: float(x['AUC']), reverse=True)
        
        # ä¿å­˜CSV
        comparison_df = pd.DataFrame(comparison_data)
        csv_path = os.path.join(output_dir, 'model_comparison.csv')
        comparison_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“‹ æ¨¡å‹å¯¹æ¯”è¡¨å·²ä¿å­˜: {csv_path}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results_path = os.path.join(output_dir, 'detailed_results.json')
        with open(detailed_results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        print(f"ğŸ“‹ è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_results_path}")

        # æ‰“å°ç»“æœ
        print("\n" + "="*80)
        print("ğŸ† BBBè‚½é¢„æµ‹æ¨¡å‹æ€§èƒ½å¯¹æ¯” (æŒ‰AUCæ’åº)")
        print("="*80)
        print(f"{'æ¨¡å‹':<15}{'AUC':<8}{'å‡†ç¡®ç‡':<8}{'ç²¾ç¡®ç‡':<8}{'å¬å›ç‡':<8}{'F1åˆ†æ•°':<8}{'AUPR':<8}")
        print("-"*80)
        
        for item in comparison_data:
            print(f"{item['æ¨¡å‹']:<15}{item['AUC']:<8}{item['å‡†ç¡®ç‡']:<8}{item['ç²¾ç¡®ç‡']:<8}"
                  f"{item['å¬å›ç‡']:<8}{item['F1åˆ†æ•°']:<8}{item['AUPR']:<8}")

        # è¾“å‡ºæœ€ä½³æ¨¡å‹
        best_model_name = comparison_data[0]['æ¨¡å‹']
        best_auc = comparison_data[0]['AUC']
        print(f"\nğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_model_name} (AUC: {best_auc})")
        print("="*80)

    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Šå¤±è´¥: {e}")

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ BBBè‚½é¢„æµ‹ç³»ç»Ÿ (ç‰¹å¾ç‹¬ç«‹å­˜å‚¨ç‰ˆ)")
    print("åŸºäºESM2ç‰¹å¾æå–å’Œå¤šç§æœºå™¨å­¦ä¹ æ–¹æ³•")
    print("æ”¯æŒdataæ–‡ä»¶å¤¹ç»Ÿä¸€ç®¡ç†å’ŒESM2ç‰¹å¾æ–‡ä»¶ç‹¬ç«‹å­˜å‚¨")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # æ£€æŸ¥ESMå¯ç”¨æ€§
    if not ESM_AVAILABLE:
        print("âŒ ESMæ¨¡å—æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…:")
        print("pip install fair-esm")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•å’Œç‰¹å¾ç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    create_feature_directories()
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ“ ç‰¹å¾å­˜å‚¨ç›®å½•: {FEATURES_DIR}")
    
    try:
        # 1. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        data_loader = BBBDataLoader()
        
        # 2. å‡†å¤‡æ•°æ®
        print("\n" + "="*60)
        print("ç¬¬ä¸€æ­¥: æ•°æ®å‡†å¤‡")
        print("="*60)
        
        train_sequences, train_labels, train_ids, train_sources = data_loader.load_all_training_data()
        test_sequences, test_labels, test_ids, test_sources = data_loader.load_all_test_data()
        
        if not train_sequences or not test_sequences:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥dataæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶")
            print("ğŸ“‹ éœ€è¦çš„æ–‡ä»¶:")
            print(f"  - {POS_TEST_FILE}")
            print(f"  - {TRAIN_POS_ORG_FILE}")
            print(f"  - {NEG_TRAIN_FILE}")
            print(f"  - {NEG_TEST_FILE}")
            print(f"  - ä¼ªæ­£æ ·æœ¬æ–‡ä»¶ (CSVæˆ–FASTAæ ¼å¼)")
            return False
        
        # 3. ESM2ç‰¹å¾æå– (æ”¯æŒç¼“å­˜)
        print("\n" + "="*60)
        print("ç¬¬äºŒæ­¥: ESM2ç‰¹å¾æå– (æ”¯æŒç¼“å­˜)")
        print("="*60)
        
        extractor = ESM2FeatureExtractor()
        
        # æå–è®­ç»ƒé›†ç‰¹å¾
        print("ğŸ”¬ å¤„ç†è®­ç»ƒé›†ç‰¹å¾...")
        X_train = extractor.extract_features(
            train_sequences, 
            dataset_type='train', 
            labels=train_labels, 
            seq_ids=train_ids
        )
        if X_train is None:
            print("âŒ è®­ç»ƒé›†ç‰¹å¾æå–å¤±è´¥")
            return False
        
        # æå–æµ‹è¯•é›†ç‰¹å¾
        print("ğŸ”¬ å¤„ç†æµ‹è¯•é›†ç‰¹å¾...")
        X_test = extractor.extract_features(
            test_sequences, 
            dataset_type='test', 
            labels=test_labels, 
            seq_ids=test_ids
        )
        if X_test is None:
            print("âŒ æµ‹è¯•é›†ç‰¹å¾æå–å¤±è´¥")
            return False
        
        # ä¿å­˜ä¼ ç»Ÿæ ¼å¼çš„ç‰¹å¾æ–‡ä»¶ï¼ˆå…¼å®¹æ€§ï¼‰
        np.save(os.path.join(OUTPUT_DIR, 'X_train.npy'), X_train)
        np.save(os.path.join(OUTPUT_DIR, 'X_test.npy'), X_test)
        np.save(os.path.join(OUTPUT_DIR, 'y_train.npy'), train_labels)
        np.save(os.path.join(OUTPUT_DIR, 'y_test.npy'), test_labels)
        print(f"ğŸ’¾ å…¼å®¹æ€§ç‰¹å¾æ•°æ®å·²ä¿å­˜åˆ° {OUTPUT_DIR}")
        
        # 4. ç‰¹å¾æ ‡å‡†åŒ–
        print("\n" + "="*30)
        print("ç¬¬ä¸‰æ­¥: ç‰¹å¾æ ‡å‡†åŒ–")
        print("="*30)
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'feature_scaler.pkl'))
        print("âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        
        # 5. æ¨¡å‹è®­ç»ƒ
        print("\n" + "="*60)
        print("ç¬¬å››æ­¥: å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
        print("="*60)
        
        param_grids = get_extended_param_grids()
        trained_models = {}
        all_results = {}
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        for model_name, param_grid in param_grids.items():
            result = train_and_evaluate_model(
                model_name, param_grid, 
                X_train_scaled, train_labels, 
                X_test_scaled, test_labels, 
                OUTPUT_DIR
            )
            
            if result:
                name, model, metrics = result
                trained_models[name] = model
                all_results[name] = metrics
        
        # 6. é›†æˆå­¦ä¹ 
        print("\n" + "="*40)
        print("ç¬¬äº”æ­¥: é›†æˆå­¦ä¹ ")
        print("="*40)
        
        if len(trained_models) >= 2:
            # æŠ•ç¥¨åˆ†ç±»å™¨
            try:
                print("ğŸ¤ è®­ç»ƒæŠ•ç¥¨åˆ†ç±»å™¨...")
                voting_models = list(trained_models.items())[:5]  # æœ€å¤š5ä¸ªæ¨¡å‹
                voting_classifier = VotingClassifier(
                    estimators=voting_models, 
                    voting='soft'
                )
                voting_classifier.fit(X_train_scaled, train_labels)
                
                # æ‰‹åŠ¨è¯„ä¼°æŠ•ç¥¨åˆ†ç±»å™¨
                y_pred = voting_classifier.predict(X_test_scaled)
                y_pred_proba = voting_classifier.predict_proba(X_test_scaled)[:, 1]
                
                voting_metrics = {
                    'accuracy': accuracy_score(test_labels, y_pred),
                    'precision': precision_score(test_labels, y_pred, zero_division=0),
                    'recall': recall_score(test_labels, y_pred, zero_division=0),
                    'f1': f1_score(test_labels, y_pred, zero_division=0),
                    'auc': roc_auc_score(test_labels, y_pred_proba),
                    'aupr': average_precision_score(test_labels, y_pred_proba),
                    'best_params': 'ensemble_voting'
                }
                
                all_results['æŠ•ç¥¨åˆ†ç±»å™¨'] = voting_metrics
                joblib.dump(voting_classifier, os.path.join(OUTPUT_DIR, 'æŠ•ç¥¨åˆ†ç±»å™¨_model.pkl'))
                
                if VISUALIZATION_ENABLED:
                    generate_model_visualizations(test_labels, y_pred, y_pred_proba, 'æŠ•ç¥¨åˆ†ç±»å™¨', OUTPUT_DIR)
                
                print("âœ… æŠ•ç¥¨åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")
                
            except Exception as e:
                print(f"âš ï¸ æŠ•ç¥¨åˆ†ç±»å™¨è®­ç»ƒå¤±è´¥: {e}")
            
            # å †å åˆ†ç±»å™¨
            if len(trained_models) >= 3:
                try:
                    print("ğŸ—ï¸ è®­ç»ƒå †å åˆ†ç±»å™¨...")
                    base_models = list(trained_models.items())[:4]  # æœ€å¤š4ä¸ªåŸºæ¨¡å‹
                    meta_model = LogisticRegression(random_state=RANDOM_STATE)
                    stacking_classifier = StackingClassifier(
                        estimators=base_models,
                        final_estimator=meta_model,
                        cv=3
                    )
                    stacking_classifier.fit(X_train_scaled, train_labels)
                    
                    # æ‰‹åŠ¨è¯„ä¼°å †å åˆ†ç±»å™¨
                    y_pred = stacking_classifier.predict(X_test_scaled)
                    y_pred_proba = stacking_classifier.predict_proba(X_test_scaled)[:, 1]
                    
                    stacking_metrics = {
                        'accuracy': accuracy_score(test_labels, y_pred),
                        'precision': precision_score(test_labels, y_pred, zero_division=0),
                        'recall': recall_score(test_labels, y_pred, zero_division=0),
                        'f1': f1_score(test_labels, y_pred, zero_division=0),
                        'auc': roc_auc_score(test_labels, y_pred_proba),
                        'aupr': average_precision_score(test_labels, y_pred_proba),
                        'best_params': 'ensemble_stacking'
                    }
                    
                    all_results['å †å åˆ†ç±»å™¨'] = stacking_metrics
                    joblib.dump(stacking_classifier, os.path.join(OUTPUT_DIR, 'å †å åˆ†ç±»å™¨_model.pkl'))
                    
                    if VISUALIZATION_ENABLED:
                        generate_model_visualizations(test_labels, y_pred, y_pred_proba, 'å †å åˆ†ç±»å™¨', OUTPUT_DIR)
                    
                    print("âœ… å †å åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")
                    
                except Exception as e:
                    print(f"âš ï¸ å †å åˆ†ç±»å™¨è®­ç»ƒå¤±è´¥: {e}")
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        print("\n" + "="*60)
        print("ç¬¬å…­æ­¥: ç”Ÿæˆç»“æœæŠ¥å‘Š")
        print("="*60)
        
        generate_model_comparison_report(all_results, OUTPUT_DIR)
        
        # ä¿å­˜æ•°æ®ä¿¡æ¯
        data_info = {
            'train_samples': len(train_sequences),
            'test_samples': len(test_sequences),
            'train_positive': sum(train_labels),
            'test_positive': sum(test_labels),
            'feature_dim': X_train.shape[1],
            'esm2_model': ESM2_MODEL_NAME,
            'data_directory': DATA_DIR,
            'features_directory': FEATURES_DIR,
            'training_time': datetime.now().isoformat()
        }
        
        with open(os.path.join(OUTPUT_DIR, 'data_info.json'), 'w', encoding='utf-8') as f:
            json.dump(data_info, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºç‰¹å¾å­˜å‚¨ä¿¡æ¯
        print(f"\nğŸ“‚ ESM2ç‰¹å¾æ–‡ä»¶å­˜å‚¨ä¿¡æ¯:")
        print(f"  ç‰¹å¾æ ¹ç›®å½•: {FEATURES_DIR}")
        print(f"  è®­ç»ƒç‰¹å¾ç›®å½•: {TRAIN_FEATURES_DIR}")
        print(f"  æµ‹è¯•ç‰¹å¾ç›®å½•: {TEST_FEATURES_DIR}")
        print(f"  ç‰¹å¾å…ƒæ•°æ®: {FEATURE_METADATA_FILE}")
        
        # æ˜¾ç¤ºç‰¹å¾æ–‡ä»¶
        metadata = load_feature_metadata()
        if metadata:
            print(f"\nğŸ“‹ å·²ä¿å­˜çš„ç‰¹å¾æ–‡ä»¶:")
            for filename, info in metadata.items():
                print(f"  ğŸ—‚ï¸ {filename}")
                print(f"     æ•°æ®é›†: {info['dataset_type']}")
                print(f"     åºåˆ—æ•°: {info['sequence_count']}")
                print(f"     ç‰¹å¾ç»´åº¦: {info['feature_dim']}")
                print(f"     æ–‡ä»¶å¤§å°: {info['file_size_mb']:.2f} MB")
                print(f"     åˆ›å»ºæ—¶é—´: {info['creation_time'][:19]}")
        
        print(f"\nğŸ‰ BBBè‚½é¢„æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")
        print(f"ğŸ—‚ï¸ ç‰¹å¾æ–‡ä»¶ä¿å­˜åœ¨: {FEATURES_DIR}")
        print(f"ğŸ“Š å…±è®­ç»ƒäº† {len(all_results)} ä¸ªæ¨¡å‹")
        print(f"ğŸ“‚ æ•°æ®æ¥æº: {DATA_DIR} æ–‡ä»¶å¤¹")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    np.random.seed(RANDOM_STATE)
    torch.manual_seed(RANDOM_STATE)
    
    # è¿è¡Œä¸»ç¨‹åº
    success = main()
    
    if success:
        print("\nâœ… ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
    else:
        print("\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥!")
    
    # åœ¨Windowsä¸‹æš‚åœï¼Œæ–¹ä¾¿æŸ¥çœ‹ç»“æœ
    if os.name == 'nt':
        input("\næŒ‰å›è½¦é”®é€€å‡º...")